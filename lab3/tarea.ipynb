{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d13bbe5-80d1-4879-bc47-d8e366f38456",
   "metadata": {},
   "source": [
    "# **Lunar Lander con Q-Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82127016-b75d-4621-a53e-8b2bb63cd3f8",
   "metadata": {},
   "source": [
    "### **1. Bibliotecas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6e0a19d-696b-440e-84eb-fe2b8761d955",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# Tal vez tengan que ejecutar lo siguiente en sus máquinas (ubuntu 20.04)\n",
    "# sudo apt-get remove swig\n",
    "# sudo apt-get install swig3.0\n",
    "# sudo ln -s /usr/bin/swig3.0 /usr/bin/swig\n",
    "# En windows tambien puede ser necesario MSVC++\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9831e9d4-7840-485c-bc38-af987a76de4f",
   "metadata": {},
   "source": [
    "### **2. Jugando a mano**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e744255-5e3f-4c17-b9dc-c6374c2f06c2",
   "metadata": {},
   "source": [
    "A continuación se puede jugar un episodio del lunar lander. Se controlan los motores con el teclado. Notar que solo se puede realizar una acción a la vez (que es parte del problema), y que en esta implementación, izq toma precedencia sobre derecha, que toma precedencia sobre el motor principal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6211ed30-b1a3-4b8e-9858-17eee433ddca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nota: hay que transformar esta celda en código para ejecutar (Esc + y)\n",
    "\n",
    "import pygame\n",
    "from pygame.locals import *\n",
    "\n",
    "# Inicializar pygame (para el control con el teclado) y el ambiente\n",
    "pygame.init()\n",
    "env = gym.make('LunarLander-v2', render_mode='human')\n",
    "env.reset()\n",
    "pygame.display.set_caption('Lunar Lander')\n",
    "\n",
    "clock = pygame.time.Clock()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    for event in pygame.event.get():\n",
    "        if event.type == QUIT:\n",
    "            done = True\n",
    "            break\n",
    "\n",
    "    keys = pygame.key.get_pressed()\n",
    "\n",
    "    # Map keys to actions\n",
    "    if keys[K_LEFT]:\n",
    "        action = 3  # Fire left orientation engine\n",
    "    elif keys[K_RIGHT]:\n",
    "        action = 1 # Fire right orientation engine\n",
    "    elif keys[K_UP]:\n",
    "        action = 2  # Fire main engine\n",
    "    else:\n",
    "        action = 0  # Do nothing\n",
    "\n",
    "    _, _, terminated, truncated, _ = env.step(action)\n",
    "    env.render()\n",
    "    clock.tick(10)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        done = True\n",
    "\n",
    "env.close()\n",
    "pygame.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d721a629-7f3f-4a70-83c0-e12f43b0f285",
   "metadata": {},
   "source": [
    "## **3. Discretizando el estado**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d35189-3a15-4ceb-b978-38226518092a",
   "metadata": {},
   "source": [
    "El estado consiste de posiciones y velocidades en (x,y,theta) y en información de contacto de los pies con la superficie.\n",
    "\n",
    "Como varios de estos son continuos, tenemos que discretizarlos para aplicar nuestro algoritmo de aprendizaje por refuerzo tabular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7b6bed3-12ba-4d92-8b09-3d3f8429217f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High:  [1.5       1.5       5.        5.        3.1415927 5.        1.\n",
      " 1.       ]\n",
      "Low:  [-1.5        0.        -5.        -5.        -3.1415927 -5.\n",
      " -0.        -0.       ]\n"
     ]
    }
   ],
   "source": [
    "# Cuántos bins queremos por dimensión\n",
    "# Pueden considerar variar este parámetro\n",
    "bins_per_dim = 15\n",
    "\n",
    "# Estado:\n",
    "# (x, y, x_vel, y_vel, theta, theta_vel, pie_izq_en_contacto, pie_derecho_en_contacto)\n",
    "NUM_BINS = [bins_per_dim, bins_per_dim, bins_per_dim, bins_per_dim, bins_per_dim, bins_per_dim, 2, 2]\n",
    "\n",
    "env = gym.make('LunarLander-v2')\n",
    "env.reset()\n",
    "\n",
    "# Tomamos los rangos del env\n",
    "OBS_SPACE_HIGH = env.observation_space.high\n",
    "OBS_SPACE_LOW = env.observation_space.low\n",
    "OBS_SPACE_LOW[1] = 0 # Para la coordenada y (altura), no podemos ir más abajo que la zona dea aterrizae (que está en el 0, 0)\n",
    "\n",
    "# Los bins para cada dimensión\n",
    "bins = [\n",
    "    np.linspace(OBS_SPACE_LOW[i], OBS_SPACE_HIGH[i], NUM_BINS[i] - 1)\n",
    "    for i in range(len(NUM_BINS) - 2) # last two are binary\n",
    "]\n",
    "# Se recomienda observar los bins para entender su estructura\n",
    "#print (\"Bins: \", bins)\n",
    "print(\"High: \", OBS_SPACE_HIGH)\n",
    "print(\"Low: \", OBS_SPACE_LOW)\n",
    "def discretize_state(state, bins):\n",
    "    \"\"\"Discretize the continuous state into a tuple of discrete indices.\"\"\"\n",
    "    state_disc = list()\n",
    "    for i in range(len(state)):\n",
    "        if i >= len(bins):  # For binary features (leg contacts)\n",
    "            state_disc.append(int(state[i]))\n",
    "        else:\n",
    "            state_disc.append(\n",
    "                np.digitize(state[i], bins[i])\n",
    "            )\n",
    "    return tuple(state_disc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bcc921-c5f6-4f06-9702-6e740b26fc6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(np.int64(7), np.int64(1), np.int64(7), np.int64(7), np.int64(7), np.int64(7), 1, 1)\n",
      "(np.int64(7), np.int64(14), np.int64(7), np.int64(7), np.int64(7), np.int64(7), 0, 0)\n"
     ]
    }
   ],
   "source": [
    "# Ejemplos\n",
    "print(discretize_state([0.0, 0.0, 0, 0, 0, 0, 1, 1], bins)) # En la zona de aterrizaje y quieto\n",
    "print(discretize_state([0, 1.5, 0, 0, 0, 0, 0, 0], bins)) # Comenzando la partida, arriba y en el centro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779c1576-eff2-4b3c-8069-f6d30243f1e6",
   "metadata": {},
   "source": [
    "## **4. Agentes y la interacción con el entorno**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d594f89c-70b1-4c7c-939f-d654a5263f1c",
   "metadata": {},
   "source": [
    "Vamos a definir una interfaz para nuestro agente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93e7059a-7b68-4b13-857b-16e96c5f9793",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agente:\n",
    "    def elegir_accion(self, estado, max_accion, explorar = True) -> int:\n",
    "        \"\"\"Elegir la accion a tomar en el estado actual y el espacio de acciones\n",
    "            - estado_anterior: el estado desde que se empezó\n",
    "            - estado_siguiente: el estado al que se llegó\n",
    "            - accion: la acción que llevo al agente desde estado_anterior a estado_siguiente\n",
    "            - recompensa: la recompensa recibida en la transicion\n",
    "            - terminado: si el episodio terminó\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def aprender(self, estado_anterior, estado_siguiente, accion, recompensa, terminado):\n",
    "        \"\"\"Aprender a partir de la tupla \n",
    "            - estado_anterior: el estado desde que se empezó\n",
    "            - estado_siguiente: el estado al que se llegó\n",
    "            - accion: la acción que llevo al agente desde estado_anterior a estado_siguiente\n",
    "            - recompensa: la recompensa recibida en la transicion\n",
    "            - terminado: si el episodio terminó en esta transición\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def fin_episodio(self):\n",
    "        \"\"\"Actualizar estructuras al final de un episodio\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c61e38-f7d3-40bf-af19-c7fefd143ffd",
   "metadata": {},
   "source": [
    "Para un agente aleatorio, la implementación sería:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3adcedd-b300-4e30-9cb1-19d5ec96fb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class AgenteAleatorio(Agente):\n",
    "    def elegir_accion(self, estado, max_accion, explorar = True) -> int:\n",
    "        # Elige una acción al azar\n",
    "        return random.randrange(max_accion)\n",
    "\n",
    "    def aprender(self, estado_anterior, estado_siguiente, accion, recompensa, terminado):\n",
    "        # No aprende\n",
    "        pass\n",
    "\n",
    "    def fin_episodio(self):\n",
    "        # Nada que actualizar\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19498b0d-eb74-431e-ac7f-612497ca07f3",
   "metadata": {},
   "source": [
    "Luego podemos definir una función para ejecutar un episodio con un agente dado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "038e9f56-f0cf-40b7-b3c3-a63b34572bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ejecutar_episodio_2(agente, aprender=True, max_iteraciones=500):\n",
    "    entorno = gym.make('LunarLander-v2').env\n",
    "    semilla = entorno.np_random.bit_generator._seed_seq.entropy\n",
    "    iteraciones = 0\n",
    "    recompensa_total = 0\n",
    "    episodio = []\n",
    "\n",
    "    termino = False\n",
    "    truncado = False\n",
    "    estado_anterior, info = entorno.reset(seed=semilla)\n",
    "    while iteraciones < max_iteraciones and not termino and not truncado:\n",
    "        # Le pedimos al agente que elija entre las posibles acciones (0..entorno.action_space.n)\n",
    "        accion = agente.elegir_accion(estado_anterior, entorno.action_space.n, aprender)\n",
    "        # Realizamos la accion\n",
    "        estado_siguiente, recompensa, termino, truncado, info = entorno.step(accion)\n",
    "        # Le informamos al agente para que aprenda\n",
    "        if aprender:\n",
    "            agente.aprender(estado_anterior, estado_siguiente, accion, recompensa, termino)\n",
    "\n",
    "        # Almacenar el estado, acción y recompensa\n",
    "        episodio.append((estado_anterior, accion, recompensa))\n",
    "\n",
    "        estado_anterior = estado_siguiente\n",
    "        iteraciones += 1\n",
    "        recompensa_total += recompensa\n",
    "\n",
    "    if aprender:\n",
    "        agente.fin_episodio()\n",
    "    entorno.close()\n",
    "    return recompensa_total, episodio, semilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa15ab0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ejecutar_episodio_1(agente, aprender = True, render = None, max_iteraciones=500):\n",
    "    entorno = gym.make('LunarLander-v2', render_mode=render).env\n",
    "    \n",
    "    iteraciones = 0\n",
    "    recompensa_total = 0\n",
    "\n",
    "    termino = False\n",
    "    truncado = False\n",
    "    estado_anterior, info = entorno.reset()\n",
    "    while iteraciones < max_iteraciones and not termino and not truncado:\n",
    "        # Le pedimos al agente que elija entre las posibles acciones (0..entorno.action_space.n)\n",
    "        accion = agente.elegir_accion(estado_anterior, entorno.action_space.n, aprender)\n",
    "        # Realizamos la accion\n",
    "        estado_siguiente, recompensa, termino, truncado, info = entorno.step(accion)\n",
    "        # Le informamos al agente para que aprenda\n",
    "        if (aprender):\n",
    "            agente.aprender(estado_anterior, estado_siguiente, accion, recompensa, termino)\n",
    "\n",
    "        estado_anterior = estado_siguiente\n",
    "        iteraciones += 1\n",
    "        recompensa_total += recompensa\n",
    "    if (aprender):\n",
    "        agente.fin_episodio()\n",
    "    entorno.close()\n",
    "    return recompensa_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a104779-f5e3-4bfa-b88b-fb44de195d49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(-213.26767006459664)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nota: hay que transformar esta celda en código para ejecutar (Esc + y)\n",
    "\n",
    "# Ejecutamos un episodio con el agente aleatorio y modo render 'human', para poder verlo\n",
    "ejecutar_episodio_1(AgenteAleatorio(), render = 'human')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b9cb2a-37b0-4115-afe8-bb89ce49f605",
   "metadata": {},
   "source": [
    "Podemos ejecutar este ambiente muchas veces y tomar métricas al respecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "536e81ee-6038-44c7-b887-35db442815ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasa de éxito: 0.0. Se obtuvo -167.8561863840643 de recompensa, en promedio\n"
     ]
    }
   ],
   "source": [
    "agente = AgenteAleatorio()\n",
    "recompensa_episodios = []\n",
    "\n",
    "exitos = 0\n",
    "num_episodios = 100\n",
    "for i in range(num_episodios):\n",
    "    recompensa = ejecutar_episodio_1(agente)\n",
    "    # Los episodios se consideran exitosos si se obutvo 200 o más de recompensa total\n",
    "    if (recompensa >= 200):\n",
    "        exitos += 1\n",
    "    recompensa_episodios += [recompensa]\n",
    "\n",
    "import numpy\n",
    "print(f\"Tasa de éxito: {exitos / num_episodios}. Se obtuvo {numpy.mean(recompensa_episodios)} de recompensa, en promedio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086cb250-7bc9-4bd4-a4cd-43cfd561facb",
   "metadata": {},
   "source": [
    "### **5. Programando un agente que aprende**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0827e3e2-a84a-462f-8b86-e03425cfc645",
   "metadata": {},
   "source": [
    "La tarea a realizar consiste en programar un agente de aprendizaje por refuerzos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3f37011-ffaa-4d08-b832-c45d0b9060da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "class AgenteRL(Agente):\n",
    "    # Agregar código aqui\n",
    "\n",
    "    # Pueden agregar parámetros al constructor\n",
    "    def __init__(self, max_accion, bins, initial_epsilon: float, epsilon_decay: float, final_epsilon: float, discount_factor: float = 0.95):\n",
    "        super().__init__()\n",
    "        \"\"\"Initialize a Reinforcement Learning agent with an empty dictionary\n",
    "        of state-action values (q_values), a learning rate and an epsilon.\n",
    "\n",
    "        Args:\n",
    "            learning_rate: The learning rate\n",
    "            initial_epsilon: The initial epsilon value\n",
    "            epsilon_decay: The decay for epsilon\n",
    "            final_epsilon: The final epsilon value\n",
    "            discount_factor: The discount factor for computing the Q-value\n",
    "        \"\"\"\n",
    "        self.q_table = defaultdict(lambda: np.zeros(max_accion))\n",
    "        self.visitas = defaultdict(lambda: np.zeros(max_accion))\n",
    "\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "        self.epsilon = initial_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.final_epsilon = final_epsilon\n",
    "\n",
    "        self.bins = bins\n",
    "\n",
    "        self.training_error = []\n",
    "    \n",
    "    def elegir_accion(self, estado, max_accion, explorar = True) -> int:\n",
    "\n",
    "        estado_discreto = discretize_state(estado, self.bins)\n",
    "        if (explorar and np.random.random() < self.epsilon):\n",
    "            return random.randrange(max_accion)\n",
    "        else:\n",
    "            return int(np.argmax(self.q_table[estado_discreto]))\n",
    "    \n",
    "    def aprender(self, estado_anterior, estado_siguiente, accion, recompensa, terminado):\n",
    "        estado_anterior_discreto = discretize_state(estado_anterior, self.bins)\n",
    "        estado_siguiente_discreto = discretize_state(estado_siguiente, self.bins)\n",
    "\n",
    "        self.visitas[estado_anterior_discreto][accion] += 1\n",
    "\n",
    "        \"\"\"Updates the Q-value of an action.\"\"\"\n",
    "        future_q_value = (not terminado) * np.max(self.q_table[estado_siguiente_discreto])\n",
    "        temporal_difference = (\n",
    "            recompensa + self.discount_factor * future_q_value - self.q_table[estado_anterior_discreto][accion]\n",
    "        )\n",
    "\n",
    "        lr = 1 / self.visitas[estado_anterior_discreto][accion]\n",
    "\n",
    "        self.q_table[estado_anterior_discreto][accion] = (\n",
    "            self.q_table[estado_anterior_discreto][accion] + lr * temporal_difference\n",
    "        )\n",
    "        self.training_error.append(temporal_difference)\n",
    "\n",
    "    def fin_episodio(self):\n",
    "        self.epsilon = max(self.final_epsilon, self.epsilon - self.epsilon_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ee51f5-aff8-4d18-934e-92acdcb617c0",
   "metadata": {},
   "source": [
    "Y ejecutar con el muchos episodios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e0524d5-0d12-46a8-8437-984b981fbae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100000 [00:00<?, ?it/s]/home/julio/.local/lib/python3.10/site-packages/numpy/_core/fromnumeric.py:3904: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/julio/.local/lib/python3.10/site-packages/numpy/_core/_methods.py:147: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "  0%|          | 10/100000 [00:00<23:52, 69.80it/s, promedio=-129, recompensa=-67.8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo nan de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1015/100000 [00:14<23:48, 69.28it/s, promedio=-176, recompensa=-182] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -176.20306955016136 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2015/100000 [00:28<22:24, 72.88it/s, promedio=-172, recompensa=-93]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -168.58455704968458 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3014/100000 [00:43<24:56, 64.80it/s, promedio=-170, recompensa=-67.2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -165.46163947940332 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4014/100000 [00:58<23:56, 66.83it/s, promedio=-167, recompensa=-34.7]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -159.1079779172706 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5013/100000 [01:14<25:32, 61.99it/s, promedio=-166, recompensa=-260] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -158.7408873799418 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6012/100000 [01:29<25:27, 61.55it/s, promedio=-164, recompensa=-112]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -155.35254596596806 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 7013/100000 [01:45<24:17, 63.79it/s, promedio=-162, recompensa=-145]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -150.17517571563508 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8012/100000 [02:01<24:06, 63.58it/s, promedio=-161, recompensa=-179] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -151.87927619033982 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 9012/100000 [02:18<24:19, 62.32it/s, promedio=-159, recompensa=-130]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -146.8368828739069 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 10012/100000 [02:35<24:38, 60.88it/s, promedio=-158, recompensa=-222] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -143.9601472971231 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 11010/100000 [02:52<27:14, 54.45it/s, promedio=-156, recompensa=-85.2] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -142.86026410001023 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 12011/100000 [03:10<25:42, 57.04it/s, promedio=-155, recompensa=-103]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -137.4240653922457 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 13011/100000 [03:28<26:33, 54.60it/s, promedio=-154, recompensa=-249] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -142.8441402646452 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 14011/100000 [03:46<26:38, 53.78it/s, promedio=-153, recompensa=-95]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -138.38076475013256 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 15010/100000 [04:05<26:07, 54.23it/s, promedio=-151, recompensa=-52.4]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -135.12216095934767 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 16010/100000 [04:25<27:06, 51.65it/s, promedio=-150, recompensa=-72.6] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -133.26765784015387 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 17009/100000 [04:45<28:52, 47.90it/s, promedio=-149, recompensa=-14.5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -132.51086075105795 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 18009/100000 [05:05<28:50, 47.37it/s, promedio=-148, recompensa=-155] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -127.21578470564357 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 19009/100000 [05:27<28:09, 47.95it/s, promedio=-147, recompensa=-268]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -129.21646835441652 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 20007/100000 [05:48<32:30, 41.02it/s, promedio=-146, recompensa=-15.3]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -122.18803071527458 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 21009/100000 [06:11<29:02, 45.34it/s, promedio=-145, recompensa=-144]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -121.53665952756339 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 22009/100000 [06:33<29:16, 44.41it/s, promedio=-143, recompensa=-40.7]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -117.97024039640733 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 23008/100000 [06:57<31:09, 41.18it/s, promedio=-142, recompensa=-20.3]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -113.37676902189827 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 24009/100000 [07:21<27:56, 45.34it/s, promedio=-141, recompensa=-91.6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -107.28474518095285 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 25007/100000 [07:46<29:47, 41.95it/s, promedio=-139, recompensa=-57]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -108.36310796595345 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 26007/100000 [08:12<31:15, 39.46it/s, promedio=-138, recompensa=-181]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -97.97960686353564 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 27007/100000 [08:38<34:07, 35.65it/s, promedio=-137, recompensa=-10.4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -102.45713854879091 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 28009/100000 [09:05<30:38, 39.16it/s, promedio=-135, recompensa=-10.5]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -103.85964808473206 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 29006/100000 [09:34<34:15, 34.54it/s, promedio=-134, recompensa=38.8]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -98.02357015753726 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 30006/100000 [10:03<32:47, 35.57it/s, promedio=-133, recompensa=-191]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -100.46459105726052 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 31006/100000 [10:33<33:03, 34.79it/s, promedio=-132, recompensa=33.9]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -99.92854027665537 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 32007/100000 [11:04<40:04, 28.27it/s, promedio=-131, recompensa=-75.6] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -101.03048510343919 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 33007/100000 [11:36<35:30, 31.45it/s, promedio=-130, recompensa=-136]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -101.78840261668623 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 34007/100000 [12:09<32:42, 33.63it/s, promedio=-129, recompensa=-43]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -101.10094086445297 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 35006/100000 [12:42<36:13, 29.90it/s, promedio=-129, recompensa=-51.9]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -111.42594856315334 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 36007/100000 [13:18<35:43, 29.86it/s, promedio=-128, recompensa=-9.97] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -104.51410070033143 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 37005/100000 [13:54<41:15, 25.45it/s, promedio=-127, recompensa=-21.9] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -103.65329802082564 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 38005/100000 [14:31<40:37, 25.44it/s, promedio=-127, recompensa=-241]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -111.67078293544165 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 39005/100000 [15:10<35:00, 29.04it/s, promedio=-126, recompensa=-94.2] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -103.45041726835494 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 40003/100000 [15:50<49:56, 20.02it/s, promedio=-126, recompensa=-8]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -104.60225973359674 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 41004/100000 [16:31<41:35, 23.64it/s, promedio=-125, recompensa=-38.9] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -101.93500672048663 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 42005/100000 [17:16<38:43, 24.96it/s, promedio=-124, recompensa=-46.4]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -88.0468242493674 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 43004/100000 [18:01<49:43, 19.11it/s, promedio=-124, recompensa=-26.3] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -89.05275206261216 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 44003/100000 [18:48<47:15, 19.75it/s, promedio=-123, recompensa=148]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -83.45103475213607 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 45004/100000 [19:39<47:30, 19.30it/s, promedio=-122, recompensa=-94]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -82.17346974011646 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 46003/100000 [20:31<49:48, 18.07it/s, promedio=-121, recompensa=-30.9] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -73.87556624901248 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 47004/100000 [21:24<47:43, 18.50it/s, promedio=-119, recompensa=-146]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -61.260104040343414 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 48003/100000 [22:18<55:14, 15.69it/s, promedio=-118, recompensa=-87.7]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -60.65530129991435 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 49005/100000 [23:14<42:11, 20.14it/s, promedio=-116, recompensa=-50.7] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -31.82597938227421 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 50004/100000 [11:16:14<41:27, 20.10it/s, promedio=-115, recompensa=-261]         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -33.21386455013932 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 51002/100000 [11:17:08<58:00, 14.08it/s, promedio=-113, recompensa=14.5]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -29.54613436041704 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 52004/100000 [11:17:58<36:03, 22.18it/s, promedio=-111, recompensa=-11.6] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -20.90664881960447 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 53003/100000 [11:18:51<46:00, 17.03it/s, promedio=-109, recompensa=-226]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -15.81359331501125 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 54004/100000 [11:19:42<39:12, 19.55it/s, promedio=-108, recompensa=-86.6] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -13.776607445514081 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 55003/100000 [11:20:33<38:46, 19.34it/s, promedio=-106, recompensa=23]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -16.21237526928025 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 56004/100000 [11:21:25<30:13, 24.26it/s, promedio=-104, recompensa=-110]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -14.007025900390015 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 57002/100000 [11:22:17<45:31, 15.74it/s, promedio=-103, recompensa=-48.5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -8.022534792624263 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 58004/100000 [11:23:12<31:25, 22.27it/s, promedio=-101, recompensa=21.2]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 0.4943447479674885 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 59004/100000 [11:24:07<37:15, 18.34it/s, promedio=-99.3, recompensa=-74.7] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -4.397104725753646 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 60004/100000 [11:25:00<34:44, 19.19it/s, promedio=-97.5, recompensa=216]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 8.270102077599189 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 61003/100000 [11:25:55<45:03, 14.42it/s, promedio=-95.7, recompensa=247]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 11.890214641862682 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 62004/100000 [11:26:50<29:09, 21.71it/s, promedio=-93.9, recompensa=-154]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 15.823264689745278 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 63004/100000 [11:27:46<32:15, 19.11it/s, promedio=-92.1, recompensa=-117]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 22.51420421786917 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 64003/100000 [11:28:42<33:26, 17.94it/s, promedio=-90.7, recompensa=-67.4]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -1.3360442575567135 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 65003/100000 [11:29:38<35:34, 16.39it/s, promedio=-89.2, recompensa=14.9]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 6.761501582062048 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 66005/100000 [11:30:34<27:17, 20.76it/s, promedio=-87.6, recompensa=-51.4] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 15.120121176698643 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 67003/100000 [11:31:29<31:19, 17.56it/s, promedio=-86.1, recompensa=-8.95]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 9.070306314105679 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 68004/100000 [11:32:26<32:21, 16.48it/s, promedio=-84.7, recompensa=-43.1] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 10.756541416967181 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 69004/100000 [11:33:24<29:33, 17.48it/s, promedio=-83.4, recompensa=297]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 7.286203244695063 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 70003/100000 [11:34:20<23:57, 20.86it/s, promedio=-81.9, recompensa=217]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 21.06394025718615 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 71003/100000 [11:35:18<27:42, 17.44it/s, promedio=-80.5, recompensa=211]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 18.14391420208414 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 72002/100000 [11:36:16<25:24, 18.36it/s, promedio=-79, recompensa=262]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 22.437774605711684 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 73003/100000 [11:37:13<25:24, 17.70it/s, promedio=-77.8, recompensa=-63.9] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 13.195077323798824 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 74003/100000 [11:38:12<26:38, 16.26it/s, promedio=-76.3, recompensa=-71.2] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 28.859037789585408 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 75003/100000 [11:39:08<25:52, 16.10it/s, promedio=-75, recompensa=259]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 27.3869574987898 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 76002/100000 [11:40:05<23:21, 17.12it/s, promedio=-73.6, recompensa=36.5]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 31.890808839518634 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 77004/100000 [11:41:03<21:42, 17.66it/s, promedio=-72.2, recompensa=50.6]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 33.09361573907141 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 78004/100000 [11:42:01<22:37, 16.21it/s, promedio=-70.7, recompensa=185]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 41.41472925618187 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 79003/100000 [11:42:58<19:10, 18.25it/s, promedio=-69.2, recompensa=-147]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 52.35827025292043 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 80003/100000 [11:43:56<20:06, 16.58it/s, promedio=-67.7, recompensa=63]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 47.237797991818795 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 81004/100000 [11:44:52<17:29, 18.10it/s, promedio=-66.4, recompensa=-190]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 39.440168800262626 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 82002/100000 [11:45:49<17:21, 17.27it/s, promedio=-65.1, recompensa=-173]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 37.35540363606143 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 83004/100000 [11:46:48<15:32, 18.23it/s, promedio=-63.9, recompensa=44.2]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 38.041067483988506 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 84003/100000 [11:47:48<14:07, 18.88it/s, promedio=-62.5, recompensa=276]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 50.105681601054236 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 85004/100000 [11:48:45<15:17, 16.35it/s, promedio=-61.3, recompensa=-196]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 36.8159332732155 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 86002/100000 [11:49:43<16:12, 14.40it/s, promedio=-60.2, recompensa=164]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 34.707313284070345 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 87003/100000 [11:50:43<11:31, 18.80it/s, promedio=-59.1, recompensa=160]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 39.742129404525954 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 88003/100000 [11:51:41<11:12, 17.85it/s, promedio=-57.8, recompensa=-120]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 53.364928155279884 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 89002/100000 [11:52:41<10:51, 16.89it/s, promedio=-56.5, recompensa=271]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 58.65866640183804 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 90004/100000 [11:53:41<08:26, 19.73it/s, promedio=-55.4, recompensa=-75.7]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 46.36556994761686 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 91002/100000 [11:54:40<07:57, 18.85it/s, promedio=-54.1, recompensa=38.8]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 59.754190158104784 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 92003/100000 [11:55:42<08:15, 16.16it/s, promedio=-53, recompensa=259]      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 44.61570626569137 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 93004/100000 [11:56:44<07:26, 15.66it/s, promedio=-51.9, recompensa=287]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 50.175108367008406 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 94004/100000 [11:57:46<06:24, 15.58it/s, promedio=-50.7, recompensa=-10.8]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 57.06405705378321 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 95004/100000 [11:58:49<05:16, 15.79it/s, promedio=-49.7, recompensa=212]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 47.56064352857761 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 96003/100000 [11:59:51<04:04, 16.33it/s, promedio=-48.6, recompensa=259]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 56.57726229035566 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 97004/100000 [12:00:53<03:17, 15.18it/s, promedio=-47.6, recompensa=-197]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 44.990093766162424 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 98002/100000 [12:01:57<02:15, 14.80it/s, promedio=-46.6, recompensa=-34.6] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 49.84738176939405 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 99003/100000 [12:03:00<01:09, 14.42it/s, promedio=-45.7, recompensa=181]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 47.70266878869875 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [12:04:05<00:00,  2.30it/s, promedio=-44.7, recompensa=259]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasa de éxito: 0.11711. Se obtuvo -44.74359840074916 de recompensa, en promedio\n"
     ]
    }
   ],
   "source": [
    "# Nota: hay que transformar esta celda en código para ejecutar (Esc + y)\n",
    "# Advertencia: este bloque es un loop infinito si el agente se deja sin implementar\n",
    "from tqdm import tqdm \n",
    "entorno = gym.make('LunarLander-v2').env\n",
    "num_episodios = 100000\n",
    "max_accion = entorno.action_space.n\n",
    "initial_epsilon = 1.0\n",
    "epsilon_decay = initial_epsilon / (num_episodios / 2)\n",
    "final_epsilon = 0.05\n",
    "discount_factor = 0.95\n",
    "\n",
    "agente = AgenteRL(max_accion = max_accion, bins = bins, initial_epsilon = initial_epsilon, epsilon_decay = epsilon_decay, final_epsilon = final_epsilon, discount_factor = discount_factor)\n",
    "exitos = 0\n",
    "recompensa_episodios = []\n",
    "with tqdm(total=num_episodios) as pbar:\n",
    "    for i in range(num_episodios):\n",
    "        recompensa = ejecutar_episodio_1(agente)\n",
    "        if i % 1000 == 0:\n",
    "            ultimas_1000_recompensas = recompensa_episodios[-1000:]\n",
    "            promedio_ultimas_1000 = np.mean(ultimas_1000_recompensas)\n",
    "            print(f\" Se obtuvo {promedio_ultimas_1000} de recompensa, en promedio en los últimos 1000 episodios\")\n",
    "        if (recompensa >= 200):\n",
    "            exitos += 1\n",
    "        recompensa_episodios += [recompensa]\n",
    "        pbar.set_postfix(recompensa=recompensa, promedio=numpy.mean(recompensa_episodios))\n",
    "        pbar.update(1)\n",
    "print(f\"Tasa de éxito: {exitos / num_episodios}. Se obtuvo {numpy.mean(recompensa_episodios)} de recompensa, en promedio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b7079e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 0 exitoso, recompensa: 245.0298774387565\n",
      "Episodio 2 exitoso, recompensa: 233.53788773812198\n",
      "Episodio 4 exitoso, recompensa: 209.77219206128882\n",
      "Episodio 11 exitoso, recompensa: 271.288874830841\n",
      "Episodio 14 exitoso, recompensa: 214.5502820862119\n",
      "Episodio 21 exitoso, recompensa: 226.22604280416425\n",
      "Episodio 30 exitoso, recompensa: 239.66109077561867\n",
      "Episodio 32 exitoso, recompensa: 202.83839244927012\n",
      "Episodio 34 exitoso, recompensa: 200.97059685154926\n",
      "Episodio 38 exitoso, recompensa: 204.44507364826492\n",
      "Episodio 41 exitoso, recompensa: 217.98511453388235\n",
      "Episodio 43 exitoso, recompensa: 208.46954614389364\n",
      "Episodio 45 exitoso, recompensa: 219.3198208669819\n",
      "Episodio 50 exitoso, recompensa: 226.21729860893672\n",
      "Episodio 51 exitoso, recompensa: 210.3631519929682\n",
      "Episodio 52 exitoso, recompensa: 251.92187388597844\n",
      "Episodio 57 exitoso, recompensa: 247.42849713625438\n",
      "Episodio 58 exitoso, recompensa: 217.3252532177924\n",
      "Episodio 64 exitoso, recompensa: 211.78598346851055\n",
      "Episodio 73 exitoso, recompensa: 269.60110588977193\n",
      "Episodio 75 exitoso, recompensa: 265.4059211154505\n",
      "Episodio 77 exitoso, recompensa: 266.60795306225293\n",
      "Episodio 78 exitoso, recompensa: 230.3066362181552\n",
      "Episodio 81 exitoso, recompensa: 222.73569132873405\n",
      "Episodio 84 exitoso, recompensa: 217.33156942422374\n",
      "Episodio 86 exitoso, recompensa: 285.1127057627424\n",
      "Episodio 87 exitoso, recompensa: 269.7589640445781\n",
      "Episodio 88 exitoso, recompensa: 264.84864384050724\n",
      "Episodio 95 exitoso, recompensa: 253.87288734037315\n",
      "Episodio 97 exitoso, recompensa: 219.97287737716053\n",
      "Episodio 99 exitoso, recompensa: 277.0528367478247\n",
      "Episodio 104 exitoso, recompensa: 222.22689692904277\n",
      "Episodio 111 exitoso, recompensa: 216.07192664892673\n",
      "Episodio 112 exitoso, recompensa: 271.1054489332038\n",
      "Episodio 113 exitoso, recompensa: 222.25987619685895\n",
      "Episodio 117 exitoso, recompensa: 233.5892233961133\n",
      "Episodio 118 exitoso, recompensa: 230.5826432603397\n",
      "Episodio 119 exitoso, recompensa: 241.01490886786866\n",
      "Episodio 124 exitoso, recompensa: 208.04666298914088\n",
      "Episodio 126 exitoso, recompensa: 226.19793655317065\n",
      "Episodio 129 exitoso, recompensa: 233.02937315450592\n",
      "Episodio 131 exitoso, recompensa: 247.5400146365292\n",
      "Episodio 135 exitoso, recompensa: 230.77594571240724\n",
      "Episodio 136 exitoso, recompensa: 284.35796114766833\n",
      "Episodio 145 exitoso, recompensa: 289.9365050712163\n",
      "Episodio 149 exitoso, recompensa: 215.83623611583573\n",
      "Episodio 151 exitoso, recompensa: 241.23274734626375\n",
      "Episodio 156 exitoso, recompensa: 211.14049088822605\n",
      "Episodio 157 exitoso, recompensa: 270.0846748707942\n",
      "Episodio 158 exitoso, recompensa: 247.954938788834\n",
      "Episodio 168 exitoso, recompensa: 234.93477076502276\n",
      "Episodio 172 exitoso, recompensa: 251.73326434618747\n",
      "Episodio 174 exitoso, recompensa: 275.8824834150916\n",
      "Episodio 176 exitoso, recompensa: 235.54377501001068\n",
      "Episodio 179 exitoso, recompensa: 235.18415662521457\n",
      "Episodio 180 exitoso, recompensa: 278.35191785976565\n",
      "Episodio 186 exitoso, recompensa: 216.92480276150258\n",
      "Episodio 187 exitoso, recompensa: 219.64491094923991\n",
      "Episodio 188 exitoso, recompensa: 281.3147270242716\n",
      "Episodio 201 exitoso, recompensa: 239.06679109002087\n",
      "Episodio 204 exitoso, recompensa: 287.6963334884334\n",
      "Episodio 208 exitoso, recompensa: 249.77821303313362\n",
      "Episodio 210 exitoso, recompensa: 268.50579270623416\n",
      "Episodio 211 exitoso, recompensa: 250.2872903875376\n",
      "Episodio 213 exitoso, recompensa: 248.69571761516812\n",
      "Episodio 237 exitoso, recompensa: 273.20575246384414\n",
      "Episodio 242 exitoso, recompensa: 244.58758695777482\n",
      "Episodio 244 exitoso, recompensa: 275.89843872582617\n",
      "Episodio 245 exitoso, recompensa: 266.998475407767\n",
      "Episodio 250 exitoso, recompensa: 215.29313461325168\n",
      "Episodio 261 exitoso, recompensa: 222.2996800989423\n",
      "Episodio 282 exitoso, recompensa: 236.19389353334114\n",
      "Episodio 285 exitoso, recompensa: 234.2274007293734\n",
      "Episodio 288 exitoso, recompensa: 227.3592806082191\n",
      "Episodio 299 exitoso, recompensa: 202.0832627240388\n",
      "Episodio 303 exitoso, recompensa: 263.23872101747145\n",
      "Episodio 310 exitoso, recompensa: 291.6271024530843\n",
      "Episodio 316 exitoso, recompensa: 234.48823572062093\n",
      "Episodio 318 exitoso, recompensa: 222.42741605494206\n",
      "Episodio 327 exitoso, recompensa: 255.6024723338399\n",
      "Episodio 333 exitoso, recompensa: 238.21599846040678\n",
      "Episodio 338 exitoso, recompensa: 270.14012124240503\n",
      "Episodio 339 exitoso, recompensa: 263.67209455686435\n",
      "Episodio 340 exitoso, recompensa: 258.8757751157128\n",
      "Episodio 341 exitoso, recompensa: 247.07675680955782\n",
      "Episodio 345 exitoso, recompensa: 253.07181617701565\n",
      "Episodio 349 exitoso, recompensa: 282.5141748256174\n",
      "Episodio 352 exitoso, recompensa: 249.5120728619592\n",
      "Episodio 353 exitoso, recompensa: 214.08336495611763\n",
      "Episodio 360 exitoso, recompensa: 272.4160097098701\n",
      "Episodio 365 exitoso, recompensa: 217.81285037892053\n",
      "Episodio 366 exitoso, recompensa: 235.95602962655667\n",
      "Episodio 372 exitoso, recompensa: 265.9449503262874\n",
      "Episodio 374 exitoso, recompensa: 250.3906999601018\n",
      "Episodio 390 exitoso, recompensa: 210.9372091972204\n",
      "Episodio 396 exitoso, recompensa: 242.7257533270429\n",
      "Episodio 401 exitoso, recompensa: 219.20226563412749\n",
      "Episodio 403 exitoso, recompensa: 249.71691599631018\n",
      "Episodio 412 exitoso, recompensa: 281.9641323214555\n",
      "Episodio 413 exitoso, recompensa: 258.3578123417373\n",
      "Episodio 418 exitoso, recompensa: 264.77440736352696\n",
      "Episodio 421 exitoso, recompensa: 232.2073234952063\n",
      "Episodio 426 exitoso, recompensa: 235.95635136159086\n",
      "Episodio 428 exitoso, recompensa: 256.5606874022978\n",
      "Episodio 433 exitoso, recompensa: 239.34335221733562\n",
      "Episodio 435 exitoso, recompensa: 273.7838116007332\n",
      "Episodio 445 exitoso, recompensa: 229.57393094156578\n",
      "Episodio 446 exitoso, recompensa: 222.5099440674083\n",
      "Episodio 453 exitoso, recompensa: 289.96741459316434\n",
      "Episodio 468 exitoso, recompensa: 217.0599678244924\n",
      "Episodio 479 exitoso, recompensa: 257.04609633870035\n",
      "Episodio 481 exitoso, recompensa: 214.47887777508402\n",
      "Episodio 485 exitoso, recompensa: 253.8619383967272\n",
      "Episodio 494 exitoso, recompensa: 268.6490043456975\n",
      "Episodio 500 exitoso, recompensa: 205.1451356088611\n",
      "Episodio 501 exitoso, recompensa: 256.16059103499856\n",
      "Episodio 510 exitoso, recompensa: 236.76781684235556\n",
      "Episodio 514 exitoso, recompensa: 261.6603720246289\n",
      "Episodio 519 exitoso, recompensa: 225.60950685131144\n",
      "Episodio 520 exitoso, recompensa: 266.85148328552873\n",
      "Episodio 522 exitoso, recompensa: 269.64320266949824\n",
      "Episodio 526 exitoso, recompensa: 272.2330082507509\n",
      "Episodio 530 exitoso, recompensa: 238.7947622218672\n",
      "Episodio 532 exitoso, recompensa: 287.0490625556406\n",
      "Episodio 538 exitoso, recompensa: 254.6971307031784\n",
      "Episodio 541 exitoso, recompensa: 268.0529198633626\n",
      "Episodio 543 exitoso, recompensa: 255.3804627684194\n",
      "Episodio 544 exitoso, recompensa: 207.52330745671026\n",
      "Episodio 548 exitoso, recompensa: 251.81818835692076\n",
      "Episodio 550 exitoso, recompensa: 238.64929271131476\n",
      "Episodio 557 exitoso, recompensa: 226.04896912737632\n",
      "Episodio 558 exitoso, recompensa: 222.8802800635438\n",
      "Episodio 559 exitoso, recompensa: 223.5500819249386\n",
      "Episodio 563 exitoso, recompensa: 234.75617737672832\n",
      "Episodio 572 exitoso, recompensa: 205.70019112306332\n",
      "Episodio 577 exitoso, recompensa: 200.5711254617962\n",
      "Episodio 583 exitoso, recompensa: 263.528433470997\n",
      "Episodio 590 exitoso, recompensa: 206.27590757963327\n",
      "Episodio 596 exitoso, recompensa: 261.78176930602694\n",
      "Episodio 599 exitoso, recompensa: 246.26604897520193\n",
      "Episodio 604 exitoso, recompensa: 223.58162073079288\n",
      "Episodio 605 exitoso, recompensa: 201.99114657618694\n",
      "Episodio 606 exitoso, recompensa: 272.72479602703197\n",
      "Episodio 609 exitoso, recompensa: 289.8248908231675\n",
      "Episodio 616 exitoso, recompensa: 206.5719021433125\n",
      "Episodio 619 exitoso, recompensa: 204.78694832324254\n",
      "Episodio 624 exitoso, recompensa: 237.82187483492467\n",
      "Episodio 627 exitoso, recompensa: 241.10219364120596\n",
      "Episodio 628 exitoso, recompensa: 238.87966573137584\n",
      "Episodio 633 exitoso, recompensa: 208.9571981747413\n",
      "Episodio 638 exitoso, recompensa: 226.69720912182623\n",
      "Episodio 641 exitoso, recompensa: 254.74913776628878\n",
      "Episodio 646 exitoso, recompensa: 226.10176812770942\n",
      "Episodio 655 exitoso, recompensa: 225.38674824848937\n",
      "Episodio 656 exitoso, recompensa: 239.98778366678232\n",
      "Episodio 658 exitoso, recompensa: 209.60585225610959\n",
      "Episodio 661 exitoso, recompensa: 243.14302506415564\n",
      "Episodio 664 exitoso, recompensa: 226.0677301504427\n",
      "Episodio 672 exitoso, recompensa: 257.7389027454152\n",
      "Episodio 677 exitoso, recompensa: 246.5822644333909\n",
      "Episodio 679 exitoso, recompensa: 284.741333320504\n",
      "Episodio 680 exitoso, recompensa: 271.8935999537588\n",
      "Episodio 682 exitoso, recompensa: 274.8761998836453\n",
      "Episodio 687 exitoso, recompensa: 257.4514731252525\n",
      "Episodio 689 exitoso, recompensa: 254.38326314993947\n",
      "Episodio 703 exitoso, recompensa: 258.469171863095\n",
      "Episodio 711 exitoso, recompensa: 245.798901281275\n",
      "Episodio 713 exitoso, recompensa: 216.23567314157324\n",
      "Episodio 714 exitoso, recompensa: 232.56000631696907\n",
      "Episodio 716 exitoso, recompensa: 224.76435912741545\n",
      "Episodio 718 exitoso, recompensa: 241.3363897590247\n",
      "Episodio 720 exitoso, recompensa: 212.86296057838553\n",
      "Episodio 722 exitoso, recompensa: 207.8693311403995\n",
      "Episodio 743 exitoso, recompensa: 236.7644174983177\n",
      "Episodio 754 exitoso, recompensa: 225.85364283282354\n",
      "Episodio 760 exitoso, recompensa: 230.38679438907232\n",
      "Episodio 761 exitoso, recompensa: 237.896685479455\n",
      "Episodio 767 exitoso, recompensa: 215.54809808866383\n",
      "Episodio 770 exitoso, recompensa: 251.5210152352547\n",
      "Episodio 773 exitoso, recompensa: 253.20443139459036\n",
      "Episodio 778 exitoso, recompensa: 274.95869041997435\n",
      "Episodio 780 exitoso, recompensa: 232.33394319700307\n",
      "Episodio 783 exitoso, recompensa: 222.01632464330524\n",
      "Episodio 784 exitoso, recompensa: 273.3010958719052\n",
      "Episodio 786 exitoso, recompensa: 253.08247904924715\n",
      "Episodio 789 exitoso, recompensa: 232.1320875854939\n",
      "Episodio 791 exitoso, recompensa: 254.01272005369526\n",
      "Episodio 794 exitoso, recompensa: 273.41386651146274\n",
      "Episodio 801 exitoso, recompensa: 248.0544944367722\n",
      "Episodio 803 exitoso, recompensa: 204.1164636418847\n",
      "Episodio 807 exitoso, recompensa: 225.41168130501455\n",
      "Episodio 813 exitoso, recompensa: 238.95144858476345\n",
      "Episodio 815 exitoso, recompensa: 265.69963598412755\n",
      "Episodio 818 exitoso, recompensa: 221.79380433740033\n",
      "Episodio 830 exitoso, recompensa: 234.9478788078669\n",
      "Episodio 836 exitoso, recompensa: 243.70658138687486\n",
      "Episodio 837 exitoso, recompensa: 201.4445405410043\n",
      "Episodio 838 exitoso, recompensa: 231.75249557857222\n",
      "Episodio 847 exitoso, recompensa: 242.665684358035\n",
      "Episodio 850 exitoso, recompensa: 257.04225214838743\n",
      "Episodio 852 exitoso, recompensa: 228.45251813774496\n",
      "Episodio 862 exitoso, recompensa: 209.21751166728882\n",
      "Episodio 867 exitoso, recompensa: 251.05196727054022\n",
      "Episodio 868 exitoso, recompensa: 272.29418046886497\n",
      "Episodio 871 exitoso, recompensa: 248.18547115220292\n",
      "Episodio 875 exitoso, recompensa: 247.9430196753849\n",
      "Episodio 883 exitoso, recompensa: 241.72583506301348\n",
      "Episodio 896 exitoso, recompensa: 231.03853250103674\n",
      "Episodio 900 exitoso, recompensa: 249.94594110394146\n",
      "Episodio 913 exitoso, recompensa: 264.3033914593933\n",
      "Episodio 916 exitoso, recompensa: 212.58857871048144\n",
      "Episodio 918 exitoso, recompensa: 222.20955307559962\n",
      "Episodio 919 exitoso, recompensa: 297.7389242827538\n",
      "Episodio 921 exitoso, recompensa: 225.10716564066848\n",
      "Episodio 924 exitoso, recompensa: 244.66962332686649\n",
      "Episodio 932 exitoso, recompensa: 248.9458540454522\n",
      "Episodio 934 exitoso, recompensa: 204.47094986991794\n",
      "Episodio 936 exitoso, recompensa: 200.0148887427599\n",
      "Episodio 940 exitoso, recompensa: 283.2789606143203\n",
      "Episodio 943 exitoso, recompensa: 274.93896494090933\n",
      "Episodio 948 exitoso, recompensa: 270.7712475376083\n",
      "Episodio 955 exitoso, recompensa: 232.9708550515839\n",
      "Episodio 959 exitoso, recompensa: 233.75891558347428\n",
      "Episodio 960 exitoso, recompensa: 247.7309901398893\n",
      "Episodio 968 exitoso, recompensa: 225.00179359201746\n",
      "Episodio 978 exitoso, recompensa: 263.5839436604897\n",
      "Episodio 983 exitoso, recompensa: 230.56599303753802\n",
      "Episodio 985 exitoso, recompensa: 240.8253782115191\n",
      "Episodio 986 exitoso, recompensa: 249.34168049418744\n",
      "Episodio 994 exitoso, recompensa: 247.86242304191595\n",
      "Episodio 999 exitoso, recompensa: 215.2829114551745\n",
      "Tasa de éxito: 0.231. Se obtuvo 29.741304287730244 de recompensa, en promedio\n"
     ]
    }
   ],
   "source": [
    "# Nota: hay que transformar esta celda en código para ejecutar (Esc + y)\n",
    "# Advertencia: este bloque es un loop infinito si el agente se deja sin implementar\n",
    "\n",
    "exitos = 0\n",
    "recompensa_episodios = []\n",
    "num_episodios = 1000\n",
    "episodios_exitosos = []\n",
    "episodios_no_exitosos = []\n",
    "semillas_exitosas = []\n",
    "semillas_no_exitosas = []\n",
    "\n",
    "for i in range(num_episodios):\n",
    "    recompensa, episodio, semilla = ejecutar_episodio_2(agente, aprender=False)\n",
    "    # Los episodios se consideran exitosos si se obtuvo 200 o más de recompensa total\n",
    "    if recompensa >= 200:\n",
    "        print(f\"Episodio {i} exitoso, recompensa: {recompensa}\")\n",
    "        exitos += 1\n",
    "        episodios_exitosos.append(episodio)\n",
    "        semillas_exitosas.append(semilla)\n",
    "    else:\n",
    "        episodios_no_exitosos.append(episodio)\n",
    "        semillas_no_exitosas.append(semilla)\n",
    "    recompensa_episodios.append(recompensa)\n",
    "\n",
    "print(f\"Tasa de éxito: {exitos / num_episodios}. Se obtuvo {np.mean(recompensa_episodios)} de recompensa, en promedio\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57d296e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para reproducir un episodio específico\n",
    "def reproducir_episodio(agente, episodio, semilla, max_iteraciones=500):\n",
    "    entorno = gym.make('LunarLander-v2', render_mode='human').env\n",
    "    entorno.reset(seed=semilla)\n",
    "    iteraciones = 0\n",
    "    termino = False\n",
    "    truncado = False\n",
    "    estado_anterior, info = entorno.reset(seed=semilla)\n",
    "    while iteraciones < max_iteraciones and not termino and not truncado:\n",
    "        accion = episodio[iteraciones][1]\n",
    "        estado_siguiente, recompensa, termino, truncado, info = entorno.step(accion)\n",
    "        entorno.render()\n",
    "        estado_anterior = estado_siguiente\n",
    "        iteraciones += 1\n",
    "    entorno.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f00ed41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d136e405",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6cf54e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[ 1.39350827 -1.21862235 -3.4506183  -1.27089773]\n",
      "[10.43430877  9.81707643  9.2750835  10.05266936]\n",
      "[8.84609998 8.62141893 7.5509486  8.55439629]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[ 0.61454678  0.61453768 -0.32261262  0.61446548]\n"
     ]
    }
   ],
   "source": [
    "# Estado:\n",
    "# (x, y, x_vel, y_vel, theta, theta_vel, pie_izq_en_contacto, pie_derecho_en_contacto)\n",
    "# Nave cayendo rapidamente hacia abajo sin rotacion en la matriz Q\n",
    "estado = discretize_state([0, 1.5, 0, -5, 0, 0, 0, 0], bins)\n",
    "print(agente.q_table[estado])\n",
    "# Nave cayendo rapidamente hacia abajo sin rotacion en la matriz Q\n",
    "estado = discretize_state([0, 1.5, 0, -4, 0, 0, 0, 0], bins)\n",
    "print(agente.q_table[estado])\n",
    "# Nave cayendo rapidamente hacia abajo sin rotacion en la matriz Q\n",
    "estado = discretize_state([0, 0.5, 0, -2, 0, 0, 0, 0], bins)\n",
    "print(agente.q_table[estado])\n",
    "estado_final = discretize_state([0, 0, 0, 0, 0, 0, 1, 1], bins)\n",
    "print(agente.q_table[estado_final])\n",
    "\n",
    "estado_2 = discretize_state([0, 0, 0, 0, 0, 0, 0, 1], bins)\n",
    "print(agente.q_table[estado_2])\n",
    "\n",
    "estado_3 = discretize_state([0, 0, 0, 0, 0, 0, 1, 0], bins)\n",
    "print(agente.q_table[estado_3])\n",
    "\n",
    "# Aterriza pero no en el centro der\n",
    "estado_4 = discretize_state([1.5, 0, 0, 0, 0, 0, 0, 0], bins)\n",
    "print(agente.q_table[estado_4])\n",
    "\n",
    "# Aterriza en el centro izq\n",
    "estado_5 = discretize_state([-1.5, 0, 0, 0, 0, 0, 0, 0], bins)\n",
    "print(agente.q_table[estado_5])\n",
    "\n",
    "# Estado inicial\n",
    "estado_6 = discretize_state([0, 1.5, 0, 0, 0, 0, 0, 0], bins)\n",
    "print(agente.q_table[estado_6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e7b346-6f23-44e2-a645-e4548ab470ef",
   "metadata": {},
   "source": [
    "Analizar los resultados de la ejecución anterior, incluyendo:\n",
    " * Un análisis de los parámetros utilizados en el algoritmo (aprendizaje, política de exploración)\n",
    " * Un análisis de algunos 'cortes' de la matriz Q y la política (p.e. qué hace la nave cuando está cayendo rápidamente hacia abajo, sin rotación)\n",
    " * Un análisis de la evolución de la recompensa promedio\n",
    " * Un análisis de los casos de éxito\n",
    " * Un análisis de los casos en el que el agente falla\n",
    " * Qué limitante del agente de RL les parece que afecta más negativamente su desempeño. Cómo lo mejorarían? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a680260",
   "metadata": {},
   "source": [
    "# Análisis de resultados\n",
    "\n",
    "En la siguiente sección estaremos realizando un análisis de los resultados obtenidos tras la implementación y el entrenamiento de un agente de RL basado en Q Learning. En esta sección explicaremos cómo construimos el agente, en qué nos basamos, qué parámetros tuvimos en cuenta y por qué. También analizaremos algunas situaciones particulares del agente para ver su comportamiento en detalle, algunos casos de éxito y de falla, cómo evolucionó la recompensa promedio a medida que avanzaba el entrenamiento y qué limitantes vemos en el agente y cómo se podrían mitigar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7858dc3d",
   "metadata": {},
   "source": [
    "## Análisis de implementación y decisiones de diseño\n",
    "\n",
    "A la hora de construir nuestro agente RL basado en Q Learning, nos basamos fuertemente en el teórico del curso, utilizando la metodología vista en clase, y también en la documentación de gymnasium, en la cual explican, sobre otro ejemplo, como entrenarlo con Q Learning utilizando la librería. El link a la documentación utilizada es el siguiente: https://gymnasium.farama.org/tutorials/training_agents/blackjack_tutorial/.\n",
    "\n",
    "### Estrategia de acción a tomar\n",
    "\n",
    "Lo primero que decidimos sobre el modelo es cómo debiamos elegir una acción, dado que si siempre elegimos la mejor acción nos perdemos de encontrar acciones que no fueron descubiertas previamente, por lo que hay que encontrar un balance entre tomar decisiones azarosas y las mejores decisiones. Ante esto teníamos 2 opciones: \n",
    "- Soft Max: En esta estrategia, se asignan probabilidades a las diferentes acciones usando una función. Las acciones con valores de Q más altos tendrán mayores probabilidades de ser seleccionadas, pero incluso las acciones con valores menores todavía tienen una probabilidad no nula de ser escogidas, lo que genera que se suela elegir las acciones con valores de Q más altos, pero que con cierta probabilidad no nula se tomen otras acciones que pueden resultar beneficiosas.\n",
    "- Epsilon-Greedy: En esta estrategia, se toma un valor epsilon, que viene dado como parámetro, y se toma la mejor acción (mayor valor de la tabla Q para ese estado) con probabilidad 1-epsilon, y una decisión azarosa con probabilidad epsilon. De esta forma se asegura que tomemos acciones azarosas e informadas, lo que contribuye a un mejor entrenamiento.\n",
    "- Epsilon-Greedy con decaimiento: Esta estrategia es una variante de la estrategia Epsilon-Greedy que aparte del valor epsilon toma un valor de epsilon minimo y un valor de decaimiento de epsilon. Por cada episodio que pasa se reduce el epsilon restandole el valor de decaimiento, hasta que en algún episodio se llega al valor mínimo de epsilon, el cual se utiliza durante el resto de entrenamiento. Esto lo que genera es que al principio del entrenamiento tomemos más decisiones azarosas, ya que no tenemos tanto conocimiento del entorno aún, y a medida que pasa el tiempo y se asume que tenemos más información y podemos tomar más decisiones informadas, la probabilidad de tomar decisiones azarosas se reduce y en su lugar se toman decisiones informadas con respecto a los valores de la tabla Q.\n",
    "\n",
    "Ante estas tres opciones decidimos inclinarnos por la tercera opción: Epsilon-Greedy con decaimiento. Nos pareció una estrategia coherente, un poco mejor que el Epsilon-Greedy sin esta variante, y creímos que en términos de implementación no era muy compleja. Decidimos probar esta estrategia y en caso de notar un bajo desempeño, cambiar a Soft Max. Cómo se verá más adelante en este informe, tras ver que los resultados fueron óptimos, se mantuvo la decisión.\n",
    "\n",
    "### Hiperparámetros a utilizar y valores utilizados\n",
    "\n",
    "#### Epsilon, Epsilon Mínimo y Decaimiento de Epsilon\n",
    "\n",
    "Tras la decisión de la estrategia a utilizar, debíamos decidir qué valores utilizar para dichos parámetros (epsilon, decaimiento de epsilon y epsilon mínimo). Para el epsilon se tomó la decisión del valor de 1, ya que entendimos que cuando recién se comienza el entrenamiento queremos explorar lo más que se pueda, ya que no tenemos ningún tipo de información del entorno aún, por lo que la probabilidad conviene que sea 1, y que luego esta vaya disminuyendo linealmente. Cómo epsilon mínimo tomamos el valor de 0.05. Esta decisión se baso mayormente en experiencia. Sabíamos que queríamos un valor bajo de epsilon mínimo ya que una vez que tengamos la tabla Q bastante llena, queremos tomar decisiones informadas la mayor parte de las veces. Al principio probamos con un valor de 0.1, de 0.2, de 0.05, y así fuimos probando con distintos valores, llegando a que los mayores resultados se daban con el valor de 0.05, aunque la diferencia no era tanta, sí había una diferencia.\n",
    "Por último, debimos decidir sobre el valor del decaimiento de epsilon.\n",
    "Sobre este valor nos encontramos con un tema a la hora de la implementación, y fue que al principio utilizamos el decaimiento de epsilon como factor que se le multiplicaba al valor de epsilon hasta llegar al epsilon mínimo. Por lo que poniamos un valor fijo alto de decaimiento de epsilon (ej: 0.995). Vimos que los resultados no eran buenos y decidimos probar con valores paramétricos, utilizando la cantidad de episodios para decidir sobre el valor. Para esto se nos ocurrió la fórmula: decaimiento de epsilon = (epsilon - epsilon min) ^ (1/episodios), con la idea de que esto haría que el epsilon fuera decayendo y en el último episodio se llegara al valor mínimo de epsilon.\n",
    "Esta opción tampoco nos trajo buenos resultados, por lo que tras revisar la documentación brindada por gymnasium, vimos que estaba bien poner un valor parámetrico en función de la cantidad de episodios, pero que la disminución del epsilon con el factor de decaimiento era a través de una resta. Epsilon = Epsilon - decaimiento_epsilon. En este caso vimos que en la documentación utilizaban un valor de decaimiento descripto por la fórmula: decaimiento_epsilon = epsilon / (num_episodios / 2). Tras utilizar esta fórmula nuestros resultados mejoraron bastante. Probamos luego variar el valor de 2 en la fórmula por distintos múltiplos de 2, ya que entendimos que por cómo es su funcionamiento, cuanto más grande dicho valor, durante menos tiempo se mantiene el epsilon en un número alto, pero vimos que la fórmula detallada por la documentación fue la que nos dió mejores resultados en la práctica.\n",
    "\n",
    "#### Tasa de aprendizaje en ambiente no determinista\n",
    "\n",
    "Dado que nos encontramos en el problema de Lunar Lander, este es un problema continuo, donde los estados son los ejes horizontal, vertical y de rotación, sus velocidades y 2 booleanos que indican si las patas de la nave están en contacto con el suelo o no. Por más de que en un principio discretizamos el estado en una cantidad de bins, esto no quita que el problema deje de ser continuo, por lo que al trabajar con un problema continuo, nos va a suceder muchas veces que al tomar una misma acción desde un mismo estado caigamos en estados distintos. Esto tiene un problema si no lo tomamos en cuenta, y es que a la hora de llenar la tabla Q con los valores de recompensa, las recompensas no van a ser las mismas desde un estado y una acción ya que los resultados posteriores dependen del estado en el que se caiga posteriormente, y estos varían cada vez que los tomamos. \n",
    "Para esto se utiliza el hiperparámetro tasa de aprendizaje.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab48702f",
   "metadata": {},
   "source": [
    "## Análisis de \"cortes\" de la matriz Q y la política"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3825b32b",
   "metadata": {},
   "source": [
    "## Análisis de evolución de recompensa promedio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac3d75d",
   "metadata": {},
   "source": [
    "## Análisis de casos (Éxitos y Fallos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbccc3e1",
   "metadata": {},
   "source": [
    "### Reproducir episodios exitosos y no exitosos de los casos de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e7768b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducir un episodio específico\n",
    "episodio_especifico = 31  # Cambia este valor al episodio que quieras reproducir\n",
    "if episodio_especifico < len(episodios_exitosos):\n",
    "    reproducir_episodio(agente, episodios_exitosos[episodio_especifico], semillas_exitosas[episodio_especifico])\n",
    "else:\n",
    "    print(f\"No hay suficientes episodios exitosos para reproducir el episodio {episodio_especifico}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "addc484e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducir un episodio específico\n",
    "episodio_especifico = 203  # Cambia este valor al episodio que quieras reproducir\n",
    "if episodio_especifico < len(episodios_no_exitosos):\n",
    "    reproducir_episodio(agente, episodios_no_exitosos[episodio_especifico], semillas_exitosas[episodio_especifico])\n",
    "else:\n",
    "    print(f\"No hay suficientes episodios no exitosos para reproducir el episodio {episodio_especifico}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a63f6a",
   "metadata": {},
   "source": [
    "### Episodios Exitosos y no exitosos\n",
    "\n",
    "La indeterminación del entorno proviene de la discretización, ya que un mismo estado discretizado puede llevar a diferentes resultados dependiendo de pequeñas variaciones no capturadas por la discretización. A pesar de esta limitación, el agente ha aprendido a actuar para un (numero porcentaje) de casos dentro de los 1000 que utilizamos para las pruebas.\n",
    "\n",
    "Para los casos exitosos vemos que el agente tiene un buen control del propulsor principal para no permitir nunca la caida a una velocidad elevada y tener la oportunidad de acomodarse de manera horizontal hacia la meta. Esto siempre desde un comienzo es controlado, lo cual al encadenar un conjunto de buenas decisiones y dandole siempre el uso justo a los propulsores laterales termina llevandolo a una recompenza exitosa.\n",
    "\n",
    "Esto es vital al comienzo de la ejecución de acciones de la nave ya que notamos una gran sensibilidad ante las decisiones de propulsarse hacia los costados en momentos inoportunos como se vera corriendo los ejemplos de casos no exitosos.\n",
    "\n",
    "En cuanto a los episodios que no son exitosos podemos ver que una mala decision de propulsarse hacia un costado, siempre se vuelve muy dificil de corregir para el agente no pudiendo controlar su velocidad y saliendose del entorno dando un caso fallido teniendo tambien la variable de poder girar en su eje complejizando mas su recuperacion al \"camino\" correcto.\n",
    "\n",
    "Vemos en otros tantos episodios, que el agente se propulsa hacia arriba por demas, al comienzo de su ejecucion, y creemos que al \"salirse\" del entorno el caso ya se considera como fallido.\n",
    "\n",
    "Concluimos que la determinacion del exito del episodio, se basa mucho en las primeras decisiones que toma, dado a la sensibilidad que tienen las decisiones desde un comienzo y su poca habilidad de recuperacion ante malas decisiones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f7d407",
   "metadata": {},
   "source": [
    "## Limitantes del agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f93fe904-e691-42ff-8fd4-b360d08431cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analizar los resultados aqui\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
