{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d13bbe5-80d1-4879-bc47-d8e366f38456",
   "metadata": {},
   "source": [
    "# **Lunar Lander con Q-Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82127016-b75d-4621-a53e-8b2bb63cd3f8",
   "metadata": {},
   "source": [
    "### **1. Bibliotecas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6e0a19d-696b-440e-84eb-fe2b8761d955",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9831e9d4-7840-485c-bc38-af987a76de4f",
   "metadata": {},
   "source": [
    "### **2. Jugando a mano**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e744255-5e3f-4c17-b9dc-c6374c2f06c2",
   "metadata": {},
   "source": [
    "A continuación se puede jugar un episodio del lunar lander. Se controlan los motores con el teclado. Notar que solo se puede realizar una acción a la vez (que es parte del problema), y que en esta implementación, izq toma precedencia sobre derecha, que toma precedencia sobre el motor principal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84df193a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "from pygame.locals import *\n",
    "\n",
    "# Inicializar pygame (para el contro|l con el teclado) y el ambiente\n",
    "pygame.init()\n",
    "env = gym.make('LunarLander-v2', render_mode='human')\n",
    "env.reset()\n",
    "pygame.display.set_caption('Lunar Lander')\n",
    "\n",
    "clock = pygame.time.Clock()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    for event in pygame.event.get():\n",
    "        if event.type == QUIT:\n",
    "            done = True\n",
    "            break\n",
    "\n",
    "    keys = pygame.key.get_pressed()\n",
    "\n",
    "    # Map keys to actions\n",
    "    if keys[K_LEFT]:\n",
    "        action = 3  # Fire left orientation engine\n",
    "    elif keys[K_RIGHT]:\n",
    "        action = 1 # Fire right orientation engine\n",
    "    elif keys[K_UP]:\n",
    "        action = 2  # Fire main engine\n",
    "    else:\n",
    "        action = 0  # Do nothing\n",
    "\n",
    "    _, _, terminated, truncated, _ = env.step(action)\n",
    "    env.render()\n",
    "    clock.tick(10)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        done = True\n",
    "\n",
    "env.close()\n",
    "pygame.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d721a629-7f3f-4a70-83c0-e12f43b0f285",
   "metadata": {},
   "source": [
    "## **3. Discretizando el estado**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d35189-3a15-4ceb-b978-38226518092a",
   "metadata": {},
   "source": [
    "El estado consiste de posiciones y velocidades en (x,y,theta) y en información de contacto de los pies con la superficie.\n",
    "\n",
    "Como varios de estos son continuos, tenemos que discretizarlos para aplicar nuestro algoritmo de aprendizaje por refuerzo tabular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7b6bed3-12ba-4d92-8b09-3d3f8429217f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cuántos bins queremos por dimensión\n",
    "# Pueden considerar variar este parámetro\n",
    "bins_per_dim = 20\n",
    "\n",
    "# Estado:\n",
    "# (x, y, x_vel, ely_v, theta, theta_vel, pie_izq_en_contacto, pie_derecho_en_contacto)\n",
    "NUM_BINS = [bins_per_dim, bins_per_dim, bins_per_dim, bins_per_dim, bins_per_dim, bins_per_dim, 2, 2]\n",
    "\n",
    "env = gym.make('LunarLander-v2')\n",
    "env.reset()\n",
    "\n",
    "# Tomamos los rangos del env\n",
    "OBS_SPACE_HIGH = env.observation_space.high\n",
    "OBS_SPACE_LOW = env.observation_space.low\n",
    "OBS_SPACE_LOW[1] = 0 # Para la coordenada y (altura), no podemos ir más abajo que la zona dea aterrizae (que está en el 0, 0)\n",
    "\n",
    "# Los bins para cada dimensión\n",
    "bins = [\n",
    "    np.linspace(OBS_SPACE_LOW[i], OBS_SPACE_HIGH[i], NUM_BINS[i] - 1)\n",
    "    for i in range(len(NUM_BINS) - 2) # last two are binary\n",
    "]\n",
    "# Se recomienda observar los bins para entender su estructura\n",
    "#print (\"Bins: \", bins)\n",
    "\n",
    "def discretize_state(state, bins):\n",
    "    \"\"\"Discretize the continuous state into a tuple of discrete indices.\"\"\"\n",
    "    state_disc = list()\n",
    "    for i in range(len(state)):\n",
    "        if i >= len(bins):  # For binary features (leg contacts)\n",
    "            state_disc.append(int(state[i]))\n",
    "        else:\n",
    "            state_disc.append(\n",
    "                np.digitize(state[i], bins[i])\n",
    "            )\n",
    "    return tuple(state_disc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45bcc921-c5f6-4f06-9702-6e740b26fc6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(np.int64(10), np.int64(1), np.int64(10), np.int64(10), np.int64(10), np.int64(10), 1, 1)\n",
      "(np.int64(10), np.int64(19), np.int64(10), np.int64(10), np.int64(10), np.int64(10), 0, 0)\n"
     ]
    }
   ],
   "source": [
    "# Ejemplos\n",
    "print(discretize_state([0.0, 0.0, 0, 0, 0, 0, 1, 1], bins)) # En la zona de aterrizaje y quieto\n",
    "print(discretize_state([0, 1.5, 0, 0, 0, 0, 0, 0], bins)) # Comenzando la partida, arriba y en el centro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779c1576-eff2-4b3c-8069-f6d30243f1e6",
   "metadata": {},
   "source": [
    "## **4. Agentes y la interacción con el entorno**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d594f89c-70b1-4c7c-939f-d654a5263f1c",
   "metadata": {},
   "source": [
    "Vamos a definir una interfaz para nuestro agente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93e7059a-7b68-4b13-857b-16e96c5f9793",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agente:\n",
    "    def elegir_accion(self, estado, max_accion, explorar = True) -> int:\n",
    "        \"\"\"Elegir la accion a tomar en el estado actual y el espacio de acciones\n",
    "            - estado_anterior: el estado desde que se empezó\n",
    "            - estado_siguiente: el estado al que se llegó\n",
    "            - accion: la acción que llevo al agente desde estado_anterior a estado_siguiente\n",
    "            - recompensa: la recompensa recibida en la transicion\n",
    "            - terminado: si el episodio terminó\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def aprender(self, estado_anterior, estado_siguiente, accion, recompensa, terminado):\n",
    "        \"\"\"Aprender a partir de la tupla \n",
    "            - estado_anterior: el estado desde que se empezó\n",
    "            - estado_siguiente: el estado al que se llegó\n",
    "            - accion: la acción que llevo al agente desde estado_anterior a estado_siguiente\n",
    "            - recompensa: la recompensa recibida en la transicion\n",
    "            - terminado: si el episodio terminó en esta transición\n",
    "        \"\"\"\n",
    "        pass\n",
    "    def fin_episodio(self):\n",
    "        \"\"\"Actualizar estructuras al final de un episodio\"\"\"\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c61e38-f7d3-40bf-af19-c7fefd143ffd",
   "metadata": {},
   "source": [
    "Para un agente aleatorio, la implementación sería:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3adcedd-b300-4e30-9cb1-19d5ec96fb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class AgenteAleatorio(Agente):\n",
    "    def elegir_accion(self, estado, max_accion, explorar = True) -> int:\n",
    "        # Elige una acción al azar\n",
    "        return random.randrange(max_accion)\n",
    "\n",
    "    def aprender(self, estado_anterior, estado_siguiente, accion, recompensa, terminado):\n",
    "        # No aprende\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19498b0d-eb74-431e-ac7f-612497ca07f3",
   "metadata": {},
   "source": [
    "Luego podemos definir una función para ejecutar un episodio con un agente dado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "038e9f56-f0cf-40b7-b3c3-a63b34572bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ejecutar_episodio(agente, aprender = True, render = None, max_iteraciones=500):\n",
    "    entorno = gym.make('LunarLander-v2').env\n",
    "\n",
    "    iteraciones = 0\n",
    "    recompensa_total = 0\n",
    "\n",
    "    termino = False\n",
    "    truncado = False\n",
    "    estado_anterior, info = entorno.reset()\n",
    "    while iteraciones < max_iteraciones and not termino and not truncado:\n",
    "        # Le pedimos al agente que elija entre las posibles acciones (0..entorno.action_space.n)\n",
    "        accion = agente.elegir_accion(estado_anterior, entorno.action_space.n, not aprender)\n",
    "        # Realizamos la accion\n",
    "        estado_siguiente, recompensa, termino, truncado, info = entorno.step(accion)\n",
    "        # Le informamos al agente para que aprenda\n",
    "        if (aprender):\n",
    "            agente.aprender(estado_anterior, estado_siguiente, accion, recompensa, termino)\n",
    "\n",
    "        estado_anterior = estado_siguiente\n",
    "        iteraciones += 1\n",
    "        recompensa_total += recompensa\n",
    "    if (aprender):\n",
    "        agente.fin_episodio()\n",
    "    entorno.close()\n",
    "\n",
    "    return recompensa_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29de7442-4565-4ce9-b442-a0e8f7c038a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(-433.0707472229256)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nota: hay que transformar esta celda en código para ejecutar (Esc + y)\n",
    "\n",
    "# Ejecutamos un episodio con el agente aleatorio y modo render 'human', para poder verlo\n",
    "ejecutar_episodio(AgenteAleatorio(), render = 'human')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b9cb2a-37b0-4115-afe8-bb89ce49f605",
   "metadata": {},
   "source": [
    "Podemos ejecutar este ambiente muchas veces y tomar métricas al respecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "536e81ee-6038-44c7-b887-35db442815ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasa de éxito: 0.0. Se obtuvo -184.9525852756085 de recompensa, en promedio\n"
     ]
    }
   ],
   "source": [
    "agente = AgenteAleatorio()\n",
    "recompensa_episodios = []\n",
    "\n",
    "exitos = 0\n",
    "num_episodios = 100\n",
    "for i in range(num_episodios):\n",
    "    recompensa = ejecutar_episodio(agente)\n",
    "    # Los episodios se consideran exitosos si se obutvo 200 o más de recompensa total\n",
    "    if (recompensa >= 200):\n",
    "        exitos += 1\n",
    "    recompensa_episodios += [recompensa]\n",
    "\n",
    "import numpy\n",
    "print(f\"Tasa de éxito: {exitos / num_episodios}. Se obtuvo {numpy.mean(recompensa_episodios)} de recompensa, en promedio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086cb250-7bc9-4bd4-a4cd-43cfd561facb",
   "metadata": {},
   "source": [
    "### **5. Programando un agente que aprende**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0827e3e2-a84a-462f-8b86-e03425cfc645",
   "metadata": {},
   "source": [
    "La tarea a realizar consiste en programar un agente de aprendizaje por refuerzos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3f37011-ffaa-4d08-b832-c45d0b9060da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class AgenteRL(Agente):\n",
    "    def __init__(self, bins, num_acciones, episodios, epsilon=1.0, epsilon_min=0.1):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon  # Factor de exploración\n",
    "        self.epsilon_min = epsilon_min  # Valor mínimo de epsilon\n",
    "        self.bins = bins  # Bins para discretizar el estado\n",
    "        self.q_table = np.zeros((*[len(b) + 1 for b in bins], 2, 2, num_acciones))  # Q-table\n",
    "        self.visit_counts = np.zeros_like(self.q_table)  # Tabla para contar visitas de estado-acción\n",
    "        self.epsilon_decay = self.epsilon_min**(1/episodios)  # Decaimiento de epsilon\n",
    "        print(self.epsilon_decay)\n",
    "    def elegir_accion(self, estado, max_accion, explorar=True) -> int:\n",
    "        # Discretizamos el estado antes de elegir una acción\n",
    "        estado_discreto = discretize_state(estado, self.bins)\n",
    "        \n",
    "        # Política epsilon-greedy para elegir acción\n",
    "        if explorar and np.random.rand() < self.epsilon:\n",
    "            # Explorar: elige una acción aleatoria\n",
    "            return np.random.randint(max_accion)\n",
    "        else:\n",
    "            # Explotar: elige la acción con el mayor valor Q para el estado actual\n",
    "            return np.argmax(self.q_table[estado_discreto])\n",
    "\n",
    "    def aprender(self, estado_anterior, estado_siguiente, accion, recompensa, terminado):\n",
    "        # Discretizamos los estados antes de actualizar la Q-table\n",
    "        estado_anterior_discreto = discretize_state(estado_anterior, self.bins)\n",
    "        estado_siguiente_discreto = discretize_state(estado_siguiente, self.bins)\n",
    "        \n",
    "        # Incrementamos el contador de visitas para el par (estado, acción)\n",
    "        self.visit_counts[estado_anterior_discreto + (accion,)] += 1\n",
    "        \n",
    "        # Calculamos alpha dinámicamente como 1 / número de visitas\n",
    "        visitas = self.visit_counts[estado_anterior_discreto + (accion,)]\n",
    "        alpha = 1 / visitas\n",
    "        \n",
    "        # Actualiza la Q-table usando la ecuación de Q-Learning\n",
    "        max_accion_siguiente = np.max(self.q_table[estado_siguiente_discreto])  # Mejor acción posible en el siguiente estado\n",
    "        q_actual = self.q_table[estado_anterior_discreto + (accion,)]  # Valor actual de Q para el estado y la acción\n",
    "        \n",
    "        # Si el episodio terminó, no hay valor futuro, de lo contrario aplicamos la ecuación de Bellman\n",
    "        if terminado:\n",
    "            q_nuevo = recompensa  # Si terminó, la recompensa es lo único que importa\n",
    "        else:\n",
    "            q_nuevo = recompensa + max_accion_siguiente  # Bellman update\n",
    "\n",
    "        # Actualizamos la Q-table con el nuevo valor Q\n",
    "        self.q_table[estado_anterior_discreto + (accion,)] += alpha * (q_nuevo - q_actual)\n",
    "\n",
    "        # Reducimos epsilon para que exploremos menos con el tiempo (decaimiento)\n",
    "\n",
    "\n",
    "    def fin_episodio(self):\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "        # Función que se llama al final de cada episodio para realizar tareas adicionales si es necesario\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ee51f5-aff8-4d18-934e-92acdcb617c0",
   "metadata": {},
   "source": [
    "Y ejecutar con el muchos episodios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0524d5-0d12-46a8-8437-984b981fbae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999769744141629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 890/100000 [00:28<58:49, 28.08it/s, promedio=-155, recompensa=-89.2]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 883 exitoso, recompensa: 260.628244432988, Se obtuvo -155.48702700562959 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1779/100000 [00:58<57:45, 28.34it/s, promedio=-143, recompensa=-118]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 1773 exitoso, recompensa: 222.7882646133673, Se obtuvo -143.42896457746983 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3322/100000 [01:50<55:22, 29.10it/s, promedio=-132, recompensa=23.9]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 3316 exitoso, recompensa: 210.95005426873064, Se obtuvo -132.00566050969152 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 3551/100000 [01:57<54:28, 29.50it/s, promedio=-130, recompensa=-128]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 3544 exitoso, recompensa: 220.81082127363996, Se obtuvo -130.52328107255624 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6206/100000 [03:26<58:58, 26.51it/s, promedio=-124, recompensa=-226]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 6202 exitoso, recompensa: 226.4380553175409, Se obtuvo -124.02036021172115 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 6726/100000 [03:43<49:20, 31.51it/s, promedio=-123, recompensa=47]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 6719 exitoso, recompensa: 218.32306680926513, Se obtuvo -122.75301252993245 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8207/100000 [04:33<58:40, 26.07it/s, promedio=-120, recompensa=-204]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 8202 exitoso, recompensa: 261.11290830800544, Se obtuvo -120.43755829251631 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 8733/100000 [04:51<56:19, 27.01it/s, promedio=-120, recompensa=-113]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 8727 exitoso, recompensa: 234.5316955642149, Se obtuvo -119.65546249487139 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 8990/100000 [05:00<54:52, 27.64it/s, promedio=-119, recompensa=-102] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 8985 exitoso, recompensa: 253.95209562363542, Se obtuvo -119.05292304594582 de recompensa, en promedio\n",
      "Episodio 8987 exitoso, recompensa: 256.48345652473586, Se obtuvo -119.00267228886298 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 9637/100000 [05:22<45:42, 32.95it/s, promedio=-118, recompensa=-309]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 9632 exitoso, recompensa: 232.27127306624885, Se obtuvo -118.20078403529916 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 9738/100000 [05:26<50:55, 29.54it/s, promedio=-118, recompensa=-77.7]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 9731 exitoso, recompensa: 257.82496405131917, Se obtuvo -118.07757600925456 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 9807/100000 [05:28<58:17, 25.79it/s, promedio=-118, recompensa=-140]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 9802 exitoso, recompensa: 262.36763482793503, Se obtuvo -117.91348523589343 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 10216/100000 [05:43<55:26, 26.99it/s, promedio=-118, recompensa=-103]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 10211 exitoso, recompensa: 244.34750234238163, Se obtuvo -117.78227540686379 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 10742/100000 [06:00<45:22, 32.79it/s, promedio=-117, recompensa=-84.1]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 10736 exitoso, recompensa: 238.95115931212933, Se obtuvo -117.22059983795648 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 11488/100000 [06:27<1:01:45, 23.89it/s, promedio=-116, recompensa=-74.5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 11483 exitoso, recompensa: 233.69962019463284, Se obtuvo -116.2204944204511 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 12294/100000 [06:56<46:19, 31.55it/s, promedio=-116, recompensa=-89.4]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 12288 exitoso, recompensa: 237.28914888130885, Se obtuvo -115.58214041754457 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 13272/100000 [07:29<53:54, 26.82it/s, promedio=-115, recompensa=-36.3]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 13266 exitoso, recompensa: 220.21787193915358, Se obtuvo -115.15700173490767 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 13362/100000 [07:33<55:38, 25.95it/s, promedio=-115, recompensa=-91.8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 13357 exitoso, recompensa: 275.6089661399514, Se obtuvo -115.02700588034507 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 13755/100000 [07:47<55:08, 26.07it/s, promedio=-115, recompensa=59.9]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 13750 exitoso, recompensa: 247.8407610217761, Se obtuvo -114.97501275985444 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 13810/100000 [07:49<47:26, 30.28it/s, promedio=-115, recompensa=-269] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 13804 exitoso, recompensa: 244.32702585036031, Se obtuvo -114.85698821515734 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 14211/100000 [08:03<52:09, 27.41it/s, promedio=-115, recompensa=-73.3]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 14205 exitoso, recompensa: 205.11746692826853, Se obtuvo -114.59426316726092 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 14408/100000 [08:10<1:02:30, 22.82it/s, promedio=-114, recompensa=-287] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 14403 exitoso, recompensa: 207.43263232908816, Se obtuvo -114.320692512008 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 14755/100000 [08:23<49:55, 28.46it/s, promedio=-114, recompensa=-135]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 14750 exitoso, recompensa: 220.83303757515932, Se obtuvo -114.12996983057515 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 15300/100000 [08:42<51:38, 27.34it/s, promedio=-114, recompensa=-59.7]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 15296 exitoso, recompensa: 213.484646873455, Se obtuvo -113.78580932262686 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 15337/100000 [08:44<52:11, 27.04it/s, promedio=-114, recompensa=-250] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 15332 exitoso, recompensa: 257.66952215292486, Se obtuvo -113.73785415197041 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 15878/100000 [09:03<48:04, 29.16it/s, promedio=-113, recompensa=-50.2]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 15873 exitoso, recompensa: 247.71233204639316, Se obtuvo -113.47437069536366 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 16189/100000 [09:14<49:41, 28.11it/s, promedio=-113, recompensa=-129]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 16184 exitoso, recompensa: 267.43765378065615, Se obtuvo -113.15246989516628 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 16727/100000 [09:33<53:13, 26.07it/s, promedio=-113, recompensa=-44.8]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 16722 exitoso, recompensa: 250.1396817408288, Se obtuvo -112.85209108650065 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 16886/100000 [09:39<49:00, 28.26it/s, promedio=-113, recompensa=-90.3]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 16879 exitoso, recompensa: 226.5002365365529, Se obtuvo -112.75306898774187 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 17407/100000 [09:58<44:18, 31.07it/s, promedio=-112, recompensa=-50]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 17401 exitoso, recompensa: 223.92182733060653, Se obtuvo -112.48423384896128 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 17498/100000 [10:01<55:22, 24.83it/s, promedio=-112, recompensa=-88.7] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 17494 exitoso, recompensa: 216.69030927235724, Se obtuvo -112.40521763205994 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 17561/100000 [10:04<48:20, 28.43it/s, promedio=-112, recompensa=9.37] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 17555 exitoso, recompensa: 260.8761363142298, Se obtuvo -112.32589380012814 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 18256/100000 [10:29<46:15, 29.45it/s, promedio=-112, recompensa=-129]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 18250 exitoso, recompensa: 269.7115322531773, Se obtuvo -112.06884157102958 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▊        | 18515/100000 [10:38<47:00, 28.89it/s, promedio=-112, recompensa=-107] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 18508 exitoso, recompensa: 216.2770496576975, Se obtuvo -111.9791642303 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 19190/100000 [11:02<43:44, 30.79it/s, promedio=-112, recompensa=-7.73]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 19184 exitoso, recompensa: 250.6154999224641, Se obtuvo -111.57947121909723 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 19242/100000 [11:04<48:47, 27.58it/s, promedio=-112, recompensa=-10.4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 19236 exitoso, recompensa: 233.42957589969737, Se obtuvo -111.55236487925299 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 20561/100000 [11:54<49:26, 26.78it/s, promedio=-111, recompensa=-82.3]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 20555 exitoso, recompensa: 231.74557973408395, Se obtuvo -111.19002229848785 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 20852/100000 [12:04<44:19, 29.76it/s, promedio=-111, recompensa=-119]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 20848 exitoso, recompensa: 246.82725929089756, Se obtuvo -111.11538334964801 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 20872/100000 [12:05<40:58, 32.19it/s, promedio=-111, recompensa=-78.9] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 20866 exitoso, recompensa: 233.5402725395008, Se obtuvo -111.0636585277461 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 20948/100000 [12:07<42:37, 30.91it/s, promedio=-111, recompensa=-64.4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 20941 exitoso, recompensa: 228.0991114024607, Se obtuvo -111.06065594547997 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 21249/100000 [12:19<53:31, 24.52it/s, promedio=-111, recompensa=-41.8] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 21243 exitoso, recompensa: 202.30929426021447, Se obtuvo -110.90883891842071 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 21820/100000 [12:41<49:44, 26.19it/s, promedio=-111, recompensa=-121]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 21814 exitoso, recompensa: 266.08334799174474, Se obtuvo -110.74037841676872 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 22591/100000 [13:09<49:46, 25.92it/s, promedio=-111, recompensa=-333] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 22586 exitoso, recompensa: 277.8762601767337, Se obtuvo -110.56392750029276 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 22610/100000 [13:10<46:32, 27.71it/s, promedio=-111, recompensa=-54]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 22605 exitoso, recompensa: 262.85260850529244, Se obtuvo -110.55194423023467 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 23094/100000 [13:28<50:21, 25.45it/s, promedio=-110, recompensa=-146]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 23090 exitoso, recompensa: 220.32468120835804, Se obtuvo -110.43356420603705 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 23222/100000 [13:33<53:02, 24.13it/s, promedio=-110, recompensa=-105] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 23218 exitoso, recompensa: 267.70108389044697, Se obtuvo -110.40982864314422 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 24274/100000 [14:12<46:00, 27.44it/s, promedio=-110, recompensa=-104] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 24268 exitoso, recompensa: 229.2589099489195, Se obtuvo -110.15765064500624 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 25288/100000 [14:48<40:59, 30.38it/s, promedio=-110, recompensa=-95.2] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 25281 exitoso, recompensa: 220.2549718440203, Se obtuvo -110.04713390021038 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 25360/100000 [14:51<41:48, 29.76it/s, promedio=-110, recompensa=-90.4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 25354 exitoso, recompensa: 247.95637412447422, Se obtuvo -110.02274317255727 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 25449/100000 [14:54<40:59, 30.32it/s, promedio=-110, recompensa=-63.9]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 25443 exitoso, recompensa: 204.83365830384946, Se obtuvo -109.99655058925651 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 25651/100000 [15:01<43:04, 28.77it/s, promedio=-110, recompensa=-120]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 25646 exitoso, recompensa: 205.29777616294524, Se obtuvo -109.93256088334773 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 27066/100000 [15:53<38:24, 31.65it/s, promedio=-110, recompensa=-77]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 27060 exitoso, recompensa: 260.1118468376093, Se obtuvo -109.6051022261919 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 27274/100000 [16:01<42:05, 28.80it/s, promedio=-110, recompensa=25.7] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 27269 exitoso, recompensa: 251.4293648950944, Se obtuvo -109.566988966479 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 27460/100000 [16:07<43:19, 27.90it/s, promedio=-110, recompensa=-188] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 27455 exitoso, recompensa: 203.98929182908574, Se obtuvo -109.52230635272961 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 28195/100000 [16:34<46:06, 25.96it/s, promedio=-109, recompensa=-114]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 28190 exitoso, recompensa: 239.15223593814937, Se obtuvo -109.4591615230698 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 28656/100000 [16:52<40:59, 29.01it/s, promedio=-109, recompensa=-68.7]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 28649 exitoso, recompensa: 247.96175332801903, Se obtuvo -109.35689344627806 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 31041/100000 [18:20<46:00, 24.98it/s, promedio=-109, recompensa=-42.3]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 31037 exitoso, recompensa: 243.6535877200297, Se obtuvo -109.2200066945919 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 31453/100000 [18:37<44:56, 25.42it/s, promedio=-109, recompensa=-297] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 31449 exitoso, recompensa: 249.42083710544387, Se obtuvo -109.19140251536938 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 33116/100000 [19:37<36:40, 30.40it/s, promedio=-109, recompensa=-73.6]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 33109 exitoso, recompensa: 263.04506365591635, Se obtuvo -109.06820507215413 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▎      | 33702/100000 [19:58<41:20, 26.73it/s, promedio=-109, recompensa=-59.2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 33695 exitoso, recompensa: 256.8890894404929, Se obtuvo -109.05589459819728 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 33876/100000 [20:05<42:41, 25.81it/s, promedio=-109, recompensa=-128] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 33871 exitoso, recompensa: 223.52802542887426, Se obtuvo -109.01598767858079 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 34070/100000 [20:13<40:10, 27.36it/s, promedio=-109, recompensa=-119]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 34064 exitoso, recompensa: 247.37652985574871, Se obtuvo -109.03149640393394 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 34577/100000 [20:32<37:26, 29.12it/s, promedio=-109, recompensa=-68.7] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 34571 exitoso, recompensa: 255.44551537042383, Se obtuvo -108.94861911209217 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 35288/100000 [20:59<45:46, 23.56it/s, promedio=-109, recompensa=-186] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 35283 exitoso, recompensa: 217.46714817769703, Se obtuvo -108.86153218747673 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 35430/100000 [21:04<46:11, 23.30it/s, promedio=-109, recompensa=-28.2] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 35425 exitoso, recompensa: 251.28343658873928, Se obtuvo -108.83890897851356 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 35714/100000 [21:15<44:33, 24.05it/s, promedio=-109, recompensa=-145]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 35710 exitoso, recompensa: 261.7391850123091, Se obtuvo -108.78138726545698 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 35819/100000 [21:19<44:37, 23.97it/s, promedio=-109, recompensa=-49.8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 35814 exitoso, recompensa: 249.41977592793137, Se obtuvo -108.77644197801247 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 36225/100000 [21:34<45:34, 23.32it/s, promedio=-109, recompensa=-123] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 36220 exitoso, recompensa: 205.2138718552534, Se obtuvo -108.76254260772927 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▋      | 36347/100000 [21:39<42:49, 24.77it/s, promedio=-109, recompensa=0.987]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 36342 exitoso, recompensa: 222.93651374049827, Se obtuvo -108.69498701618167 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 36821/100000 [21:56<41:23, 25.44it/s, promedio=-109, recompensa=-130]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 36817 exitoso, recompensa: 245.05924469658453, Se obtuvo -108.59920060451849 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 38452/100000 [22:57<37:35, 27.29it/s, promedio=-108, recompensa=-132]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 38446 exitoso, recompensa: 278.3987561099417, Se obtuvo -108.41380369201605 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▊      | 38503/100000 [23:00<44:27, 23.06it/s, promedio=-108, recompensa=-118] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 38498 exitoso, recompensa: 230.8985057314297, Se obtuvo -108.38478365299214 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▊      | 38548/100000 [23:01<43:00, 23.81it/s, promedio=-108, recompensa=-73]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 38544 exitoso, recompensa: 226.94704938337276, Se obtuvo -108.36066764658968 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▊      | 38726/100000 [23:08<35:51, 28.48it/s, promedio=-108, recompensa=-164] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 38721 exitoso, recompensa: 263.1239864573208, Se obtuvo -108.31345809495086 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 39442/100000 [23:35<38:09, 26.45it/s, promedio=-108, recompensa=18.3] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 39436 exitoso, recompensa: 236.33166509137763, Se obtuvo -108.23589060870678 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 39824/100000 [23:50<42:56, 23.36it/s, promedio=-108, recompensa=53]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 39819 exitoso, recompensa: 217.60420598033798, Se obtuvo -108.1813211869884 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 40504/100000 [24:16<36:10, 27.41it/s, promedio=-108, recompensa=-151] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 40498 exitoso, recompensa: 204.97404557338965, Se obtuvo -108.03145139197918 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████▏     | 41255/100000 [24:44<34:34, 28.31it/s, promedio=-108, recompensa=-158]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 41250 exitoso, recompensa: 235.2473656901657, Se obtuvo -107.93433612582997 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████▏     | 41456/100000 [24:52<41:59, 23.24it/s, promedio=-108, recompensa=-117] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 41451 exitoso, recompensa: 233.05139965066556, Se obtuvo -107.9571358761535 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████▏     | 41466/100000 [24:53<39:31, 24.68it/s, promedio=-108, recompensa=-130] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 41461 exitoso, recompensa: 235.36328883124312, Se obtuvo -107.95707047912532 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 42560/100000 [25:34<34:45, 27.54it/s, promedio=-108, recompensa=-157]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 42555 exitoso, recompensa: 271.67703794113606, Se obtuvo -107.98207975083523 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 42846/100000 [25:45<40:19, 23.62it/s, promedio=-108, recompensa=-168] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 42842 exitoso, recompensa: 245.59796809414303, Se obtuvo -107.92434961904775 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▎     | 43508/100000 [26:11<38:38, 24.37it/s, promedio=-108, recompensa=-224] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 43503 exitoso, recompensa: 257.3908097169487, Se obtuvo -107.9219605722141 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▎     | 43547/100000 [26:12<32:19, 29.10it/s, promedio=-108, recompensa=-168] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 43541 exitoso, recompensa: 262.4182698856757, Se obtuvo -107.9259527028494 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 44244/100000 [26:39<35:51, 25.91it/s, promedio=-108, recompensa=-56.8] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 44239 exitoso, recompensa: 218.07012567510077, Se obtuvo -107.8504365093334 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 44307/100000 [26:41<37:26, 24.79it/s, promedio=-108, recompensa=-208] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 44303 exitoso, recompensa: 201.70749248745705, Se obtuvo -107.85483956474272 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▍     | 44627/100000 [26:53<34:01, 27.12it/s, promedio=-108, recompensa=-280]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 44622 exitoso, recompensa: 290.0854037373577, Se obtuvo -107.82529856249624 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▋     | 46339/100000 [27:59<40:35, 22.03it/s, promedio=-108, recompensa=-137]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 46335 exitoso, recompensa: 202.60153718468504, Se obtuvo -107.74689971137546 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▋     | 46437/100000 [28:02<36:27, 24.48it/s, promedio=-108, recompensa=-169] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 46431 exitoso, recompensa: 263.85080198021996, Se obtuvo -107.73899386139328 de recompensa, en promedio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 47770/100000 [28:55<32:51, 26.50it/s, promedio=-108, recompensa=8.48]   "
     ]
    }
   ],
   "source": [
    "# Nota: hay que transformar esta celda en código para ejecutar (Esc + y)\n",
    "# Advertencia: este bloque es un loop infinito si el agente se deja sin implementar\n",
    "from tqdm import tqdm \n",
    "entorno = gym.make('LunarLander-v2').env\n",
    "num_episodios = 100000\n",
    "agente = AgenteRL(bins=bins, num_acciones=entorno.action_space.n, episodios=num_episodios)\n",
    "exitos = 0\n",
    "recompensa_episodios = []\n",
    "with tqdm(total=num_episodios) as pbar:\n",
    "    for i in range(num_episodios):\n",
    "        recompensa = ejecutar_episodio(agente)\n",
    "        if (recompensa >= 200):\n",
    "            print(f\"Episodio {i} exitoso, recompensa: {recompensa}, Se obtuvo {numpy.mean(recompensa_episodios)} de recompensa, en promedio\")\n",
    "            exitos += 1\n",
    "        recompensa_episodios += [recompensa]\n",
    "        pbar.set_postfix(recompensa=recompensa, promedio=numpy.mean(recompensa_episodios))\n",
    "        pbar.update(1)\n",
    "print(f\"Tasa de éxito: {exitos / num_episodios}. Se obtuvo {numpy.mean(recompensa_episodios)} de recompensa, en promedio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa860fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasa de éxito: 0.0. Se obtuvo -114.92617420767856 de recompensa, en promedio\n"
     ]
    }
   ],
   "source": [
    "# Nota: hay que transformar esta celda en código para ejecutar (Esc + y)\n",
    "# Advertencia: este bloque es un loop infinito si el agente se deja sin implementar\n",
    "\n",
    "entorno = gym.make('LunarLander-v2').env\n",
    "exitos = 0\n",
    "recompensa_episodios = []\n",
    "num_episodios = 1000\n",
    "for i in range(num_episodios):\n",
    "    recompensa = ejecutar_episodio(agente,aprender=False)\n",
    "    # Los episodios se consideran exitosos si se obutvo 200 o más de recompensa total\n",
    "    if (recompensa >= 200):\n",
    "        print(f\"Episodio {i} exitoso, recompensa: {recompensa}\")\n",
    "        exitos += 1\n",
    "    recompensa_episodios += [recompensa]\n",
    "print(f\"Tasa de éxito: {exitos / num_episodios}. Se obtuvo {numpy.mean(recompensa_episodios)} de recompensa, en promedio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e7b346-6f23-44e2-a645-e4548ab470ef",
   "metadata": {},
   "source": [
    "Analizar los resultados de la ejecución anterior, incluyendo:\n",
    " * Un análisis de los parámetros utilizados en el algoritmo (aprendizaje, política de exploración)\n",
    " * Un análisis de algunos 'cortes' de la matriz Q y la política (p.e. qué hace la nave cuando está cayendo rápidamente hacia abajo, sin rotación)\n",
    " * Un análisis de la evolución de la recompensa promedio\n",
    " * Un análisis de los casos de éxito\n",
    " * Un análisis de los casos en el que el agente falla\n",
    " * Qué limitante del agente de RL les parece que afecta más negativamente su desempeño. Cómo lo mejorarían? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f93fe904-e691-42ff-8fd4-b360d08431cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analizar los resultados aqui\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f723fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
