{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d13bbe5-80d1-4879-bc47-d8e366f38456",
   "metadata": {},
   "source": [
    "# **Lunar Lander con Q-Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82127016-b75d-4621-a53e-8b2bb63cd3f8",
   "metadata": {},
   "source": [
    "### **1. Bibliotecas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6e0a19d-696b-440e-84eb-fe2b8761d955",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# Tal vez tengan que ejecutar lo siguiente en sus máquinas (ubuntu 20.04)\n",
    "# sudo apt-get remove swig\n",
    "# sudo apt-get install swig3.0\n",
    "# sudo ln -s /usr/bin/swig3.0 /usr/bin/swig\n",
    "# En windows tambien puede ser necesario MSVC++\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9831e9d4-7840-485c-bc38-af987a76de4f",
   "metadata": {},
   "source": [
    "### **2. Jugando a mano**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e744255-5e3f-4c17-b9dc-c6374c2f06c2",
   "metadata": {},
   "source": [
    "A continuación se puede jugar un episodio del lunar lander. Se controlan los motores con el teclado. Notar que solo se puede realizar una acción a la vez (que es parte del problema), y que en esta implementación, izq toma precedencia sobre derecha, que toma precedencia sobre el motor principal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6211ed30-b1a3-4b8e-9858-17eee433ddca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nota: hay que transformar esta celda en código para ejecutar (Esc + y)\n",
    "\n",
    "import pygame\n",
    "from pygame.locals import *\n",
    "\n",
    "# Inicializar pygame (para el control con el teclado) y el ambiente\n",
    "pygame.init()\n",
    "env = gym.make('LunarLander-v2', render_mode='human')\n",
    "env.reset()\n",
    "pygame.display.set_caption('Lunar Lander')\n",
    "\n",
    "clock = pygame.time.Clock()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    for event in pygame.event.get():\n",
    "        if event.type == QUIT:\n",
    "            done = True\n",
    "            break\n",
    "\n",
    "    keys = pygame.key.get_pressed()\n",
    "\n",
    "    # Map keys to actions\n",
    "    if keys[K_LEFT]:\n",
    "        action = 3  # Fire left orientation engine\n",
    "    elif keys[K_RIGHT]:\n",
    "        action = 1 # Fire right orientation engine\n",
    "    elif keys[K_UP]:\n",
    "        action = 2  # Fire main engine\n",
    "    else:\n",
    "        action = 0  # Do nothing\n",
    "\n",
    "    _, _, terminated, truncated, _ = env.step(action)\n",
    "    env.render()\n",
    "    clock.tick(10)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        done = True\n",
    "\n",
    "env.close()\n",
    "pygame.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d721a629-7f3f-4a70-83c0-e12f43b0f285",
   "metadata": {},
   "source": [
    "## **3. Discretizando el estado**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d35189-3a15-4ceb-b978-38226518092a",
   "metadata": {},
   "source": [
    "El estado consiste de posiciones y velocidades en (x,y,theta) y en información de contacto de los pies con la superficie.\n",
    "\n",
    "Como varios de estos son continuos, tenemos que discretizarlos para aplicar nuestro algoritmo de aprendizaje por refuerzo tabular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7b6bed3-12ba-4d92-8b09-3d3f8429217f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High:  [1.5       1.5       5.        5.        3.1415927 5.        1.\n",
      " 1.       ]\n",
      "Low:  [-1.5        0.        -5.        -5.        -3.1415927 -5.\n",
      " -0.        -0.       ]\n"
     ]
    }
   ],
   "source": [
    "# Cuántos bins queremos por dimensión\n",
    "# Pueden considerar variar este parámetro\n",
    "bins_per_dim = 15\n",
    "\n",
    "# Estado:\n",
    "# (x, y, x_vel, y_vel, theta, theta_vel, pie_izq_en_contacto, pie_derecho_en_contacto)\n",
    "NUM_BINS = [bins_per_dim, bins_per_dim, bins_per_dim, bins_per_dim, bins_per_dim, bins_per_dim, 2, 2]\n",
    "\n",
    "env = gym.make('LunarLander-v2')\n",
    "env.reset()\n",
    "\n",
    "# Tomamos los rangos del env\n",
    "OBS_SPACE_HIGH = env.observation_space.high\n",
    "OBS_SPACE_LOW = env.observation_space.low\n",
    "OBS_SPACE_LOW[1] = 0 # Para la coordenada y (altura), no podemos ir más abajo que la zona dea aterrizae (que está en el 0, 0)\n",
    "\n",
    "# Los bins para cada dimensión\n",
    "bins = [\n",
    "    np.linspace(OBS_SPACE_LOW[i], OBS_SPACE_HIGH[i], NUM_BINS[i] - 1)\n",
    "    for i in range(len(NUM_BINS) - 2) # last two are binary\n",
    "]\n",
    "# Se recomienda observar los bins para entender su estructura\n",
    "#print (\"Bins: \", bins)\n",
    "print(\"High: \", OBS_SPACE_HIGH)\n",
    "print(\"Low: \", OBS_SPACE_LOW)\n",
    "def discretize_state(state, bins):\n",
    "    \"\"\"Discretize the continuous state into a tuple of discrete indices.\"\"\"\n",
    "    state_disc = list()\n",
    "    for i in range(len(state)):\n",
    "        if i >= len(bins):  # For binary features (leg contacts)\n",
    "            state_disc.append(int(state[i]))\n",
    "        else:\n",
    "            state_disc.append(\n",
    "                np.digitize(state[i], bins[i])\n",
    "            )\n",
    "    return tuple(state_disc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45bcc921-c5f6-4f06-9702-6e740b26fc6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(np.int64(7), np.int64(1), np.int64(7), np.int64(7), np.int64(7), np.int64(7), 1, 1)\n",
      "(np.int64(7), np.int64(14), np.int64(7), np.int64(7), np.int64(7), np.int64(7), 0, 0)\n"
     ]
    }
   ],
   "source": [
    "# Ejemplos\n",
    "print(discretize_state([0.0, 0.0, 0, 0, 0, 0, 1, 1], bins)) # En la zona de aterrizaje y quieto\n",
    "print(discretize_state([0, 1.5, 0, 0, 0, 0, 0, 0], bins)) # Comenzando la partida, arriba y en el centro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779c1576-eff2-4b3c-8069-f6d30243f1e6",
   "metadata": {},
   "source": [
    "## **4. Agentes y la interacción con el entorno**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d594f89c-70b1-4c7c-939f-d654a5263f1c",
   "metadata": {},
   "source": [
    "Vamos a definir una interfaz para nuestro agente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93e7059a-7b68-4b13-857b-16e96c5f9793",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agente:\n",
    "    def elegir_accion(self, estado, max_accion, explorar = True) -> int:\n",
    "        \"\"\"Elegir la accion a tomar en el estado actual y el espacio de acciones\n",
    "            - estado_anterior: el estado desde que se empezó\n",
    "            - estado_siguiente: el estado al que se llegó\n",
    "            - accion: la acción que llevo al agente desde estado_anterior a estado_siguiente\n",
    "            - recompensa: la recompensa recibida en la transicion\n",
    "            - terminado: si el episodio terminó\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def aprender(self, estado_anterior, estado_siguiente, accion, recompensa, terminado):\n",
    "        \"\"\"Aprender a partir de la tupla \n",
    "            - estado_anterior: el estado desde que se empezó\n",
    "            - estado_siguiente: el estado al que se llegó\n",
    "            - accion: la acción que llevo al agente desde estado_anterior a estado_siguiente\n",
    "            - recompensa: la recompensa recibida en la transicion\n",
    "            - terminado: si el episodio terminó en esta transición\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def fin_episodio(self):\n",
    "        \"\"\"Actualizar estructuras al final de un episodio\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c61e38-f7d3-40bf-af19-c7fefd143ffd",
   "metadata": {},
   "source": [
    "Para un agente aleatorio, la implementación sería:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3adcedd-b300-4e30-9cb1-19d5ec96fb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class AgenteAleatorio(Agente):\n",
    "    def elegir_accion(self, estado, max_accion, explorar = True) -> int:\n",
    "        # Elige una acción al azar\n",
    "        return random.randrange(max_accion)\n",
    "\n",
    "    def aprender(self, estado_anterior, estado_siguiente, accion, recompensa, terminado):\n",
    "        # No aprende\n",
    "        pass\n",
    "\n",
    "    def fin_episodio(self):\n",
    "        # Nada que actualizar\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19498b0d-eb74-431e-ac7f-612497ca07f3",
   "metadata": {},
   "source": [
    "Luego podemos definir una función para ejecutar un episodio con un agente dado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "038e9f56-f0cf-40b7-b3c3-a63b34572bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ejecutar_episodio(agente, aprender = True, render = None, max_iteraciones=500):\n",
    "    entorno = gym.make('LunarLander-v2', render_mode=render).env\n",
    "    \n",
    "    iteraciones = 0\n",
    "    recompensa_total = 0\n",
    "\n",
    "    termino = False\n",
    "    truncado = False\n",
    "    estado_anterior, info = entorno.reset()\n",
    "    while iteraciones < max_iteraciones and not termino and not truncado:\n",
    "        # Le pedimos al agente que elija entre las posibles acciones (0..entorno.action_space.n)\n",
    "        accion = agente.elegir_accion(estado_anterior, entorno.action_space.n, aprender)\n",
    "        # Realizamos la accion\n",
    "        estado_siguiente, recompensa, termino, truncado, info = entorno.step(accion)\n",
    "        # Le informamos al agente para que aprenda\n",
    "        if (aprender):\n",
    "            agente.aprender(estado_anterior, estado_siguiente, accion, recompensa, termino)\n",
    "\n",
    "        estado_anterior = estado_siguiente\n",
    "        iteraciones += 1\n",
    "        recompensa_total += recompensa\n",
    "    if (aprender):\n",
    "        agente.fin_episodio()\n",
    "    entorno.close()\n",
    "    return recompensa_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a104779-f5e3-4bfa-b88b-fb44de195d49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(-104.25733799682678)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nota: hay que transformar esta celda en código para ejecutar (Esc + y)\n",
    "\n",
    "# Ejecutamos un episodio con el agente aleatorio y modo render 'human', para poder verlo\n",
    "ejecutar_episodio(AgenteAleatorio(), render = 'human')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b9cb2a-37b0-4115-afe8-bb89ce49f605",
   "metadata": {},
   "source": [
    "Podemos ejecutar este ambiente muchas veces y tomar métricas al respecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "536e81ee-6038-44c7-b887-35db442815ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasa de éxito: 0.0. Se obtuvo -169.90152325832003 de recompensa, en promedio\n"
     ]
    }
   ],
   "source": [
    "agente = AgenteAleatorio()\n",
    "recompensa_episodios = []\n",
    "\n",
    "exitos = 0\n",
    "num_episodios = 100\n",
    "for i in range(num_episodios):\n",
    "    recompensa = ejecutar_episodio(agente)\n",
    "    # Los episodios se consideran exitosos si se obutvo 200 o más de recompensa total\n",
    "    if (recompensa >= 200):\n",
    "        exitos += 1\n",
    "    recompensa_episodios += [recompensa]\n",
    "\n",
    "import numpy\n",
    "print(f\"Tasa de éxito: {exitos / num_episodios}. Se obtuvo {numpy.mean(recompensa_episodios)} de recompensa, en promedio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086cb250-7bc9-4bd4-a4cd-43cfd561facb",
   "metadata": {},
   "source": [
    "### **5. Programando un agente que aprende**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0827e3e2-a84a-462f-8b86-e03425cfc645",
   "metadata": {},
   "source": [
    "La tarea a realizar consiste en programar un agente de aprendizaje por refuerzos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3f37011-ffaa-4d08-b832-c45d0b9060da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "class AgenteRL(Agente):\n",
    "    # Agregar código aqui\n",
    "\n",
    "    # Pueden agregar parámetros al constructor\n",
    "    def __init__(self, max_accion, bins, initial_epsilon: float, epsilon_decay: float, final_epsilon: float, discount_factor: float = 0.95):\n",
    "        super().__init__()\n",
    "        \"\"\"Initialize a Reinforcement Learning agent with an empty dictionary\n",
    "        of state-action values (q_values), a learning rate and an epsilon.\n",
    "\n",
    "        Args:\n",
    "            learning_rate: The learning rate\n",
    "            initial_epsilon: The initial epsilon value\n",
    "            epsilon_decay: The decay for epsilon\n",
    "            final_epsilon: The final epsilon value\n",
    "            discount_factor: The discount factor for computing the Q-value\n",
    "        \"\"\"\n",
    "        self.q_table = defaultdict(lambda: np.zeros(max_accion))\n",
    "        self.visitas = defaultdict(lambda: np.zeros(max_accion))\n",
    "\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "        self.epsilon = initial_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.final_epsilon = final_epsilon\n",
    "\n",
    "        self.bins = bins\n",
    "\n",
    "        self.training_error = []\n",
    "    \n",
    "    def elegir_accion(self, estado, max_accion, explorar = True) -> int:\n",
    "\n",
    "        estado_discreto = discretize_state(estado, self.bins)\n",
    "        if (explorar and np.random.random() < self.epsilon):\n",
    "            return random.randrange(max_accion)\n",
    "        else:\n",
    "            return int(np.argmax(self.q_table[estado_discreto]))\n",
    "    \n",
    "    def aprender(self, estado_anterior, estado_siguiente, accion, recompensa, terminado):\n",
    "        estado_anterior_discreto = discretize_state(estado_anterior, self.bins)\n",
    "        estado_siguiente_discreto = discretize_state(estado_siguiente, self.bins)\n",
    "\n",
    "        self.visitas[estado_anterior_discreto][accion] += 1\n",
    "\n",
    "        \"\"\"Updates the Q-value of an action.\"\"\"\n",
    "        future_q_value = (not terminado) * np.max(self.q_table[estado_siguiente_discreto])\n",
    "        temporal_difference = (\n",
    "            recompensa + self.discount_factor * future_q_value - self.q_table[estado_anterior_discreto][accion]\n",
    "        )\n",
    "\n",
    "        lr = 1 / self.visitas[estado_anterior_discreto][accion]\n",
    "\n",
    "        self.q_table[estado_anterior_discreto][accion] = (\n",
    "            self.q_table[estado_anterior_discreto][accion] + lr * temporal_difference\n",
    "        )\n",
    "        self.training_error.append(temporal_difference)\n",
    "\n",
    "    def fin_episodio(self):\n",
    "        self.epsilon = max(self.final_epsilon, self.epsilon - self.epsilon_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ee51f5-aff8-4d18-934e-92acdcb617c0",
   "metadata": {},
   "source": [
    "Y ejecutar con el muchos episodios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e0524d5-0d12-46a8-8437-984b981fbae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]/home/julio/.local/lib/python3.10/site-packages/numpy/_core/fromnumeric.py:3904: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/julio/.local/lib/python3.10/site-packages/numpy/_core/_methods.py:147: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "  0%|          | 9/10000 [00:00<02:36, 63.68it/s, promedio=-177, recompensa=-414] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo nan de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1013/10000 [00:16<02:21, 63.42it/s, promedio=-162, recompensa=-74.8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -163.12802467743523 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2011/10000 [00:35<02:32, 52.23it/s, promedio=-155, recompensa=-109]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -147.0510355111974 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3008/10000 [00:58<03:03, 38.15it/s, promedio=-146, recompensa=-283]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -128.09173633813037 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4006/10000 [01:28<03:09, 31.71it/s, promedio=-145, recompensa=-60.3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -141.23919507058133 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5003/10000 [02:06<04:05, 20.35it/s, promedio=-142, recompensa=-70.8]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -129.78717829849032 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6004/10000 [02:49<02:50, 23.39it/s, promedio=-133, recompensa=-47.8] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -90.65348238757313 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7005/10000 [03:31<02:08, 23.22it/s, promedio=-124, recompensa=-39.3] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -69.07272416387069 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8005/10000 [04:15<01:28, 22.54it/s, promedio=-116, recompensa=-69.5]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -62.106232901033714 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9003/10000 [05:00<00:51, 19.33it/s, promedio=-109, recompensa=-47.5]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -48.36376092504068 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [05:45<00:00, 28.94it/s, promedio=-101, recompensa=-94.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasa de éxito: 0.0351. Se obtuvo -101.47377887979772 de recompensa, en promedio\n"
     ]
    }
   ],
   "source": [
    "# Nota: hay que transformar esta celda en código para ejecutar (Esc + y)\n",
    "# Advertencia: este bloque es un loop infinito si el agente se deja sin implementar\n",
    "from tqdm import tqdm \n",
    "entorno = gym.make('LunarLander-v2').env\n",
    "num_episodios = 100000\n",
    "max_accion = entorno.action_space.n\n",
    "initial_epsilon = 1.0\n",
    "epsilon_decay = initial_epsilon / (num_episodios / 2)\n",
    "final_epsilon = 0.05\n",
    "discount_factor = 0.95\n",
    "\n",
    "agente = AgenteRL(max_accion = max_accion, bins = bins, initial_epsilon = initial_epsilon, epsilon_decay = epsilon_decay, final_epsilon = final_epsilon, discount_factor = discount_factor)\n",
    "exitos = 0\n",
    "recompensa_episodios = []\n",
    "with tqdm(total=num_episodios) as pbar:\n",
    "    for i in range(num_episodios):\n",
    "        recompensa = ejecutar_episodio(agente)\n",
    "        if i % 1000 == 0:\n",
    "            ultimas_1000_recompensas = recompensa_episodios[-1000:]\n",
    "            promedio_ultimas_1000 = np.mean(ultimas_1000_recompensas)\n",
    "            print(f\" Se obtuvo {promedio_ultimas_1000} de recompensa, en promedio en los últimos 1000 episodios\")\n",
    "        if (recompensa >= 200):\n",
    "            exitos += 1\n",
    "        recompensa_episodios += [recompensa]\n",
    "        pbar.set_postfix(recompensa=recompensa, promedio=numpy.mean(recompensa_episodios))\n",
    "        pbar.update(1)\n",
    "print(f\"Tasa de éxito: {exitos / num_episodios}. Se obtuvo {numpy.mean(recompensa_episodios)} de recompensa, en promedio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b7079e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 6 exitoso, recompensa: 268.13116411594643\n",
      "Episodio 14 exitoso, recompensa: 249.33242900160633\n",
      "Episodio 16 exitoso, recompensa: 301.0171947924978\n",
      "Episodio 19 exitoso, recompensa: 274.30589432994543\n",
      "Episodio 25 exitoso, recompensa: 212.29565155177\n",
      "Episodio 26 exitoso, recompensa: 260.4641392409631\n",
      "Episodio 27 exitoso, recompensa: 289.53369501354143\n",
      "Episodio 28 exitoso, recompensa: 228.50444231573678\n",
      "Episodio 30 exitoso, recompensa: 251.42573131790834\n",
      "Episodio 31 exitoso, recompensa: 273.96806238209587\n",
      "Episodio 32 exitoso, recompensa: 204.43916637090666\n",
      "Episodio 38 exitoso, recompensa: 254.1531165100468\n",
      "Episodio 42 exitoso, recompensa: 255.4817674462194\n",
      "Episodio 44 exitoso, recompensa: 244.32230250852126\n",
      "Episodio 45 exitoso, recompensa: 251.09426614509\n",
      "Episodio 46 exitoso, recompensa: 287.4614073373566\n",
      "Episodio 51 exitoso, recompensa: 267.4499579630059\n",
      "Episodio 55 exitoso, recompensa: 219.9848000729709\n",
      "Episodio 63 exitoso, recompensa: 231.71184796459937\n",
      "Episodio 72 exitoso, recompensa: 240.70072120626165\n",
      "Episodio 74 exitoso, recompensa: 273.2698309068802\n",
      "Episodio 75 exitoso, recompensa: 294.51658914835\n",
      "Episodio 77 exitoso, recompensa: 230.9195091764132\n",
      "Episodio 80 exitoso, recompensa: 233.5193584544902\n",
      "Episodio 81 exitoso, recompensa: 208.01700901636525\n",
      "Episodio 82 exitoso, recompensa: 278.85835309544694\n",
      "Episodio 83 exitoso, recompensa: 211.77479348485627\n",
      "Episodio 86 exitoso, recompensa: 222.90416541387495\n",
      "Episodio 88 exitoso, recompensa: 243.8974013097819\n",
      "Episodio 89 exitoso, recompensa: 228.3688069012587\n",
      "Episodio 94 exitoso, recompensa: 247.8863385467066\n",
      "Episodio 96 exitoso, recompensa: 285.56409730654843\n",
      "Episodio 98 exitoso, recompensa: 209.03515978156153\n",
      "Episodio 103 exitoso, recompensa: 251.8835857313478\n",
      "Episodio 104 exitoso, recompensa: 261.6882238522954\n",
      "Episodio 105 exitoso, recompensa: 218.9375869474477\n",
      "Episodio 109 exitoso, recompensa: 279.2532619971538\n",
      "Episodio 111 exitoso, recompensa: 283.2309815931559\n",
      "Episodio 114 exitoso, recompensa: 234.38066538976304\n",
      "Episodio 117 exitoso, recompensa: 298.76587962521137\n",
      "Episodio 120 exitoso, recompensa: 255.8949803369372\n",
      "Episodio 122 exitoso, recompensa: 209.95038881553387\n",
      "Episodio 126 exitoso, recompensa: 285.82764480726166\n",
      "Episodio 128 exitoso, recompensa: 222.55473498532837\n",
      "Episodio 130 exitoso, recompensa: 249.19031344761086\n",
      "Episodio 131 exitoso, recompensa: 278.812728879857\n",
      "Episodio 137 exitoso, recompensa: 285.0443009601978\n",
      "Episodio 138 exitoso, recompensa: 263.30115278927724\n",
      "Episodio 143 exitoso, recompensa: 256.94540195144873\n",
      "Episodio 144 exitoso, recompensa: 287.59114874993054\n",
      "Episodio 147 exitoso, recompensa: 253.69689813215078\n",
      "Episodio 156 exitoso, recompensa: 219.2250252434903\n",
      "Episodio 159 exitoso, recompensa: 238.9148845476326\n",
      "Episodio 161 exitoso, recompensa: 261.98494955780325\n",
      "Episodio 162 exitoso, recompensa: 279.2272985590166\n",
      "Episodio 164 exitoso, recompensa: 260.91559691485867\n",
      "Episodio 167 exitoso, recompensa: 236.53443376941357\n",
      "Episodio 168 exitoso, recompensa: 257.08503491665573\n",
      "Episodio 173 exitoso, recompensa: 244.2337585646417\n",
      "Episodio 175 exitoso, recompensa: 239.198416021887\n",
      "Episodio 176 exitoso, recompensa: 209.56018798298078\n",
      "Episodio 180 exitoso, recompensa: 231.68713433374234\n",
      "Episodio 181 exitoso, recompensa: 265.7728129758065\n",
      "Episodio 182 exitoso, recompensa: 211.99862607528087\n",
      "Episodio 185 exitoso, recompensa: 236.36341945310323\n",
      "Episodio 186 exitoso, recompensa: 271.71339051298594\n",
      "Episodio 192 exitoso, recompensa: 243.72794809984438\n",
      "Episodio 193 exitoso, recompensa: 240.3424721954907\n",
      "Episodio 194 exitoso, recompensa: 285.2482843772826\n",
      "Episodio 195 exitoso, recompensa: 247.46174179634335\n",
      "Episodio 199 exitoso, recompensa: 256.8895143967225\n",
      "Episodio 200 exitoso, recompensa: 275.37826853570505\n",
      "Episodio 201 exitoso, recompensa: 214.07798327641086\n",
      "Episodio 204 exitoso, recompensa: 228.6213683572606\n",
      "Episodio 205 exitoso, recompensa: 275.9855885978705\n",
      "Episodio 206 exitoso, recompensa: 307.6850772727594\n",
      "Episodio 208 exitoso, recompensa: 266.74913051434754\n",
      "Episodio 224 exitoso, recompensa: 269.12440161284076\n",
      "Episodio 226 exitoso, recompensa: 265.79886996566074\n",
      "Episodio 233 exitoso, recompensa: 218.43079823855865\n",
      "Episodio 234 exitoso, recompensa: 277.15138578104813\n",
      "Episodio 235 exitoso, recompensa: 273.94047138702547\n",
      "Episodio 240 exitoso, recompensa: 253.67631160175958\n",
      "Episodio 246 exitoso, recompensa: 226.98293180294067\n",
      "Episodio 248 exitoso, recompensa: 246.67176884165556\n",
      "Episodio 252 exitoso, recompensa: 237.54266415056833\n",
      "Episodio 253 exitoso, recompensa: 208.00875715429794\n",
      "Episodio 254 exitoso, recompensa: 261.36402747192614\n",
      "Episodio 260 exitoso, recompensa: 200.28683951449196\n",
      "Episodio 264 exitoso, recompensa: 256.5974657586927\n",
      "Episodio 267 exitoso, recompensa: 204.2827879135705\n",
      "Episodio 268 exitoso, recompensa: 259.1993309735734\n",
      "Episodio 272 exitoso, recompensa: 242.1759082153862\n",
      "Episodio 276 exitoso, recompensa: 277.5193197845094\n",
      "Episodio 285 exitoso, recompensa: 266.657295773192\n",
      "Episodio 287 exitoso, recompensa: 207.55227147609816\n",
      "Episodio 289 exitoso, recompensa: 283.15702221409913\n",
      "Episodio 298 exitoso, recompensa: 284.17027259213904\n",
      "Episodio 299 exitoso, recompensa: 227.64060278676146\n",
      "Episodio 301 exitoso, recompensa: 229.03512923246893\n",
      "Episodio 308 exitoso, recompensa: 244.33256248108094\n",
      "Episodio 312 exitoso, recompensa: 247.6034716782427\n",
      "Episodio 317 exitoso, recompensa: 229.36849332455571\n",
      "Episodio 318 exitoso, recompensa: 211.24075083704608\n",
      "Episodio 319 exitoso, recompensa: 249.1015958559139\n",
      "Episodio 321 exitoso, recompensa: 257.3907234644579\n",
      "Episodio 322 exitoso, recompensa: 262.6785426416771\n",
      "Episodio 325 exitoso, recompensa: 242.0045647039214\n",
      "Episodio 326 exitoso, recompensa: 257.45519191289566\n",
      "Episodio 332 exitoso, recompensa: 216.6599043408597\n",
      "Episodio 334 exitoso, recompensa: 283.34144013075934\n",
      "Episodio 339 exitoso, recompensa: 228.72578048018323\n",
      "Episodio 341 exitoso, recompensa: 286.164264383176\n",
      "Episodio 352 exitoso, recompensa: 241.57486669413848\n",
      "Episodio 353 exitoso, recompensa: 262.9099207261763\n",
      "Episodio 354 exitoso, recompensa: 246.48011175098608\n",
      "Episodio 357 exitoso, recompensa: 271.75399204838493\n",
      "Episodio 358 exitoso, recompensa: 248.27591846004387\n",
      "Episodio 362 exitoso, recompensa: 268.24127424982817\n",
      "Episodio 365 exitoso, recompensa: 241.95677834888954\n",
      "Episodio 367 exitoso, recompensa: 208.7471923049273\n",
      "Episodio 371 exitoso, recompensa: 250.67292703759364\n",
      "Episodio 373 exitoso, recompensa: 211.51666215071253\n",
      "Episodio 377 exitoso, recompensa: 210.1275729181872\n",
      "Episodio 378 exitoso, recompensa: 289.5066307594429\n",
      "Episodio 382 exitoso, recompensa: 201.61948063205065\n",
      "Episodio 384 exitoso, recompensa: 234.3678656994567\n",
      "Episodio 385 exitoso, recompensa: 247.58994358950446\n",
      "Episodio 387 exitoso, recompensa: 246.56270752475712\n",
      "Episodio 390 exitoso, recompensa: 261.0410808348872\n",
      "Episodio 395 exitoso, recompensa: 233.70349613597355\n",
      "Episodio 397 exitoso, recompensa: 286.70660785866534\n",
      "Episodio 398 exitoso, recompensa: 230.80743323868901\n",
      "Episodio 399 exitoso, recompensa: 237.04495430836644\n",
      "Episodio 401 exitoso, recompensa: 243.91257960086784\n",
      "Episodio 404 exitoso, recompensa: 259.6707750889391\n",
      "Episodio 408 exitoso, recompensa: 249.66066857019553\n",
      "Episodio 411 exitoso, recompensa: 242.61392090297304\n",
      "Episodio 412 exitoso, recompensa: 230.48513364652945\n",
      "Episodio 414 exitoso, recompensa: 238.3076529943057\n",
      "Episodio 415 exitoso, recompensa: 210.942876934489\n",
      "Episodio 416 exitoso, recompensa: 211.31970469079118\n",
      "Episodio 417 exitoso, recompensa: 269.79427923074076\n",
      "Episodio 426 exitoso, recompensa: 249.13400323669518\n",
      "Episodio 427 exitoso, recompensa: 216.98839921810708\n",
      "Episodio 429 exitoso, recompensa: 265.4650996310846\n",
      "Episodio 430 exitoso, recompensa: 223.01173538436365\n",
      "Episodio 433 exitoso, recompensa: 288.47763543017334\n",
      "Episodio 439 exitoso, recompensa: 207.04388755708428\n",
      "Episodio 441 exitoso, recompensa: 252.91897050393382\n",
      "Episodio 443 exitoso, recompensa: 260.6091060220092\n",
      "Episodio 444 exitoso, recompensa: 213.23356324047978\n",
      "Episodio 452 exitoso, recompensa: 224.99856200599444\n",
      "Episodio 453 exitoso, recompensa: 240.7305278490513\n",
      "Episodio 455 exitoso, recompensa: 264.65403610962113\n",
      "Episodio 460 exitoso, recompensa: 303.0256276789238\n",
      "Episodio 461 exitoso, recompensa: 270.9423530824705\n",
      "Episodio 463 exitoso, recompensa: 261.00919227402164\n",
      "Episodio 464 exitoso, recompensa: 245.0707645511376\n",
      "Episodio 466 exitoso, recompensa: 281.4236643125764\n",
      "Episodio 467 exitoso, recompensa: 221.58592682836652\n",
      "Episodio 468 exitoso, recompensa: 246.82055332150566\n",
      "Episodio 469 exitoso, recompensa: 235.52036878813266\n",
      "Episodio 473 exitoso, recompensa: 303.99759868607396\n",
      "Episodio 479 exitoso, recompensa: 267.41312513051605\n",
      "Episodio 480 exitoso, recompensa: 211.62577845502392\n",
      "Episodio 482 exitoso, recompensa: 261.38668911669583\n",
      "Episodio 483 exitoso, recompensa: 275.43497507305756\n",
      "Episodio 487 exitoso, recompensa: 229.61971675975803\n",
      "Episodio 489 exitoso, recompensa: 294.4547507885929\n",
      "Episodio 500 exitoso, recompensa: 283.9367041635884\n",
      "Episodio 501 exitoso, recompensa: 202.31322946234687\n",
      "Episodio 507 exitoso, recompensa: 289.29839908579936\n",
      "Episodio 510 exitoso, recompensa: 271.6596759596481\n",
      "Episodio 511 exitoso, recompensa: 257.3670348476247\n",
      "Episodio 512 exitoso, recompensa: 252.62575883445388\n",
      "Episodio 517 exitoso, recompensa: 205.5475083165885\n",
      "Episodio 519 exitoso, recompensa: 278.3421880335077\n",
      "Episodio 520 exitoso, recompensa: 228.54133395954418\n",
      "Episodio 522 exitoso, recompensa: 229.56677718233135\n",
      "Episodio 523 exitoso, recompensa: 219.47043419771967\n",
      "Episodio 525 exitoso, recompensa: 274.8551330710335\n",
      "Episodio 527 exitoso, recompensa: 205.38088076269491\n",
      "Episodio 529 exitoso, recompensa: 210.05564562472813\n",
      "Episodio 533 exitoso, recompensa: 236.3887149475184\n",
      "Episodio 537 exitoso, recompensa: 243.14745060306706\n",
      "Episodio 549 exitoso, recompensa: 231.7610783809858\n",
      "Episodio 557 exitoso, recompensa: 246.14527964936042\n",
      "Episodio 559 exitoso, recompensa: 261.5865773980962\n",
      "Episodio 560 exitoso, recompensa: 259.632272714191\n",
      "Episodio 562 exitoso, recompensa: 265.169855096163\n",
      "Episodio 564 exitoso, recompensa: 202.2895232531314\n",
      "Episodio 570 exitoso, recompensa: 206.77936446679416\n",
      "Episodio 575 exitoso, recompensa: 235.13773156375098\n",
      "Episodio 576 exitoso, recompensa: 231.26534678189634\n",
      "Episodio 577 exitoso, recompensa: 285.1207846361136\n",
      "Episodio 578 exitoso, recompensa: 241.04401556344922\n",
      "Episodio 579 exitoso, recompensa: 220.13483340845033\n",
      "Episodio 583 exitoso, recompensa: 244.54084145840028\n",
      "Episodio 584 exitoso, recompensa: 274.6142697664987\n",
      "Episodio 589 exitoso, recompensa: 240.6809806744415\n",
      "Episodio 593 exitoso, recompensa: 229.42309348225294\n",
      "Episodio 594 exitoso, recompensa: 305.55250641658455\n",
      "Episodio 597 exitoso, recompensa: 258.14410149184386\n",
      "Episodio 599 exitoso, recompensa: 226.21229532754415\n",
      "Episodio 600 exitoso, recompensa: 247.7110651094323\n",
      "Episodio 603 exitoso, recompensa: 213.80425817768347\n",
      "Episodio 604 exitoso, recompensa: 217.19346899458128\n",
      "Episodio 612 exitoso, recompensa: 272.84093340601476\n",
      "Episodio 617 exitoso, recompensa: 246.04449674408508\n",
      "Episodio 619 exitoso, recompensa: 267.92831665712157\n",
      "Episodio 620 exitoso, recompensa: 254.89356134696538\n",
      "Episodio 621 exitoso, recompensa: 214.75684716672058\n",
      "Episodio 622 exitoso, recompensa: 294.2582880260612\n",
      "Episodio 626 exitoso, recompensa: 212.52596684951413\n",
      "Episodio 628 exitoso, recompensa: 266.15945099622354\n",
      "Episodio 631 exitoso, recompensa: 280.01195285385575\n",
      "Episodio 634 exitoso, recompensa: 254.53671399627737\n",
      "Episodio 638 exitoso, recompensa: 234.46235605443266\n",
      "Episodio 642 exitoso, recompensa: 221.60443108100952\n",
      "Episodio 645 exitoso, recompensa: 289.74585722166\n",
      "Episodio 648 exitoso, recompensa: 268.2556272267906\n",
      "Episodio 649 exitoso, recompensa: 230.84426145168845\n",
      "Episodio 650 exitoso, recompensa: 278.6253764974282\n",
      "Episodio 654 exitoso, recompensa: 220.20708365049083\n",
      "Episodio 662 exitoso, recompensa: 200.87769429038738\n",
      "Episodio 666 exitoso, recompensa: 274.3950353557237\n",
      "Episodio 669 exitoso, recompensa: 250.6884927726883\n",
      "Episodio 671 exitoso, recompensa: 244.22073336377576\n",
      "Episodio 673 exitoso, recompensa: 267.00066339908994\n",
      "Episodio 676 exitoso, recompensa: 211.47848942995643\n",
      "Episodio 678 exitoso, recompensa: 249.83327370246954\n",
      "Episodio 685 exitoso, recompensa: 216.6208805281339\n",
      "Episodio 691 exitoso, recompensa: 236.5411378584865\n",
      "Episodio 694 exitoso, recompensa: 235.62102701337747\n",
      "Episodio 696 exitoso, recompensa: 224.3135200615593\n",
      "Episodio 697 exitoso, recompensa: 296.8558035686382\n",
      "Episodio 703 exitoso, recompensa: 211.17823480281533\n",
      "Episodio 704 exitoso, recompensa: 214.22988702584934\n",
      "Episodio 710 exitoso, recompensa: 206.47370961739233\n",
      "Episodio 711 exitoso, recompensa: 237.95446361028266\n",
      "Episodio 713 exitoso, recompensa: 282.4550705509996\n",
      "Episodio 716 exitoso, recompensa: 277.19938384627335\n",
      "Episodio 719 exitoso, recompensa: 234.42808534058923\n",
      "Episodio 725 exitoso, recompensa: 289.2813361357264\n",
      "Episodio 726 exitoso, recompensa: 246.56912875713778\n",
      "Episodio 730 exitoso, recompensa: 212.18150707385993\n",
      "Episodio 734 exitoso, recompensa: 268.6489593266266\n",
      "Episodio 735 exitoso, recompensa: 245.2050199812546\n",
      "Episodio 743 exitoso, recompensa: 261.0258353568996\n",
      "Episodio 746 exitoso, recompensa: 243.86966708009965\n",
      "Episodio 754 exitoso, recompensa: 274.0182650901012\n",
      "Episodio 756 exitoso, recompensa: 263.77423189215216\n",
      "Episodio 757 exitoso, recompensa: 244.87960647920275\n",
      "Episodio 758 exitoso, recompensa: 216.55224560101993\n",
      "Episodio 760 exitoso, recompensa: 248.29965056523014\n",
      "Episodio 761 exitoso, recompensa: 254.42740002782872\n",
      "Episodio 764 exitoso, recompensa: 274.0088596058425\n",
      "Episodio 765 exitoso, recompensa: 202.2693741828486\n",
      "Episodio 767 exitoso, recompensa: 230.418716068293\n",
      "Episodio 768 exitoso, recompensa: 245.98178328448793\n",
      "Episodio 771 exitoso, recompensa: 285.15055153715144\n",
      "Episodio 774 exitoso, recompensa: 236.1425060227502\n",
      "Episodio 775 exitoso, recompensa: 269.6138209724182\n",
      "Episodio 777 exitoso, recompensa: 236.80871046376117\n",
      "Episodio 784 exitoso, recompensa: 308.5163701650841\n",
      "Episodio 786 exitoso, recompensa: 221.65151747770165\n",
      "Episodio 787 exitoso, recompensa: 283.3352480981251\n",
      "Episodio 790 exitoso, recompensa: 255.2841121595119\n",
      "Episodio 798 exitoso, recompensa: 214.12254878193954\n",
      "Episodio 801 exitoso, recompensa: 200.78620194895623\n",
      "Episodio 808 exitoso, recompensa: 204.68495087291143\n",
      "Episodio 811 exitoso, recompensa: 257.9526497279926\n",
      "Episodio 817 exitoso, recompensa: 203.64070657697033\n",
      "Episodio 819 exitoso, recompensa: 223.2357992141986\n",
      "Episodio 828 exitoso, recompensa: 265.7143406738684\n",
      "Episodio 833 exitoso, recompensa: 221.14399786668196\n",
      "Episodio 842 exitoso, recompensa: 242.4419299758253\n",
      "Episodio 843 exitoso, recompensa: 275.110984719948\n",
      "Episodio 846 exitoso, recompensa: 278.7314022324045\n",
      "Episodio 848 exitoso, recompensa: 264.5611130415517\n",
      "Episodio 850 exitoso, recompensa: 242.5920261068882\n",
      "Episodio 855 exitoso, recompensa: 264.0173923573508\n",
      "Episodio 857 exitoso, recompensa: 264.50380566398115\n",
      "Episodio 858 exitoso, recompensa: 225.84432419206598\n",
      "Episodio 859 exitoso, recompensa: 259.10081463752124\n",
      "Episodio 860 exitoso, recompensa: 220.48753548762275\n",
      "Episodio 867 exitoso, recompensa: 260.823193477097\n",
      "Episodio 868 exitoso, recompensa: 232.5441560107807\n",
      "Episodio 874 exitoso, recompensa: 256.83523218880947\n",
      "Episodio 875 exitoso, recompensa: 243.06004379102615\n",
      "Episodio 880 exitoso, recompensa: 262.52348183141646\n",
      "Episodio 881 exitoso, recompensa: 229.97255746648017\n",
      "Episodio 884 exitoso, recompensa: 277.4014628915545\n",
      "Episodio 886 exitoso, recompensa: 234.85167895623255\n",
      "Episodio 887 exitoso, recompensa: 298.9125468817788\n",
      "Episodio 888 exitoso, recompensa: 221.511672124764\n",
      "Episodio 889 exitoso, recompensa: 251.1202930344331\n",
      "Episodio 891 exitoso, recompensa: 220.5604172202641\n",
      "Episodio 894 exitoso, recompensa: 261.1976396122761\n",
      "Episodio 895 exitoso, recompensa: 208.8796651400023\n",
      "Episodio 897 exitoso, recompensa: 276.0630009883387\n",
      "Episodio 900 exitoso, recompensa: 240.45715205128894\n",
      "Episodio 902 exitoso, recompensa: 243.96184282213264\n",
      "Episodio 905 exitoso, recompensa: 258.0780152534022\n",
      "Episodio 907 exitoso, recompensa: 296.4425979088258\n",
      "Episodio 908 exitoso, recompensa: 246.44002694927113\n",
      "Episodio 913 exitoso, recompensa: 239.70993690278954\n",
      "Episodio 915 exitoso, recompensa: 308.5955140181909\n",
      "Episodio 918 exitoso, recompensa: 207.9222999400357\n",
      "Episodio 920 exitoso, recompensa: 258.67160874256956\n",
      "Episodio 926 exitoso, recompensa: 276.2156565957565\n",
      "Episodio 928 exitoso, recompensa: 243.16342673257782\n",
      "Episodio 938 exitoso, recompensa: 220.73429275013356\n",
      "Episodio 940 exitoso, recompensa: 209.63919696738367\n",
      "Episodio 941 exitoso, recompensa: 290.1782060426084\n",
      "Episodio 944 exitoso, recompensa: 202.24291391048143\n",
      "Episodio 945 exitoso, recompensa: 273.63371024951346\n",
      "Episodio 946 exitoso, recompensa: 292.39578436714567\n",
      "Episodio 948 exitoso, recompensa: 299.2387361562038\n",
      "Episodio 949 exitoso, recompensa: 246.94447630532662\n",
      "Episodio 953 exitoso, recompensa: 223.35285864942085\n",
      "Episodio 954 exitoso, recompensa: 253.35269050624612\n",
      "Episodio 957 exitoso, recompensa: 229.24581542060807\n",
      "Episodio 958 exitoso, recompensa: 229.12076423524317\n",
      "Episodio 959 exitoso, recompensa: 221.60331790670432\n",
      "Episodio 964 exitoso, recompensa: 223.96069629491984\n",
      "Episodio 966 exitoso, recompensa: 256.83455239979793\n",
      "Episodio 967 exitoso, recompensa: 212.5265109839233\n",
      "Episodio 972 exitoso, recompensa: 287.7687186686795\n",
      "Episodio 975 exitoso, recompensa: 254.14368077295944\n",
      "Episodio 977 exitoso, recompensa: 236.0857245938455\n",
      "Episodio 979 exitoso, recompensa: 214.67342545609776\n",
      "Episodio 982 exitoso, recompensa: 273.6203729458964\n",
      "Episodio 984 exitoso, recompensa: 280.79884414069636\n",
      "Episodio 991 exitoso, recompensa: 214.47819461178116\n",
      "Episodio 992 exitoso, recompensa: 281.2690623115568\n",
      "Episodio 993 exitoso, recompensa: 200.92611057696928\n",
      "Tasa de éxito: 0.338. Se obtuvo 80.10615730295025 de recompensa, en promedio\n"
     ]
    }
   ],
   "source": [
    "# Nota: hay que transformar esta celda en código para ejecutar (Esc + y)\n",
    "# Advertencia: este bloque es un loop infinito si el agente se deja sin implementar\n",
    "\n",
    "entorno = gym.make('LunarLander-v2').env\n",
    "exitos = 0\n",
    "recompensa_episodios = []\n",
    "num_episodios = 1000\n",
    "for i in range(num_episodios):\n",
    "    recompensa = ejecutar_episodio(agente,aprender=False)\n",
    "    # Los episodios se consideran exitosos si se obutvo 200 o más de recompensa total\n",
    "    if (recompensa >= 200):\n",
    "        print(f\"Episodio {i} exitoso, recompensa: {recompensa}\")\n",
    "        exitos += 1\n",
    "    recompensa_episodios += [recompensa]\n",
    "print(f\"Tasa de éxito: {exitos / num_episodios}. Se obtuvo {numpy.mean(recompensa_episodios)} de recompensa, en promedio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6cf54e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'discretize_state' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Estado:\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# (x, y, x_vel, y_vel, theta, theta_vel, pie_izq_en_contacto, pie_derecho_en_contacto)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Nave cayendo rapidamente hacia abajo sin rotacion en la matriz Q\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m estado \u001b[38;5;241m=\u001b[39m \u001b[43mdiscretize_state\u001b[49m([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1.5\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m], bins)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(agente\u001b[38;5;241m.\u001b[39mq_table[estado])\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Nave cayendo rapidamente hacia abajo sin rotacion en la matriz Q\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'discretize_state' is not defined"
     ]
    }
   ],
   "source": [
    "# Estado:\n",
    "# (x, y, x_vel, y_vel, theta, theta_vel, pie_izq_en_contacto, pie_derecho_en_contacto)\n",
    "# Nave cayendo rapidamente hacia abajo sin rotacion en la matriz Q\n",
    "estado = discretize_state([0, 1.5, 0, -5, 0, 0, 0, 0], bins)\n",
    "print(agente.q_table[estado])\n",
    "# Nave cayendo rapidamente hacia abajo sin rotacion en la matriz Q\n",
    "estado = discretize_state([0, 1.5, 0, -4, 0, 0, 0, 0], bins)\n",
    "print(agente.q_table[estado])\n",
    "# Nave cayendo rapidamente hacia abajo sin rotacion en la matriz Q\n",
    "estado = discretize_state([0, 0.5, 0, -2, 0, 0, 0, 0], bins)\n",
    "print(agente.q_table[estado])\n",
    "estado_final = discretize_state([0, 0, 0, 0, 0, 0, 1, 1], bins)\n",
    "print(agente.q_table[estado_final])\n",
    "\n",
    "estado_2 = discretize_state([0, 0, 0, 0, 0, 0, 0, 1], bins)\n",
    "print(agente.q_table[estado_2])\n",
    "\n",
    "estado_3 = discretize_state([0, 0, 0, 0, 0, 0, 1, 0], bins)\n",
    "print(agente.q_table[estado_3])\n",
    "\n",
    "# Aterriza pero no en el centro der\n",
    "estado_4 = discretize_state([1.5, 0, 0, 0, 0, 0, 0, 0], bins)\n",
    "print(agente.q_table[estado_4])\n",
    "\n",
    "# Aterriza en el centro izq\n",
    "estado_5 = discretize_state([-1.5, 0, 0, 0, 0, 0, 0, 0], bins)\n",
    "print(agente.q_table[estado_5])\n",
    "\n",
    "# Estado inicial\n",
    "estado_6 = discretize_state([0, 1.5, 0, 0, 0, 0, 0, 0], bins)\n",
    "print(agente.q_table[estado_6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e7b346-6f23-44e2-a645-e4548ab470ef",
   "metadata": {},
   "source": [
    "Analizar los resultados de la ejecución anterior, incluyendo:\n",
    " * Un análisis de los parámetros utilizados en el algoritmo (aprendizaje, política de exploración)\n",
    " * Un análisis de algunos 'cortes' de la matriz Q y la política (p.e. qué hace la nave cuando está cayendo rápidamente hacia abajo, sin rotación)\n",
    " * Un análisis de la evolución de la recompensa promedio\n",
    " * Un análisis de los casos de éxito\n",
    " * Un análisis de los casos en el que el agente falla\n",
    " * Qué limitante del agente de RL les parece que afecta más negativamente su desempeño. Cómo lo mejorarían? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a680260",
   "metadata": {},
   "source": [
    "# Análisis de resultados\n",
    "\n",
    "En la siguiente sección estaremos realizando un análisis de los resultados obtenidos tras la implementación y el entrenamiento de un agente de RL basado en Q Learning. En esta sección explicaremos cómo construimos el agente, en qué nos basamos, qué parámetros tuvimos en cuenta y por qué. También analizaremos algunas situaciones particulares del agente para ver su comportamiento en detalle, algunos casos de éxito y de falla, cómo evolucionó la recompensa promedio a medida que avanzaba el entrenamiento y qué limitantes vemos en el agente y cómo se podrían mitigar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7858dc3d",
   "metadata": {},
   "source": [
    "## Análisis de implementación y decisiones de diseño\n",
    "\n",
    "A la hora de construir nuestro agente RL basado en Q Learning, nos basamos fuertemente en el teórico del curso, utilizando la metodología vista en clase, y también en la documentación de gymnasium, en la cual explican, sobre otro ejemplo, como entrenarlo con Q Learning utilizando la librería. El link a la documentación utilizada es el siguiente: https://gymnasium.farama.org/tutorials/training_agents/blackjack_tutorial/.\n",
    "\n",
    "### Estrategia de acción a tomar\n",
    "\n",
    "Lo primero que decidimos sobre el modelo es cómo debiamos elegir una acción, dado que si siempre elegimos la mejor acción nos perdemos de encontrar acciones que no fueron descubiertas previamente, por lo que hay que encontrar un balance entre tomar decisiones azarosas y las mejores decisiones. Ante esto teníamos 2 opciones: \n",
    "- Soft Max: En esta estrategia, se asignan probabilidades a las diferentes acciones usando una función. Las acciones con valores de Q más altos tendrán mayores probabilidades de ser seleccionadas, pero incluso las acciones con valores menores todavía tienen una probabilidad no nula de ser escogidas, lo que genera que se suela elegir las acciones con valores de Q más altos, pero que con cierta probabilidad no nula se tomen otras acciones que pueden resultar beneficiosas.\n",
    "- Epsilon-Greedy: En esta estrategia, se toma un valor epsilon, que viene dado como parámetro, y se toma la mejor acción (mayor valor de la tabla Q para ese estado) con probabilidad 1-epsilon, y una decisión azarosa con probabilidad epsilon. De esta forma se asegura que tomemos acciones azarosas e informadas, lo que contribuye a un mejor entrenamiento.\n",
    "- Epsilon-Greedy con decaimiento: Esta estrategia es una variante de la estrategia Epsilon-Greedy que aparte del valor epsilon toma un valor de epsilon minimo y un valor de decaimiento de epsilon. Por cada episodio que pasa se reduce el epsilon restandole el valor de decaimiento, hasta que en algún episodio se llega al valor mínimo de epsilon, el cual se utiliza durante el resto de entrenamiento. Esto lo que genera es que al principio del entrenamiento tomemos más decisiones azarosas, ya que no tenemos tanto conocimiento del entorno aún, y a medida que pasa el tiempo y se asume que tenemos más información y podemos tomar más decisiones informadas, la probabilidad de tomar decisiones azarosas se reduce y en su lugar se toman decisiones informadas con respecto a los valores de la tabla Q.\n",
    "\n",
    "Ante estas tres opciones decidimos inclinarnos por la tercera opción: Epsilon-Greedy con decaimiento. Nos pareció una estrategia coherente, un poco mejor que el Epsilon-Greedy sin esta variante, y creímos que en términos de implementación no era muy compleja. Decidimos probar esta estrategia y en caso de notar un bajo desempeño, cambiar a Soft Max. Cómo se verá más adelante en este informe, tras ver que los resultados fueron óptimos, se mantuvo la decisión.\n",
    "\n",
    "### Hiperparámetros a utilizar y valores utilizados\n",
    "\n",
    "#### Epsilon, Epsilon Mínimo y Decaimiento de Epsilon\n",
    "\n",
    "Tras la decisión de la estrategia a utilizar, debíamos decidir qué valores utilizar para dichos parámetros (epsilon, decaimiento de epsilon y epsilon mínimo). Para el epsilon se tomó la decisión del valor de 1, ya que entendimos que cuando recién se comienza el entrenamiento queremos explorar lo más que se pueda, ya que no tenemos ningún tipo de información del entorno aún, por lo que la probabilidad conviene que sea 1, y que luego esta vaya disminuyendo linealmente. Cómo epsilon mínimo tomamos el valor de 0.05. Esta decisión se baso mayormente en experiencia. Sabíamos que queríamos un valor bajo de epsilon mínimo ya que una vez que tengamos la tabla Q bastante llena, queremos tomar decisiones informadas la mayor parte de las veces. Al principio probamos con un valor de 0.1, de 0.2, de 0.05, y así fuimos probando con distintos valores, llegando a que los mayores resultados se daban con el valor de 0.05, aunque la diferencia no era tanta, sí había una diferencia.\n",
    "Por último, debimos decidir sobre el valor del decaimiento de epsilon.\n",
    "Sobre este valor nos encontramos con un tema a la hora de la implementación, y fue que al principio utilizamos el decaimiento de epsilon como factor que se le multiplicaba al valor de epsilon hasta llegar al epsilon mínimo. Por lo que poniamos un valor fijo alto de decaimiento de epsilon (ej: 0.995). Vimos que los resultados no eran buenos y decidimos probar con valores paramétricos, utilizando la cantidad de episodios para decidir sobre el valor. Para esto se nos ocurrió la fórmula: decaimiento de epsilon = (epsilon - epsilon min) ^ (1/episodios), con la idea de que esto haría que el epsilon fuera decayendo y en el último episodio se llegara al valor mínimo de epsilon.\n",
    "Esta opción tampoco nos trajo buenos resultados, por lo que tras revisar la documentación brindada por gymnasium, vimos que estaba bien poner un valor parámetrico en función de la cantidad de episodios, pero que la disminución del epsilon con el factor de decaimiento era a través de una resta. Epsilon = Epsilon - decaimiento_epsilon. En este caso vimos que en la documentación utilizaban un valor de decaimiento descripto por la fórmula: decaimiento_epsilon = epsilon / (num_episodios / 2). Tras utilizar esta fórmula nuestros resultados mejoraron bastante. Probamos luego variar el valor de 2 en la fórmula por distintos múltiplos de 2, ya que entendimos que por cómo es su funcionamiento, cuanto más grande dicho valor, durante menos tiempo se mantiene el epsilon en un número alto, pero vimos que la fórmula detallada por la documentación fue la que nos dió mejores resultados en la práctica.\n",
    "\n",
    "#### Tasa de aprendizaje en ambiente no determinista\n",
    "\n",
    "Dado que nos encontramos en el problema de Lunar Lander, este es un problema continuo, donde los estados son los ejes horizontal, vertical y de rotación, sus velocidades y 2 booleanos que indican si las patas de la nave están en contacto con el suelo o no. Por más de que en un principio discretizamos el estado en una cantidad de bins, esto no quita que el problema deje de ser continuo, por lo que al trabajar con un problema continuo, nos va a suceder muchas veces que al tomar una misma acción desde un mismo estado caigamos en estados distintos. Esto tiene un problema si no lo tomamos en cuenta, y es que a la hora de llenar la tabla Q con los valores de recompensa, las recompensas no van a ser las mismas desde un estado y una acción ya que los resultados posteriores dependen del estado en el que se caiga posteriormente, y estos varían cada vez que los tomamos. \n",
    "Para esto se utiliza el hiperparámetro tasa de aprendizaje.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab48702f",
   "metadata": {},
   "source": [
    "## Análisis de \"cortes\" de la matriz Q y la política"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3825b32b",
   "metadata": {},
   "source": [
    "## Análisis de evolución de recompensa promedio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac3d75d",
   "metadata": {},
   "source": [
    "## Análisis de casos (Éxitos y Fallos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f7d407",
   "metadata": {},
   "source": [
    "## Limitantes del agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f93fe904-e691-42ff-8fd4-b360d08431cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analizar los resultados aqui\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
