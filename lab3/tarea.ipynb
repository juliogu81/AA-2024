{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d13bbe5-80d1-4879-bc47-d8e366f38456",
   "metadata": {},
   "source": [
    "# **Lunar Lander con Q-Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82127016-b75d-4621-a53e-8b2bb63cd3f8",
   "metadata": {},
   "source": [
    "### **1. Bibliotecas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6e0a19d-696b-440e-84eb-fe2b8761d955",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# Tal vez tengan que ejecutar lo siguiente en sus máquinas (ubuntu 20.04)\n",
    "# sudo apt-get remove swig\n",
    "# sudo apt-get install swig3.0\n",
    "# sudo ln -s /usr/bin/swig3.0 /usr/bin/swig\n",
    "# En windows tambien puede ser necesario MSVC++\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9831e9d4-7840-485c-bc38-af987a76de4f",
   "metadata": {},
   "source": [
    "### **2. Jugando a mano**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e744255-5e3f-4c17-b9dc-c6374c2f06c2",
   "metadata": {},
   "source": [
    "A continuación se puede jugar un episodio del lunar lander. Se controlan los motores con el teclado. Notar que solo se puede realizar una acción a la vez (que es parte del problema), y que en esta implementación, izq toma precedencia sobre derecha, que toma precedencia sobre el motor principal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6211ed30-b1a3-4b8e-9858-17eee433ddca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nota: hay que transformar esta celda en código para ejecutar (Esc + y)\n",
    "\n",
    "import pygame\n",
    "from pygame.locals import *\n",
    "\n",
    "# Inicializar pygame (para el control con el teclado) y el ambiente\n",
    "pygame.init()\n",
    "env = gym.make('LunarLander-v2', render_mode='human')\n",
    "env.reset()\n",
    "pygame.display.set_caption('Lunar Lander')\n",
    "\n",
    "clock = pygame.time.Clock()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    for event in pygame.event.get():\n",
    "        if event.type == QUIT:\n",
    "            done = True\n",
    "            break\n",
    "\n",
    "    keys = pygame.key.get_pressed()\n",
    "\n",
    "    # Map keys to actions\n",
    "    if keys[K_LEFT]:\n",
    "        action = 3  # Fire left orientation engine\n",
    "    elif keys[K_RIGHT]:\n",
    "        action = 1 # Fire right orientation engine\n",
    "    elif keys[K_UP]:\n",
    "        action = 2  # Fire main engine\n",
    "    else:\n",
    "        action = 0  # Do nothing\n",
    "\n",
    "    _, _, terminated, truncated, _ = env.step(action)\n",
    "    env.render()\n",
    "    clock.tick(10)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        done = True\n",
    "\n",
    "env.close()\n",
    "pygame.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d721a629-7f3f-4a70-83c0-e12f43b0f285",
   "metadata": {},
   "source": [
    "## **3. Discretizando el estado**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d35189-3a15-4ceb-b978-38226518092a",
   "metadata": {},
   "source": [
    "El estado consiste de posiciones y velocidades en (x,y,theta) y en información de contacto de los pies con la superficie.\n",
    "\n",
    "Como varios de estos son continuos, tenemos que discretizarlos para aplicar nuestro algoritmo de aprendizaje por refuerzo tabular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7b6bed3-12ba-4d92-8b09-3d3f8429217f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High:  [1.5       1.5       5.        5.        3.1415927 5.        1.\n",
      " 1.       ]\n",
      "Low:  [-1.5        0.        -5.        -5.        -3.1415927 -5.\n",
      " -0.        -0.       ]\n"
     ]
    }
   ],
   "source": [
    "# Cuántos bins queremos por dimensión\n",
    "# Pueden considerar variar este parámetro\n",
    "bins_per_dim = 15\n",
    "\n",
    "# Estado:\n",
    "# (x, y, x_vel, y_vel, theta, theta_vel, pie_izq_en_contacto, pie_derecho_en_contacto)\n",
    "NUM_BINS = [bins_per_dim, bins_per_dim, bins_per_dim, bins_per_dim, bins_per_dim, bins_per_dim, 2, 2]\n",
    "\n",
    "env = gym.make('LunarLander-v2')\n",
    "env.reset()\n",
    "\n",
    "# Tomamos los rangos del env\n",
    "OBS_SPACE_HIGH = env.observation_space.high\n",
    "OBS_SPACE_LOW = env.observation_space.low\n",
    "OBS_SPACE_LOW[1] = 0 # Para la coordenada y (altura), no podemos ir más abajo que la zona dea aterrizae (que está en el 0, 0)\n",
    "\n",
    "# Los bins para cada dimensión\n",
    "bins = [\n",
    "    np.linspace(OBS_SPACE_LOW[i], OBS_SPACE_HIGH[i], NUM_BINS[i] - 1)\n",
    "    for i in range(len(NUM_BINS) - 2) # last two are binary\n",
    "]\n",
    "# Se recomienda observar los bins para entender su estructura\n",
    "#print (\"Bins: \", bins)\n",
    "print(\"High: \", OBS_SPACE_HIGH)\n",
    "print(\"Low: \", OBS_SPACE_LOW)\n",
    "def discretize_state(state, bins):\n",
    "    \"\"\"Discretize the continuous state into a tuple of discrete indices.\"\"\"\n",
    "    state_disc = list()\n",
    "    for i in range(len(state)):\n",
    "        if i >= len(bins):  # For binary features (leg contacts)\n",
    "            state_disc.append(int(state[i]))\n",
    "        else:\n",
    "            state_disc.append(\n",
    "                np.digitize(state[i], bins[i])\n",
    "            )\n",
    "    return tuple(state_disc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bcc921-c5f6-4f06-9702-6e740b26fc6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(np.int64(7), np.int64(1), np.int64(7), np.int64(7), np.int64(7), np.int64(7), 1, 1)\n",
      "(np.int64(7), np.int64(14), np.int64(7), np.int64(7), np.int64(7), np.int64(7), 0, 0)\n"
     ]
    }
   ],
   "source": [
    "# Ejemplos\n",
    "print(discretize_state([0.0, 0.0, 0, 0, 0, 0, 1, 1], bins)) # En la zona de aterrizaje y quieto\n",
    "print(discretize_state([0, 1.5, 0, 0, 0, 0, 0, 0], bins)) # Comenzando la partida, arriba y en el centro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779c1576-eff2-4b3c-8069-f6d30243f1e6",
   "metadata": {},
   "source": [
    "## **4. Agentes y la interacción con el entorno**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d594f89c-70b1-4c7c-939f-d654a5263f1c",
   "metadata": {},
   "source": [
    "Vamos a definir una interfaz para nuestro agente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93e7059a-7b68-4b13-857b-16e96c5f9793",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agente:\n",
    "    def elegir_accion(self, estado, max_accion, explorar = True) -> int:\n",
    "        \"\"\"Elegir la accion a tomar en el estado actual y el espacio de acciones\n",
    "            - estado_anterior: el estado desde que se empezó\n",
    "            - estado_siguiente: el estado al que se llegó\n",
    "            - accion: la acción que llevo al agente desde estado_anterior a estado_siguiente\n",
    "            - recompensa: la recompensa recibida en la transicion\n",
    "            - terminado: si el episodio terminó\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def aprender(self, estado_anterior, estado_siguiente, accion, recompensa, terminado):\n",
    "        \"\"\"Aprender a partir de la tupla \n",
    "            - estado_anterior: el estado desde que se empezó\n",
    "            - estado_siguiente: el estado al que se llegó\n",
    "            - accion: la acción que llevo al agente desde estado_anterior a estado_siguiente\n",
    "            - recompensa: la recompensa recibida en la transicion\n",
    "            - terminado: si el episodio terminó en esta transición\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def fin_episodio(self):\n",
    "        \"\"\"Actualizar estructuras al final de un episodio\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c61e38-f7d3-40bf-af19-c7fefd143ffd",
   "metadata": {},
   "source": [
    "Para un agente aleatorio, la implementación sería:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3adcedd-b300-4e30-9cb1-19d5ec96fb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class AgenteAleatorio(Agente):\n",
    "    def elegir_accion(self, estado, max_accion, explorar = True) -> int:\n",
    "        # Elige una acción al azar\n",
    "        return random.randrange(max_accion)\n",
    "\n",
    "    def aprender(self, estado_anterior, estado_siguiente, accion, recompensa, terminado):\n",
    "        # No aprende\n",
    "        pass\n",
    "\n",
    "    def fin_episodio(self):\n",
    "        # Nada que actualizar\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19498b0d-eb74-431e-ac7f-612497ca07f3",
   "metadata": {},
   "source": [
    "Luego podemos definir una función para ejecutar un episodio con un agente dado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "038e9f56-f0cf-40b7-b3c3-a63b34572bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ejecutar_episodio_2(agente, aprender=True, max_iteraciones=500):\n",
    "    entorno = gym.make('LunarLander-v2').env\n",
    "    semilla = entorno.np_random.bit_generator._seed_seq.entropy\n",
    "    iteraciones = 0\n",
    "    recompensa_total = 0\n",
    "    episodio = []\n",
    "\n",
    "    termino = False\n",
    "    truncado = False\n",
    "    estado_anterior, info = entorno.reset(seed=semilla)\n",
    "    while iteraciones < max_iteraciones and not termino and not truncado:\n",
    "        # Le pedimos al agente que elija entre las posibles acciones (0..entorno.action_space.n)\n",
    "        accion = agente.elegir_accion(estado_anterior, entorno.action_space.n, aprender)\n",
    "        # Realizamos la accion\n",
    "        estado_siguiente, recompensa, termino, truncado, info = entorno.step(accion)\n",
    "        # Le informamos al agente para que aprenda\n",
    "        if aprender:\n",
    "            agente.aprender(estado_anterior, estado_siguiente, accion, recompensa, termino)\n",
    "\n",
    "        # Almacenar el estado, acción y recompensa\n",
    "        episodio.append((estado_anterior, accion, recompensa))\n",
    "\n",
    "        estado_anterior = estado_siguiente\n",
    "        iteraciones += 1\n",
    "        recompensa_total += recompensa\n",
    "\n",
    "    if aprender:\n",
    "        agente.fin_episodio()\n",
    "    entorno.close()\n",
    "    return recompensa_total, episodio, semilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa15ab0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ejecutar_episodio_1(agente, aprender = True, render = None, max_iteraciones=500):\n",
    "    entorno = gym.make('LunarLander-v2', render_mode=render).env\n",
    "    \n",
    "    iteraciones = 0\n",
    "    recompensa_total = 0\n",
    "\n",
    "    termino = False\n",
    "    truncado = False\n",
    "    estado_anterior, info = entorno.reset()\n",
    "    while iteraciones < max_iteraciones and not termino and not truncado:\n",
    "        # Le pedimos al agente que elija entre las posibles acciones (0..entorno.action_space.n)\n",
    "        accion = agente.elegir_accion(estado_anterior, entorno.action_space.n, aprender)\n",
    "        # Realizamos la accion\n",
    "        estado_siguiente, recompensa, termino, truncado, info = entorno.step(accion)\n",
    "        # Le informamos al agente para que aprenda\n",
    "        if (aprender):\n",
    "            agente.aprender(estado_anterior, estado_siguiente, accion, recompensa, termino)\n",
    "\n",
    "        estado_anterior = estado_siguiente\n",
    "        iteraciones += 1\n",
    "        recompensa_total += recompensa\n",
    "    if (aprender):\n",
    "        agente.fin_episodio()\n",
    "    entorno.close()\n",
    "    return recompensa_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a104779-f5e3-4bfa-b88b-fb44de195d49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(-186.58359706317572)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nota: hay que transformar esta celda en código para ejecutar (Esc + y)\n",
    "\n",
    "# Ejecutamos un episodio con el agente aleatorio y modo render 'human', para poder verlo\n",
    "ejecutar_episodio_1(AgenteAleatorio(), render = 'human')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b9cb2a-37b0-4115-afe8-bb89ce49f605",
   "metadata": {},
   "source": [
    "Podemos ejecutar este ambiente muchas veces y tomar métricas al respecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "536e81ee-6038-44c7-b887-35db442815ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasa de éxito: 0.0. Se obtuvo -174.52106331064874 de recompensa, en promedio\n"
     ]
    }
   ],
   "source": [
    "agente = AgenteAleatorio()\n",
    "recompensa_episodios = []\n",
    "\n",
    "exitos = 0\n",
    "num_episodios = 100\n",
    "for i in range(num_episodios):\n",
    "    recompensa = ejecutar_episodio_1(agente)\n",
    "    # Los episodios se consideran exitosos si se obutvo 200 o más de recompensa total\n",
    "    if (recompensa >= 200):\n",
    "        exitos += 1\n",
    "    recompensa_episodios += [recompensa]\n",
    "\n",
    "import numpy\n",
    "print(f\"Tasa de éxito: {exitos / num_episodios}. Se obtuvo {numpy.mean(recompensa_episodios)} de recompensa, en promedio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086cb250-7bc9-4bd4-a4cd-43cfd561facb",
   "metadata": {},
   "source": [
    "### **5. Programando un agente que aprende**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0827e3e2-a84a-462f-8b86-e03425cfc645",
   "metadata": {},
   "source": [
    "La tarea a realizar consiste en programar un agente de aprendizaje por refuerzos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3f37011-ffaa-4d08-b832-c45d0b9060da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "class AgenteRL(Agente):\n",
    "    # Agregar código aqui\n",
    "\n",
    "    # Pueden agregar parámetros al constructor\n",
    "    def __init__(self, max_accion, bins, initial_epsilon: float, epsilon_decay: float, final_epsilon: float, discount_factor: float = 0.95):\n",
    "        super().__init__()\n",
    "        \"\"\"Initialize a Reinforcement Learning agent with an empty dictionary\n",
    "        of state-action values (q_values), a learning rate and an epsilon.\n",
    "\n",
    "        Args:\n",
    "            learning_rate: The learning rate\n",
    "            initial_epsilon: The initial epsilon value\n",
    "            epsilon_decay: The decay for epsilon\n",
    "            final_epsilon: The final epsilon value\n",
    "            discount_factor: The discount factor for computing the Q-value\n",
    "        \"\"\"\n",
    "        self.q_table = defaultdict(lambda: np.zeros(max_accion))\n",
    "        self.visitas = defaultdict(lambda: np.zeros(max_accion))\n",
    "\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "        self.epsilon = initial_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.final_epsilon = final_epsilon\n",
    "\n",
    "        self.bins = bins\n",
    "\n",
    "        self.training_error = []\n",
    "    \n",
    "    def elegir_accion(self, estado, max_accion, explorar = True) -> int:\n",
    "\n",
    "        estado_discreto = discretize_state(estado, self.bins)\n",
    "        if (explorar and np.random.random() < self.epsilon):\n",
    "            return random.randrange(max_accion)\n",
    "        else:\n",
    "            return int(np.argmax(self.q_table[estado_discreto]))\n",
    "    \n",
    "    def aprender(self, estado_anterior, estado_siguiente, accion, recompensa, terminado):\n",
    "        estado_anterior_discreto = discretize_state(estado_anterior, self.bins)\n",
    "        estado_siguiente_discreto = discretize_state(estado_siguiente, self.bins)\n",
    "\n",
    "        self.visitas[estado_anterior_discreto][accion] += 1\n",
    "\n",
    "        \"\"\"Updates the Q-value of an action.\"\"\"\n",
    "        future_q_value = (not terminado) * np.max(self.q_table[estado_siguiente_discreto])\n",
    "        temporal_difference = (\n",
    "            recompensa + self.discount_factor * future_q_value - self.q_table[estado_anterior_discreto][accion]\n",
    "        )\n",
    "\n",
    "        lr = 1 / self.visitas[estado_anterior_discreto][accion]\n",
    "\n",
    "        self.q_table[estado_anterior_discreto][accion] = (\n",
    "            self.q_table[estado_anterior_discreto][accion] + lr * temporal_difference\n",
    "        )\n",
    "        self.training_error.append(temporal_difference)\n",
    "\n",
    "    def fin_episodio(self):\n",
    "        self.epsilon = max(self.final_epsilon, self.epsilon - self.epsilon_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ee51f5-aff8-4d18-934e-92acdcb617c0",
   "metadata": {},
   "source": [
    "Y ejecutar con el muchos episodios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0524d5-0d12-46a8-8437-984b981fbae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julio/.local/lib/python3.10/site-packages/numpy/_core/fromnumeric.py:3904: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/julio/.local/lib/python3.10/site-packages/numpy/_core/_methods.py:147: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "  0%|          | 7/5000 [00:00<01:30, 54.90it/s, promedio=-132, recompensa=-96.8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo nan de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1009/5000 [00:18<01:25, 46.87it/s, promedio=-155, recompensa=-108] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -154.97783449360355 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 1536/5000 [00:30<01:24, 40.94it/s, promedio=-144, recompensa=-65.5]"
     ]
    }
   ],
   "source": [
    "# Nota: hay que transformar esta celda en código para ejecutar (Esc + y)\n",
    "# Advertencia: este bloque es un loop infinito si el agente se deja sin implementar\n",
    "from tqdm import tqdm \n",
    "entorno = gym.make('LunarLander-v2').env\n",
    "num_episodios = 5000\n",
    "max_accion = entorno.action_space.n\n",
    "initial_epsilon = 1.0\n",
    "epsilon_decay = initial_epsilon / (num_episodios / 2)\n",
    "final_epsilon = 0.05\n",
    "discount_factor = 0.95\n",
    "\n",
    "agente = AgenteRL(max_accion = max_accion, bins = bins, initial_epsilon = initial_epsilon, epsilon_decay = epsilon_decay, final_epsilon = final_epsilon, discount_factor = discount_factor)\n",
    "exitos = 0\n",
    "recompensa_episodios = []\n",
    "promedio_recompensa = []\n",
    "with tqdm(total=num_episodios) as pbar:\n",
    "    for i in range(num_episodios):\n",
    "        recompensa = ejecutar_episodio_1(agente)\n",
    "        if i % 1000 == 0:\n",
    "            ultimas_1000_recompensas = recompensa_episodios[-1000:]\n",
    "            promedio_ultimas_1000 = np.mean(ultimas_1000_recompensas)\n",
    "            promedio_recompensa.append(promedio_ultimas_1000)\n",
    "            print(f\" Se obtuvo {promedio_ultimas_1000} de recompensa, en promedio en los últimos 1000 episodios\")\n",
    "        if (recompensa >= 200):\n",
    "            exitos += 1\n",
    "        recompensa_episodios += [recompensa]\n",
    "        pbar.set_postfix(recompensa=recompensa, promedio=numpy.mean(recompensa_episodios))\n",
    "        pbar.update(1)\n",
    "print(f\"Tasa de éxito: {exitos / num_episodios}. Se obtuvo {numpy.mean(recompensa_episodios)} de recompensa, en promedio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b7079e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 0 exitoso, recompensa: 245.0298774387565\n",
      "Episodio 2 exitoso, recompensa: 233.53788773812198\n",
      "Episodio 4 exitoso, recompensa: 209.77219206128882\n",
      "Episodio 11 exitoso, recompensa: 271.288874830841\n",
      "Episodio 14 exitoso, recompensa: 214.5502820862119\n",
      "Episodio 21 exitoso, recompensa: 226.22604280416425\n",
      "Episodio 30 exitoso, recompensa: 239.66109077561867\n",
      "Episodio 32 exitoso, recompensa: 202.83839244927012\n",
      "Episodio 34 exitoso, recompensa: 200.97059685154926\n",
      "Episodio 38 exitoso, recompensa: 204.44507364826492\n",
      "Episodio 41 exitoso, recompensa: 217.98511453388235\n",
      "Episodio 43 exitoso, recompensa: 208.46954614389364\n",
      "Episodio 45 exitoso, recompensa: 219.3198208669819\n",
      "Episodio 50 exitoso, recompensa: 226.21729860893672\n",
      "Episodio 51 exitoso, recompensa: 210.3631519929682\n",
      "Episodio 52 exitoso, recompensa: 251.92187388597844\n",
      "Episodio 57 exitoso, recompensa: 247.42849713625438\n",
      "Episodio 58 exitoso, recompensa: 217.3252532177924\n",
      "Episodio 64 exitoso, recompensa: 211.78598346851055\n",
      "Episodio 73 exitoso, recompensa: 269.60110588977193\n",
      "Episodio 75 exitoso, recompensa: 265.4059211154505\n",
      "Episodio 77 exitoso, recompensa: 266.60795306225293\n",
      "Episodio 78 exitoso, recompensa: 230.3066362181552\n",
      "Episodio 81 exitoso, recompensa: 222.73569132873405\n",
      "Episodio 84 exitoso, recompensa: 217.33156942422374\n",
      "Episodio 86 exitoso, recompensa: 285.1127057627424\n",
      "Episodio 87 exitoso, recompensa: 269.7589640445781\n",
      "Episodio 88 exitoso, recompensa: 264.84864384050724\n",
      "Episodio 95 exitoso, recompensa: 253.87288734037315\n",
      "Episodio 97 exitoso, recompensa: 219.97287737716053\n",
      "Episodio 99 exitoso, recompensa: 277.0528367478247\n",
      "Episodio 104 exitoso, recompensa: 222.22689692904277\n",
      "Episodio 111 exitoso, recompensa: 216.07192664892673\n",
      "Episodio 112 exitoso, recompensa: 271.1054489332038\n",
      "Episodio 113 exitoso, recompensa: 222.25987619685895\n",
      "Episodio 117 exitoso, recompensa: 233.5892233961133\n",
      "Episodio 118 exitoso, recompensa: 230.5826432603397\n",
      "Episodio 119 exitoso, recompensa: 241.01490886786866\n",
      "Episodio 124 exitoso, recompensa: 208.04666298914088\n",
      "Episodio 126 exitoso, recompensa: 226.19793655317065\n",
      "Episodio 129 exitoso, recompensa: 233.02937315450592\n",
      "Episodio 131 exitoso, recompensa: 247.5400146365292\n",
      "Episodio 135 exitoso, recompensa: 230.77594571240724\n",
      "Episodio 136 exitoso, recompensa: 284.35796114766833\n",
      "Episodio 145 exitoso, recompensa: 289.9365050712163\n",
      "Episodio 149 exitoso, recompensa: 215.83623611583573\n",
      "Episodio 151 exitoso, recompensa: 241.23274734626375\n",
      "Episodio 156 exitoso, recompensa: 211.14049088822605\n",
      "Episodio 157 exitoso, recompensa: 270.0846748707942\n",
      "Episodio 158 exitoso, recompensa: 247.954938788834\n",
      "Episodio 168 exitoso, recompensa: 234.93477076502276\n",
      "Episodio 172 exitoso, recompensa: 251.73326434618747\n",
      "Episodio 174 exitoso, recompensa: 275.8824834150916\n",
      "Episodio 176 exitoso, recompensa: 235.54377501001068\n",
      "Episodio 179 exitoso, recompensa: 235.18415662521457\n",
      "Episodio 180 exitoso, recompensa: 278.35191785976565\n",
      "Episodio 186 exitoso, recompensa: 216.92480276150258\n",
      "Episodio 187 exitoso, recompensa: 219.64491094923991\n",
      "Episodio 188 exitoso, recompensa: 281.3147270242716\n",
      "Episodio 201 exitoso, recompensa: 239.06679109002087\n",
      "Episodio 204 exitoso, recompensa: 287.6963334884334\n",
      "Episodio 208 exitoso, recompensa: 249.77821303313362\n",
      "Episodio 210 exitoso, recompensa: 268.50579270623416\n",
      "Episodio 211 exitoso, recompensa: 250.2872903875376\n",
      "Episodio 213 exitoso, recompensa: 248.69571761516812\n",
      "Episodio 237 exitoso, recompensa: 273.20575246384414\n",
      "Episodio 242 exitoso, recompensa: 244.58758695777482\n",
      "Episodio 244 exitoso, recompensa: 275.89843872582617\n",
      "Episodio 245 exitoso, recompensa: 266.998475407767\n",
      "Episodio 250 exitoso, recompensa: 215.29313461325168\n",
      "Episodio 261 exitoso, recompensa: 222.2996800989423\n",
      "Episodio 282 exitoso, recompensa: 236.19389353334114\n",
      "Episodio 285 exitoso, recompensa: 234.2274007293734\n",
      "Episodio 288 exitoso, recompensa: 227.3592806082191\n",
      "Episodio 299 exitoso, recompensa: 202.0832627240388\n",
      "Episodio 303 exitoso, recompensa: 263.23872101747145\n",
      "Episodio 310 exitoso, recompensa: 291.6271024530843\n",
      "Episodio 316 exitoso, recompensa: 234.48823572062093\n",
      "Episodio 318 exitoso, recompensa: 222.42741605494206\n",
      "Episodio 327 exitoso, recompensa: 255.6024723338399\n",
      "Episodio 333 exitoso, recompensa: 238.21599846040678\n",
      "Episodio 338 exitoso, recompensa: 270.14012124240503\n",
      "Episodio 339 exitoso, recompensa: 263.67209455686435\n",
      "Episodio 340 exitoso, recompensa: 258.8757751157128\n",
      "Episodio 341 exitoso, recompensa: 247.07675680955782\n",
      "Episodio 345 exitoso, recompensa: 253.07181617701565\n",
      "Episodio 349 exitoso, recompensa: 282.5141748256174\n",
      "Episodio 352 exitoso, recompensa: 249.5120728619592\n",
      "Episodio 353 exitoso, recompensa: 214.08336495611763\n",
      "Episodio 360 exitoso, recompensa: 272.4160097098701\n",
      "Episodio 365 exitoso, recompensa: 217.81285037892053\n",
      "Episodio 366 exitoso, recompensa: 235.95602962655667\n",
      "Episodio 372 exitoso, recompensa: 265.9449503262874\n",
      "Episodio 374 exitoso, recompensa: 250.3906999601018\n",
      "Episodio 390 exitoso, recompensa: 210.9372091972204\n",
      "Episodio 396 exitoso, recompensa: 242.7257533270429\n",
      "Episodio 401 exitoso, recompensa: 219.20226563412749\n",
      "Episodio 403 exitoso, recompensa: 249.71691599631018\n",
      "Episodio 412 exitoso, recompensa: 281.9641323214555\n",
      "Episodio 413 exitoso, recompensa: 258.3578123417373\n",
      "Episodio 418 exitoso, recompensa: 264.77440736352696\n",
      "Episodio 421 exitoso, recompensa: 232.2073234952063\n",
      "Episodio 426 exitoso, recompensa: 235.95635136159086\n",
      "Episodio 428 exitoso, recompensa: 256.5606874022978\n",
      "Episodio 433 exitoso, recompensa: 239.34335221733562\n",
      "Episodio 435 exitoso, recompensa: 273.7838116007332\n",
      "Episodio 445 exitoso, recompensa: 229.57393094156578\n",
      "Episodio 446 exitoso, recompensa: 222.5099440674083\n",
      "Episodio 453 exitoso, recompensa: 289.96741459316434\n",
      "Episodio 468 exitoso, recompensa: 217.0599678244924\n",
      "Episodio 479 exitoso, recompensa: 257.04609633870035\n",
      "Episodio 481 exitoso, recompensa: 214.47887777508402\n",
      "Episodio 485 exitoso, recompensa: 253.8619383967272\n",
      "Episodio 494 exitoso, recompensa: 268.6490043456975\n",
      "Episodio 500 exitoso, recompensa: 205.1451356088611\n",
      "Episodio 501 exitoso, recompensa: 256.16059103499856\n",
      "Episodio 510 exitoso, recompensa: 236.76781684235556\n",
      "Episodio 514 exitoso, recompensa: 261.6603720246289\n",
      "Episodio 519 exitoso, recompensa: 225.60950685131144\n",
      "Episodio 520 exitoso, recompensa: 266.85148328552873\n",
      "Episodio 522 exitoso, recompensa: 269.64320266949824\n",
      "Episodio 526 exitoso, recompensa: 272.2330082507509\n",
      "Episodio 530 exitoso, recompensa: 238.7947622218672\n",
      "Episodio 532 exitoso, recompensa: 287.0490625556406\n",
      "Episodio 538 exitoso, recompensa: 254.6971307031784\n",
      "Episodio 541 exitoso, recompensa: 268.0529198633626\n",
      "Episodio 543 exitoso, recompensa: 255.3804627684194\n",
      "Episodio 544 exitoso, recompensa: 207.52330745671026\n",
      "Episodio 548 exitoso, recompensa: 251.81818835692076\n",
      "Episodio 550 exitoso, recompensa: 238.64929271131476\n",
      "Episodio 557 exitoso, recompensa: 226.04896912737632\n",
      "Episodio 558 exitoso, recompensa: 222.8802800635438\n",
      "Episodio 559 exitoso, recompensa: 223.5500819249386\n",
      "Episodio 563 exitoso, recompensa: 234.75617737672832\n",
      "Episodio 572 exitoso, recompensa: 205.70019112306332\n",
      "Episodio 577 exitoso, recompensa: 200.5711254617962\n",
      "Episodio 583 exitoso, recompensa: 263.528433470997\n",
      "Episodio 590 exitoso, recompensa: 206.27590757963327\n",
      "Episodio 596 exitoso, recompensa: 261.78176930602694\n",
      "Episodio 599 exitoso, recompensa: 246.26604897520193\n",
      "Episodio 604 exitoso, recompensa: 223.58162073079288\n",
      "Episodio 605 exitoso, recompensa: 201.99114657618694\n",
      "Episodio 606 exitoso, recompensa: 272.72479602703197\n",
      "Episodio 609 exitoso, recompensa: 289.8248908231675\n",
      "Episodio 616 exitoso, recompensa: 206.5719021433125\n",
      "Episodio 619 exitoso, recompensa: 204.78694832324254\n",
      "Episodio 624 exitoso, recompensa: 237.82187483492467\n",
      "Episodio 627 exitoso, recompensa: 241.10219364120596\n",
      "Episodio 628 exitoso, recompensa: 238.87966573137584\n",
      "Episodio 633 exitoso, recompensa: 208.9571981747413\n",
      "Episodio 638 exitoso, recompensa: 226.69720912182623\n",
      "Episodio 641 exitoso, recompensa: 254.74913776628878\n",
      "Episodio 646 exitoso, recompensa: 226.10176812770942\n",
      "Episodio 655 exitoso, recompensa: 225.38674824848937\n",
      "Episodio 656 exitoso, recompensa: 239.98778366678232\n",
      "Episodio 658 exitoso, recompensa: 209.60585225610959\n",
      "Episodio 661 exitoso, recompensa: 243.14302506415564\n",
      "Episodio 664 exitoso, recompensa: 226.0677301504427\n",
      "Episodio 672 exitoso, recompensa: 257.7389027454152\n",
      "Episodio 677 exitoso, recompensa: 246.5822644333909\n",
      "Episodio 679 exitoso, recompensa: 284.741333320504\n",
      "Episodio 680 exitoso, recompensa: 271.8935999537588\n",
      "Episodio 682 exitoso, recompensa: 274.8761998836453\n",
      "Episodio 687 exitoso, recompensa: 257.4514731252525\n",
      "Episodio 689 exitoso, recompensa: 254.38326314993947\n",
      "Episodio 703 exitoso, recompensa: 258.469171863095\n",
      "Episodio 711 exitoso, recompensa: 245.798901281275\n",
      "Episodio 713 exitoso, recompensa: 216.23567314157324\n",
      "Episodio 714 exitoso, recompensa: 232.56000631696907\n",
      "Episodio 716 exitoso, recompensa: 224.76435912741545\n",
      "Episodio 718 exitoso, recompensa: 241.3363897590247\n",
      "Episodio 720 exitoso, recompensa: 212.86296057838553\n",
      "Episodio 722 exitoso, recompensa: 207.8693311403995\n",
      "Episodio 743 exitoso, recompensa: 236.7644174983177\n",
      "Episodio 754 exitoso, recompensa: 225.85364283282354\n",
      "Episodio 760 exitoso, recompensa: 230.38679438907232\n",
      "Episodio 761 exitoso, recompensa: 237.896685479455\n",
      "Episodio 767 exitoso, recompensa: 215.54809808866383\n",
      "Episodio 770 exitoso, recompensa: 251.5210152352547\n",
      "Episodio 773 exitoso, recompensa: 253.20443139459036\n",
      "Episodio 778 exitoso, recompensa: 274.95869041997435\n",
      "Episodio 780 exitoso, recompensa: 232.33394319700307\n",
      "Episodio 783 exitoso, recompensa: 222.01632464330524\n",
      "Episodio 784 exitoso, recompensa: 273.3010958719052\n",
      "Episodio 786 exitoso, recompensa: 253.08247904924715\n",
      "Episodio 789 exitoso, recompensa: 232.1320875854939\n",
      "Episodio 791 exitoso, recompensa: 254.01272005369526\n",
      "Episodio 794 exitoso, recompensa: 273.41386651146274\n",
      "Episodio 801 exitoso, recompensa: 248.0544944367722\n",
      "Episodio 803 exitoso, recompensa: 204.1164636418847\n",
      "Episodio 807 exitoso, recompensa: 225.41168130501455\n",
      "Episodio 813 exitoso, recompensa: 238.95144858476345\n",
      "Episodio 815 exitoso, recompensa: 265.69963598412755\n",
      "Episodio 818 exitoso, recompensa: 221.79380433740033\n",
      "Episodio 830 exitoso, recompensa: 234.9478788078669\n",
      "Episodio 836 exitoso, recompensa: 243.70658138687486\n",
      "Episodio 837 exitoso, recompensa: 201.4445405410043\n",
      "Episodio 838 exitoso, recompensa: 231.75249557857222\n",
      "Episodio 847 exitoso, recompensa: 242.665684358035\n",
      "Episodio 850 exitoso, recompensa: 257.04225214838743\n",
      "Episodio 852 exitoso, recompensa: 228.45251813774496\n",
      "Episodio 862 exitoso, recompensa: 209.21751166728882\n",
      "Episodio 867 exitoso, recompensa: 251.05196727054022\n",
      "Episodio 868 exitoso, recompensa: 272.29418046886497\n",
      "Episodio 871 exitoso, recompensa: 248.18547115220292\n",
      "Episodio 875 exitoso, recompensa: 247.9430196753849\n",
      "Episodio 883 exitoso, recompensa: 241.72583506301348\n",
      "Episodio 896 exitoso, recompensa: 231.03853250103674\n",
      "Episodio 900 exitoso, recompensa: 249.94594110394146\n",
      "Episodio 913 exitoso, recompensa: 264.3033914593933\n",
      "Episodio 916 exitoso, recompensa: 212.58857871048144\n",
      "Episodio 918 exitoso, recompensa: 222.20955307559962\n",
      "Episodio 919 exitoso, recompensa: 297.7389242827538\n",
      "Episodio 921 exitoso, recompensa: 225.10716564066848\n",
      "Episodio 924 exitoso, recompensa: 244.66962332686649\n",
      "Episodio 932 exitoso, recompensa: 248.9458540454522\n",
      "Episodio 934 exitoso, recompensa: 204.47094986991794\n",
      "Episodio 936 exitoso, recompensa: 200.0148887427599\n",
      "Episodio 940 exitoso, recompensa: 283.2789606143203\n",
      "Episodio 943 exitoso, recompensa: 274.93896494090933\n",
      "Episodio 948 exitoso, recompensa: 270.7712475376083\n",
      "Episodio 955 exitoso, recompensa: 232.9708550515839\n",
      "Episodio 959 exitoso, recompensa: 233.75891558347428\n",
      "Episodio 960 exitoso, recompensa: 247.7309901398893\n",
      "Episodio 968 exitoso, recompensa: 225.00179359201746\n",
      "Episodio 978 exitoso, recompensa: 263.5839436604897\n",
      "Episodio 983 exitoso, recompensa: 230.56599303753802\n",
      "Episodio 985 exitoso, recompensa: 240.8253782115191\n",
      "Episodio 986 exitoso, recompensa: 249.34168049418744\n",
      "Episodio 994 exitoso, recompensa: 247.86242304191595\n",
      "Episodio 999 exitoso, recompensa: 215.2829114551745\n",
      "Tasa de éxito: 0.231. Se obtuvo 29.741304287730244 de recompensa, en promedio\n"
     ]
    }
   ],
   "source": [
    "# Nota: hay que transformar esta celda en código para ejecutar (Esc + y)\n",
    "# Advertencia: este bloque es un loop infinito si el agente se deja sin implementar\n",
    "\n",
    "exitos = 0\n",
    "recompensa_episodios = []\n",
    "num_episodios = 1000\n",
    "episodios_exitosos = []\n",
    "episodios_no_exitosos = []\n",
    "semillas_exitosas = []\n",
    "semillas_no_exitosas = []\n",
    "\n",
    "for i in range(num_episodios):\n",
    "    recompensa, episodio, semilla = ejecutar_episodio_2(agente, aprender=False)\n",
    "    # Los episodios se consideran exitosos si se obtuvo 200 o más de recompensa total\n",
    "    if recompensa >= 200:\n",
    "        print(f\"Episodio {i} exitoso, recompensa: {recompensa}\")\n",
    "        exitos += 1\n",
    "        episodios_exitosos.append(episodio)\n",
    "        semillas_exitosas.append(semilla)\n",
    "    else:\n",
    "        episodios_no_exitosos.append(episodio)\n",
    "        semillas_no_exitosas.append(semilla)\n",
    "    recompensa_episodios.append(recompensa)\n",
    "\n",
    "print(f\"Tasa de éxito: {exitos / num_episodios}. Se obtuvo {np.mean(recompensa_episodios)} de recompensa, en promedio\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e7b346-6f23-44e2-a645-e4548ab470ef",
   "metadata": {},
   "source": [
    "Analizar los resultados de la ejecución anterior, incluyendo:\n",
    " * Un análisis de los parámetros utilizados en el algoritmo (aprendizaje, política de exploración)\n",
    " * Un análisis de algunos 'cortes' de la matriz Q y la política (p.e. qué hace la nave cuando está cayendo rápidamente hacia abajo, sin rotación)\n",
    " * Un análisis de la evolución de la recompensa promedio\n",
    " * Un análisis de los casos de éxito\n",
    " * Un análisis de los casos en el que el agente falla\n",
    " * Qué limitante del agente de RL les parece que afecta más negativamente su desempeño. Cómo lo mejorarían? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a680260",
   "metadata": {},
   "source": [
    "# Análisis de resultados\n",
    "\n",
    "En la siguiente sección estaremos realizando un análisis de los resultados obtenidos tras la implementación y el entrenamiento de un agente de RL basado en Q Learning. En esta sección explicaremos cómo construimos el agente, en qué nos basamos, qué parámetros tuvimos en cuenta y por qué. También analizaremos algunas situaciones particulares del agente para ver su comportamiento en detalle, algunos casos de éxito y de falla, cómo evolucionó la recompensa promedio a medida que avanzaba el entrenamiento y qué limitantes vemos en el agente y cómo se podrían mitigar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7858dc3d",
   "metadata": {},
   "source": [
    "## Análisis de implementación y decisiones de diseño\n",
    "\n",
    "A la hora de construir nuestro agente RL basado en Q Learning, nos basamos fuertemente en el teórico del curso, utilizando la metodología vista en clase, y también en la documentación de gymnasium, en la cual explican, sobre otro ejemplo, como entrenarlo con Q Learning utilizando la librería. El link a la documentación utilizada es el siguiente: https://gymnasium.farama.org/tutorials/training_agents/blackjack_tutorial/.\n",
    "\n",
    "### Estrategia de acción a tomar\n",
    "\n",
    "Lo primero que decidimos sobre el modelo es cómo debiamos elegir una acción, dado que si siempre elegimos la mejor acción nos perdemos de encontrar acciones que no fueron descubiertas previamente, por lo que hay que encontrar un balance entre tomar decisiones azarosas y las mejores decisiones. Ante esto teníamos 2 opciones: \n",
    "- Soft Max: En esta estrategia, se asignan probabilidades a las diferentes acciones usando una función. Las acciones con valores de Q más altos tendrán mayores probabilidades de ser seleccionadas, pero incluso las acciones con valores menores todavía tienen una probabilidad no nula de ser escogidas, lo que genera que se suela elegir las acciones con valores de Q más altos, pero que con cierta probabilidad no nula se tomen otras acciones que pueden resultar beneficiosas.\n",
    "- Epsilon-Greedy: En esta estrategia, se toma un valor epsilon, que viene dado como parámetro, y se toma la mejor acción (mayor valor de la tabla Q para ese estado) con probabilidad 1-epsilon, y una decisión azarosa con probabilidad epsilon. De esta forma se asegura que tomemos acciones azarosas e informadas, lo que contribuye a un mejor entrenamiento.\n",
    "- Epsilon-Greedy con decaimiento: Esta estrategia es una variante de la estrategia Epsilon-Greedy que aparte del valor epsilon toma un valor de epsilon minimo y un valor de decaimiento de epsilon. Por cada episodio que pasa se reduce el epsilon restandole el valor de decaimiento, hasta que en algún episodio se llega al valor mínimo de epsilon, el cual se utiliza durante el resto de entrenamiento. Esto lo que genera es que al principio del entrenamiento tomemos más decisiones azarosas, ya que no tenemos tanto conocimiento del entorno aún, y a medida que pasa el tiempo y se asume que tenemos más información y podemos tomar más decisiones informadas, la probabilidad de tomar decisiones azarosas se reduce y en su lugar se toman decisiones informadas con respecto a los valores de la tabla Q.\n",
    "\n",
    "Ante estas tres opciones decidimos inclinarnos por la tercera opción: Epsilon-Greedy con decaimiento. Nos pareció una estrategia coherente, un poco mejor que el Epsilon-Greedy sin esta variante, y creímos que en términos de implementación no era muy compleja. Decidimos probar esta estrategia y en caso de notar un bajo desempeño, cambiar a Soft Max. Cómo se verá más adelante en este informe, tras ver que los resultados fueron óptimos, se mantuvo la decisión.\n",
    "\n",
    "### Hiperparámetros a utilizar y valores utilizados\n",
    "\n",
    "#### Epsilon, Epsilon Mínimo y Decaimiento de Epsilon\n",
    "\n",
    "Tras la decisión de la estrategia a utilizar, debíamos decidir qué valores utilizar para dichos parámetros (epsilon, decaimiento de epsilon y epsilon mínimo). Para el epsilon se tomó la decisión del valor de 1, ya que entendimos que cuando recién se comienza el entrenamiento queremos explorar lo más que se pueda, ya que no tenemos ningún tipo de información del entorno aún, por lo que la probabilidad conviene que sea 1, y que luego esta vaya disminuyendo linealmente. Cómo epsilon mínimo tomamos el valor de 0.05. Esta decisión se baso mayormente en experiencia. Sabíamos que queríamos un valor bajo de epsilon mínimo ya que una vez que tengamos la tabla Q bastante llena, queremos tomar decisiones informadas la mayor parte de las veces. Al principio probamos con un valor de 0.1, de 0.2, de 0.05, y así fuimos probando con distintos valores, llegando a que los mayores resultados se daban con el valor de 0.05, aunque la diferencia no era tanta, sí había una diferencia.\n",
    "Por último, debimos decidir sobre el valor del decaimiento de epsilon.\n",
    "Sobre este valor nos encontramos con un tema a la hora de la implementación, y fue que al principio utilizamos el decaimiento de epsilon como factor que se le multiplicaba al valor de epsilon hasta llegar al epsilon mínimo. Por lo que poniamos un valor fijo alto de decaimiento de epsilon (ej: 0.995). Vimos que los resultados no eran buenos y decidimos probar con valores paramétricos, utilizando la cantidad de episodios para decidir sobre el valor. Para esto se nos ocurrió la fórmula: decaimiento de epsilon = (epsilon - epsilon min) ^ (1/episodios), con la idea de que esto haría que el epsilon fuera decayendo y en el último episodio se llegara al valor mínimo de epsilon.\n",
    "Esta opción tampoco nos trajo buenos resultados, por lo que tras revisar la documentación brindada por gymnasium, vimos que estaba bien poner un valor parámetrico en función de la cantidad de episodios, pero que la disminución del epsilon con el factor de decaimiento era a través de una resta. Epsilon = Epsilon - decaimiento_epsilon. En este caso vimos que en la documentación utilizaban un valor de decaimiento descripto por la fórmula: decaimiento_epsilon = epsilon / (num_episodios / 2). Tras utilizar esta fórmula nuestros resultados mejoraron bastante. Probamos luego variar el valor de 2 en la fórmula por distintos múltiplos de 2, ya que entendimos que por cómo es su funcionamiento, cuanto más grande dicho valor, durante menos tiempo se mantiene el epsilon en un número alto, pero vimos que la fórmula detallada por la documentación fue la que nos dió mejores resultados en la práctica.\n",
    "\n",
    "#### Tasa de aprendizaje en ambiente no determinista alpha\n",
    "\n",
    "Dado que nos encontramos en el problema de Lunar Lander, este es un problema continuo, donde el estado está definido por los ejes horizontal, vertical y de rotación, sus velocidades y 2 booleanos que indican si las patas de la nave están en contacto con el suelo o no. Por más de que en un principio discretizamos el estado en una cantidad de bins, esto no quita que el problema siga siendo continuo, por lo que al trabajar con un problema continuo, nos va a suceder muchas veces que al tomar una misma acción desde un mismo estado caigamos en estados distintos. Esto tiene un problema si no lo tomamos en cuenta, y es que a la hora de llenar la tabla Q con los valores de recompensa, las recompensas no van a ser las mismas desde un estado y una acción ya que los resultados posteriores dependen del estado en el que se caiga posteriormente, y estos varían cada vez que los tomamos. \n",
    "Para esto se utiliza el hiperparámetro tasa de aprendizaje.\n",
    "El cual se agrega en la etapa de aprendizaje en la actualización de la tabla Q:\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0fc44224",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "self.q_table[estado_anterior][accion] = (\n",
    "            self.q_table[estado_anterior][accion] + tasa_aprendizaje * (recompensa + np.max(self.q_table[estado_siguiente]) - self.q_table[estado_anterior][accion])\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de17e49",
   "metadata": {},
   "source": [
    "Aquí vemos que se multiplica a la tasa de aprendizaje por la recompensa del movimiento que se realizó, más la recompensa máxima del estado al que se llegó, menos el valor de la tabla Q en el par estado-acción que se está llenando.\n",
    "\n",
    "Vemos aquí como la tasa de aprendizaje determina cuánto peso se le da a la nueva información en comparación con la información previamente aprendida. Cuanto más grande sea esta tasa, mayor será el peso que se le otorga.\n",
    "\n",
    "Para definir un valor para la tasa de aprendizaje, primero probamos con valores fijos chicos, como 0.1 y 0.01. Los resultados con valores fijos no fueron muy favorables, y nuestro agente no lograba desempeñarse bien, obteniendo muy bajos porcentajes de éxito. \n",
    "Luego se utilizó la fórmula vista en clase, de definir la tasa de aprendizaje en función del estado acción que se está llenando. La tasa de aprendizaje se define como 1/visitas[estado][acción], donde visitas[estado][acción] es la cantidad de veces que se pasó por el dicho estado y acción en el proceso. Esto tiene sentido ya que a medida que vamos teniendo más información de ese entorno, el impacto de un nuevo valor debería ser menos, mientras que al principio cuando no hay tanta información, debería tener más impacto.\n",
    "\n",
    "Al utilizar dicho valor variable, el desempeño mejoró notoriamente. Este cambio fue el que nos hizo pasar de un desempeño sin entrenamiento de aproximadamente 1 éxito en 1000, a tener más de un 10% de éxito en 1000 episodios. \n",
    "\n",
    "### Factor de descuento gamma\n",
    "\n",
    "El último hiperparámetro utilizado en este caso es el parámetro de descuento gamma. Este hiperparámetro es mayormente utilizado cuando se trata con tareas posiblemente infinitas, como el caso de un avión que debe mantenerse volando indefinidamente. Es un valor que en la fórmula de actualización de la tabla Q se multiplica por el valor máximo que se encuentra en el estado futuro del par estado-acción tomado, lo que genera que se de más valor a las recompensas inmediatas que a los valores futuros.\n",
    "\n",
    "La decisión de utilizar este hiperparámetro por nuestra parte fue completamente empírica. Sin utilizar dicho parámetro el desempeño del agente decae drásticamente, y al utilizarlo mejora. No tenemos claro la razón por la cual esto sucede, ya que la tarea que estamos intentando aprender es una tarea finita de aterrizaje. Creemos que puede llegar a tener que ver con el hecho de que para aterrizar la nave debe mantenerse en el aire de forma controlada primero, por lo cual puede que se desempeñe mejor si tiene más en cuenta la recompensa de los movimientos actuales que la mantengan en el aire, recta y con velocidades bajas, antes que las recompensas de los valores Q de estados futuros.\n",
    "\n",
    "El valor utilizado para gamma fue de 0.95, el cual también fue decidido empíricamente. Variamos los valores de gamma y este fue el que tuvo mayor desempeño.\n",
    "\n",
    "### Cantidad de bins\n",
    "\n",
    "Por último, si bien no es un parámetro de la función, cabe destacar nuestro cambio sobre la cantidad de bins utilizados. Por defecto en la tarea venía puesto como 20 bins, lo que significaba que los ejes continuos que representan el estado serían particionados en 20 valores cada uno. Notamos que el desempeño en este caso no era muy bueno por lo cual al ya tener implementado el resto de parámetros, variamos el valor de bins y vimos que mejoraba al cambiarlo. Tras probar con varios valores, mayores y menores a éste, vimos que en 15 bins nos daba resultados muy buenos, y esta es la razón por la cual decidimos cambiar ese valor y documentarlo aquí."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab48702f",
   "metadata": {},
   "source": [
    "## Análisis de \"cortes\" de la matriz Q y la política"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3c54cce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[ 1.39350827 -1.21862235 -3.4506183  -1.27089773]\n",
      "[10.43430877  9.81707643  9.2750835  10.05266936]\n",
      "[8.84609998 8.62141893 7.5509486  8.55439629]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[ 0.61454678  0.61453768 -0.32261262  0.61446548]\n"
     ]
    }
   ],
   "source": [
    "# Estado:\n",
    "# (x, y, x_vel, y_vel, theta, theta_vel, pie_izq_en_contacto, pie_derecho_en_contacto)\n",
    "# Nave cayendo rapidamente hacia abajo sin rotacion en la matriz Q\n",
    "estado = discretize_state([0, 1.5, 0, -5, 0, 0, 0, 0], bins)\n",
    "print(agente.q_table[estado])\n",
    "# Nave cayendo rapidamente hacia abajo sin rotacion en la matriz Q\n",
    "estado = discretize_state([0, 1.5, 0, -4, 0, 0, 0, 0], bins)\n",
    "print(agente.q_table[estado])\n",
    "# Nave cayendo rapidamente hacia abajo sin rotacion en la matriz Q\n",
    "estado = discretize_state([0, 0.5, 0, -2, 0, 0, 0, 0], bins)\n",
    "print(agente.q_table[estado])\n",
    "estado_final = discretize_state([0, 0, 0, 0, 0, 0, 1, 1], bins)\n",
    "print(agente.q_table[estado_final])\n",
    "\n",
    "estado_2 = discretize_state([0, 0, 0, 0, 0, 0, 0, 1], bins)\n",
    "print(agente.q_table[estado_2])\n",
    "\n",
    "estado_3 = discretize_state([0, 0, 0, 0, 0, 0, 1, 0], bins)\n",
    "print(agente.q_table[estado_3])\n",
    "\n",
    "# Aterriza pero no en el centro der\n",
    "estado_4 = discretize_state([1.5, 0, 0, 0, 0, 0, 0, 0], bins)\n",
    "print(agente.q_table[estado_4])\n",
    "\n",
    "# Aterriza en el centro izq\n",
    "estado_5 = discretize_state([-1.5, 0, 0, 0, 0, 0, 0, 0], bins)\n",
    "print(agente.q_table[estado_5])\n",
    "\n",
    "# Estado inicial\n",
    "estado_6 = discretize_state([0, 1.5, 0, 0, 0, 0, 0, 0], bins)\n",
    "print(agente.q_table[estado_6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3825b32b",
   "metadata": {},
   "source": [
    "## Análisis de evolución de recompensa promedio\n",
    "\n",
    "### Fase inicial (Episodios 1 - 20.000):\n",
    "En los primeros 20.000 episodios, el agente tiene un desempeño consistentemente bajo, con recompensas promedio que oscilan entre -176 y -122. Esto es típico al inicio del entrenamiento, cuando el agente está explorando y cometiendo errores mientras aprende las dinámicas del entorno. Aquí vemos que el valor de la recompensa promedio mejora lentamente, pero sigue siendo negativo.\n",
    "\n",
    "### Fase de estabilización (Episodios 20.000 - 40.000):\n",
    "Entre los episodios 20,000 y 40,000, el agente muestra una tendencia hacia la estabilización. La recompensa promedio continúa mejorando, y se aproxima a -100 en varios puntos, pero aún se registran caídas, como los episodios en torno a los 35,000, donde la recompensa promedio disminuye brevemente. Esto puede indicar que el agente sigue cometiendo errores significativos, pero empieza a aprender estrategias básicas.\n",
    "\n",
    "### Mejora sostenida (Episodios 40.000 - 60.000):\n",
    "A partir de los 40.000 episodios, se observa una mejora más pronunciada en la recompensa promedio. El agente pasa de recompensas alrededor de -100 a valores cercanos a 0 cuando se acerca a los 60.000. Esto sugiere que el agente ha comenzado a entender mejor cómo aterrizar con éxito en el entorno LunarLander. La recompensa promedio incluso llega a valores positivos por primera vez alrededor de los 57.000 episodios, lo que indica un progreso significativo en el aprendizaje.\n",
    "\n",
    "### Fase de alto rendimiento (Episodios 60.000 - 100.000):\n",
    "A partir de los 60.000 episodios, el agente experimenta un incremento continuo en su desempeño. Las recompensas promedio se mantienen en su mayoría positivas, llegando a valores como 52, 59, e incluso 58 hacia el final del entrenamiento. Este periodo muestra que el agente ha alcanzado un nivel considerable de competencia, aunque sigue habiendo fluctuaciones, posiblemente debido a exploraciones que el agente realiza esporádicamente para optimizar su política de acciones.\n",
    "\n",
    "A continuación se grafica la recompensa promedio en intervalos de 1000 episodios. Se entrenó con 100.000 episodios y cada 1000 de ellos, se separaron y se calculó su recompensa promedio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9b5953d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (98,) and (1000,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Crear la gráfica\u001b[39;00m\n\u001b[1;32m      9\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m4\u001b[39m))\n\u001b[0;32m---> 10\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepisodios_ajustados\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecompensas_promedio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRecompensa Promedio\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mblue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Añadir etiquetas y título\u001b[39;00m\n\u001b[1;32m     13\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpisodios (Intervalos de 1000)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/matplotlib/pyplot.py:3794\u001b[0m, in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3786\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mplot)\n\u001b[1;32m   3787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot\u001b[39m(\n\u001b[1;32m   3788\u001b[0m     \u001b[38;5;241m*\u001b[39margs: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m ArrayLike \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3792\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3793\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Line2D]:\n\u001b[0;32m-> 3794\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3795\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3796\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscalex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscalex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3797\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscaley\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaley\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3798\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3799\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3800\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/matplotlib/axes/_axes.py:1779\u001b[0m, in \u001b[0;36mAxes.plot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;124;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1776\u001b[0m \u001b[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[1;32m   1777\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1778\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[38;5;241m.\u001b[39mLine2D)\n\u001b[0;32m-> 1779\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[1;32m   1781\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_line(line)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/matplotlib/axes/_base.py:296\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[0;34m(self, axes, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m     this \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    295\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 296\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/matplotlib/axes/_base.py:486\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[0;34m(self, axes, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    483\u001b[0m     axes\u001b[38;5;241m.\u001b[39myaxis\u001b[38;5;241m.\u001b[39mupdate_units(y)\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m--> 486\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y must have same first dimension, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    487\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhave shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    489\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y can be no greater than 2D, but have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    490\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (98,) and (1000,)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Datos de recompensa promedio a lo largo de los episodios\n",
    "episodios_ajustados = list(range(1, 100))\n",
    "\n",
    "recompensas_promedio = recompensa_episodios\n",
    "\n",
    "# Crear la gráfica\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(episodios_ajustados, recompensas_promedio, label='Recompensa Promedio', color='blue')\n",
    "\n",
    "# Añadir etiquetas y título\n",
    "plt.xlabel('Episodios (Intervalos de 1000)')\n",
    "plt.ylabel('Recompensa Promedio')\n",
    "plt.title('Evolución de la Recompensa Promedio por Intervalos de 1000 Episodios')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "# Mostrar la gráfica\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac3d75d",
   "metadata": {},
   "source": [
    "## Análisis de casos (Éxitos y Fallos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ed685b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para reproducir un episodio específico\n",
    "def reproducir_episodio(agente, episodio, semilla, max_iteraciones=500):\n",
    "    entorno = gym.make('LunarLander-v2', render_mode='human').env\n",
    "    entorno.reset(seed=semilla)\n",
    "    iteraciones = 0\n",
    "    termino = False\n",
    "    truncado = False\n",
    "    estado_anterior, info = entorno.reset(seed=semilla)\n",
    "    while iteraciones < max_iteraciones and not termino and not truncado:\n",
    "        accion = episodio[iteraciones][1]\n",
    "        estado_siguiente, recompensa, termino, truncado, info = entorno.step(accion)\n",
    "        entorno.render()\n",
    "        estado_anterior = estado_siguiente\n",
    "        iteraciones += 1\n",
    "    entorno.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dc736a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cbccc3e1",
   "metadata": {},
   "source": [
    "### Reproducir episodios exitosos y no exitosos de los casos de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e7768b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducir un episodio específico\n",
    "episodio_especifico = 31  # Cambia este valor al episodio que quieras reproducir\n",
    "if episodio_especifico < len(episodios_exitosos):\n",
    "    reproducir_episodio(agente, episodios_exitosos[episodio_especifico], semillas_exitosas[episodio_especifico])\n",
    "else:\n",
    "    print(f\"No hay suficientes episodios exitosos para reproducir el episodio {episodio_especifico}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "addc484e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducir un episodio específico\n",
    "episodio_especifico = 203  # Cambia este valor al episodio que quieras reproducir\n",
    "if episodio_especifico < len(episodios_no_exitosos):\n",
    "    reproducir_episodio(agente, episodios_no_exitosos[episodio_especifico], semillas_exitosas[episodio_especifico])\n",
    "else:\n",
    "    print(f\"No hay suficientes episodios no exitosos para reproducir el episodio {episodio_especifico}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a63f6a",
   "metadata": {},
   "source": [
    "### Episodios Exitosos y no exitosos\n",
    "\n",
    "La indeterminación del entorno en el problema del Lunar Lander proviene de la discretización de los estados. Dado que un mismo estado discretizado puede llevar a diferentes resultados debido a pequeñas variaciones que no se capturan en la discretización, el comportamiento del agente no siempre es predecible. El agente ha logrado aprender y adaptarse de manera efectiva en un (porcentaje) de los 1,000 episodios de prueba.\n",
    "\n",
    "1. Episodios Exitosos\n",
    "En los casos exitosos, el agente ha demostrado un control eficiente del propulsor principal, asegurándose de que la nave nunca descienda a una velocidad demasiado alta. Este control es fundamental para permitir que el agente pueda ajustar su orientación horizontal hacia la zona de aterrizaje. Desde el inicio, el agente toma decisiones que mantienen un equilibrio entre el uso del propulsor principal y los propulsores laterales, lo que le permite encadenar acciones efectivas que lo llevan a una recompensa exitosa.\n",
    "\n",
    "Un patrón clave en los episodios exitosos es que, al evitar el uso excesivo de los propulsores laterales en momentos inadecuados, el agente es capaz de mantener una trayectoria controlada hacia la meta. Esto es particularmente importante en las primeras etapas del episodio, donde las decisiones erróneas tienen un mayor impacto negativo en el aterrizaje. Al ejecutar correctamente los propulsores en los momentos críticos, el agente asegura un aterrizaje seguro y controlado.\n",
    "\n",
    "En cuanto a los episodios que no son exitosos podemos ver que una mala decision de propulsarse hacia un costado, siempre se vuelve muy dificil de corregir para el agente no pudiendo controlar su velocidad y saliendose del entorno dando un caso fallido teniendo tambien la variable de poder girar en su eje complejizando mas su recuperacion al \"camino\" correcto.\n",
    "\n",
    "Vemos en otros tantos episodios, que el agente se propulsa hacia arriba por demas, al comienzo de su ejecucion, y creemos que al \"salirse\" del entorno el caso ya se considera como fallido.\n",
    "\n",
    "Concluimos que la determinacion del exito del episodio, se basa mucho en las primeras decisiones que toma, dado a la sensibilidad que tienen las decisiones desde un comienzo y su poca habilidad de recuperacion ante malas decisiones.\n",
    "\n",
    "2. Episodios No Exitosos\n",
    "\n",
    "En los episodios fallidos, observamos que el principal factor que contribuye al fracaso es una decisión incorrecta de propulsarse hacia un costado al comienzo del episodio. Cuando el agente toma una mala decision al comienzo en este caso, propulsarse hacia un lado, pierde el control de la velocidad o de la orientación, lo que hace que sea muy difícil corregir la trayectoria posteriormente. En estos casos, el agente no logra reducir su velocidad o ajustar su orientación a tiempo, y termina saliéndose del entorno, lo que resulta en un episodio fallido.\n",
    "\n",
    "Otro patrón común en los episodios no exitosos es el uso excesivo del propulsor principal al inicio del episodio. Esto genera que la nave se desplace demasiado hacia arriba, y al \"salirse\" del entorno, esto sucede al principio también ya que la nave esta cerca del \"borde superior\" lo que causaria que una mala decision de propulsarse por demás al comienzo lo llevaria a fallar.\n",
    "\n",
    "3. Conclusiones sobre el Éxito y el Fracaso\n",
    "Podemos concluir que el éxito de un episodio está altamente influenciado por las primeras decisiones que toma el agente. Dado el alto nivel de sensibilidad del entorno a las acciones iniciales, una mala decisión al inicio del episodio puede llevar a una situación difícil de corregir. Parte de este problema radica en la limitación del agente a poder prender mas de un propulsor a la vez, asi como tambien la limitacion de tomar un espacio discretizado para la decisiones.\n",
    "\n",
    "La clave del éxito radica en que el agente logre encadenar un conjunto de decisiones correctas desde el principio, evitando movimientos innecesarios y ajustando cuidadosamente el uso de los propulsores. Cuando esto ocurre, el agente tiene una alta probabilidad de realizar un aterrizaje exitoso, minimizando las penalizaciones y maximizando las recompensas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f7d407",
   "metadata": {},
   "source": [
    "## Limitantes del agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f93fe904-e691-42ff-8fd4-b360d08431cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analizar los resultados aqui\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
