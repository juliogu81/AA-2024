{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d13bbe5-80d1-4879-bc47-d8e366f38456",
   "metadata": {},
   "source": [
    "# **Lunar Lander con Q-Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82127016-b75d-4621-a53e-8b2bb63cd3f8",
   "metadata": {},
   "source": [
    "### **1. Bibliotecas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6e0a19d-696b-440e-84eb-fe2b8761d955",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# Tal vez tengan que ejecutar lo siguiente en sus máquinas (ubuntu 20.04)\n",
    "# sudo apt-get remove swig\n",
    "# sudo apt-get install swig3.0\n",
    "# sudo ln -s /usr/bin/swig3.0 /usr/bin/swig\n",
    "# En windows tambien puede ser necesario MSVC++\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9831e9d4-7840-485c-bc38-af987a76de4f",
   "metadata": {},
   "source": [
    "### **2. Jugando a mano**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e744255-5e3f-4c17-b9dc-c6374c2f06c2",
   "metadata": {},
   "source": [
    "A continuación se puede jugar un episodio del lunar lander. Se controlan los motores con el teclado. Notar que solo se puede realizar una acción a la vez (que es parte del problema), y que en esta implementación, izq toma precedencia sobre derecha, que toma precedencia sobre el motor principal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6211ed30-b1a3-4b8e-9858-17eee433ddca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nota: hay que transformar esta celda en código para ejecutar (Esc + y)\n",
    "\n",
    "import pygame\n",
    "from pygame.locals import *\n",
    "\n",
    "# Inicializar pygame (para el control con el teclado) y el ambiente\n",
    "pygame.init()\n",
    "env = gym.make('LunarLander-v2', render_mode='human')\n",
    "env.reset()\n",
    "pygame.display.set_caption('Lunar Lander')\n",
    "\n",
    "clock = pygame.time.Clock()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    for event in pygame.event.get():\n",
    "        if event.type == QUIT:\n",
    "            done = True\n",
    "            break\n",
    "\n",
    "    keys = pygame.key.get_pressed()\n",
    "\n",
    "    # Map keys to actions\n",
    "    if keys[K_LEFT]:\n",
    "        action = 3  # Fire left orientation engine\n",
    "    elif keys[K_RIGHT]:\n",
    "        action = 1 # Fire right orientation engine\n",
    "    elif keys[K_UP]:\n",
    "        action = 2  # Fire main engine\n",
    "    else:\n",
    "        action = 0  # Do nothing\n",
    "\n",
    "    _, _, terminated, truncated, _ = env.step(action)\n",
    "    env.render()\n",
    "    clock.tick(10)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        done = True\n",
    "\n",
    "env.close()\n",
    "pygame.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d721a629-7f3f-4a70-83c0-e12f43b0f285",
   "metadata": {},
   "source": [
    "## **3. Discretizando el estado**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d35189-3a15-4ceb-b978-38226518092a",
   "metadata": {},
   "source": [
    "El estado consiste de posiciones y velocidades en (x,y,theta) y en información de contacto de los pies con la superficie.\n",
    "\n",
    "Como varios de estos son continuos, tenemos que discretizarlos para aplicar nuestro algoritmo de aprendizaje por refuerzo tabular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b6bed3-12ba-4d92-8b09-3d3f8429217f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cuántos bins queremos por dimensión\n",
    "# Pueden considerar variar este parámetro\n",
    "bins_per_dim = 15\n",
    "\n",
    "# Estado:\n",
    "# (x, y, x_vel, y_vel, theta, theta_vel, pie_izq_en_contacto, pie_derecho_en_contacto)\n",
    "NUM_BINS = [bins_per_dim, bins_per_dim, bins_per_dim, bins_per_dim, bins_per_dim, bins_per_dim, 2, 2]\n",
    "\n",
    "env = gym.make('LunarLander-v2')\n",
    "env.reset()\n",
    "\n",
    "# Tomamos los rangos del env\n",
    "OBS_SPACE_HIGH = env.observation_space.high\n",
    "OBS_SPACE_LOW = env.observation_space.low\n",
    "OBS_SPACE_LOW[1] = 0 # Para la coordenada y (altura), no podemos ir más abajo que la zona dea aterrizae (que está en el 0, 0)\n",
    "\n",
    "# Los bins para cada dimensión\n",
    "bins = [\n",
    "    np.linspace(OBS_SPACE_LOW[i], OBS_SPACE_HIGH[i], NUM_BINS[i] - 1)\n",
    "    for i in range(len(NUM_BINS) - 2) # last two are binary\n",
    "]\n",
    "# Se recomienda observar los bins para entender su estructura\n",
    "#print (\"Bins: \", bins)\n",
    "def discretize_state(state, bins):\n",
    "    \"\"\"Discretize the continuous state into a tuple of discrete indices.\"\"\"\n",
    "    state_disc = list()\n",
    "    for i in range(len(state)):\n",
    "        if i >= len(bins):  # For binary features (leg contacts)\n",
    "            state_disc.append(int(state[i]))\n",
    "        else:\n",
    "            state_disc.append(\n",
    "                np.digitize(state[i], bins[i])\n",
    "            )\n",
    "    return tuple(state_disc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bcc921-c5f6-4f06-9702-6e740b26fc6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(np.int64(7), np.int64(1), np.int64(7), np.int64(7), np.int64(7), np.int64(7), 1, 1)\n",
      "(np.int64(7), np.int64(14), np.int64(7), np.int64(7), np.int64(7), np.int64(7), 0, 0)\n"
     ]
    }
   ],
   "source": [
    "# Ejemplos\n",
    "print(discretize_state([0.0, 0.0, 0, 0, 0, 0, 1, 1], bins)) # En la zona de aterrizaje y quieto\n",
    "print(discretize_state([0, 1.5, 0, 0, 0, 0, 0, 0], bins)) # Comenzando la partida, arriba y en el centro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779c1576-eff2-4b3c-8069-f6d30243f1e6",
   "metadata": {},
   "source": [
    "## **4. Agentes y la interacción con el entorno**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d594f89c-70b1-4c7c-939f-d654a5263f1c",
   "metadata": {},
   "source": [
    "Vamos a definir una interfaz para nuestro agente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93e7059a-7b68-4b13-857b-16e96c5f9793",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agente:\n",
    "    def elegir_accion(self, estado, max_accion, explorar = True) -> int:\n",
    "        \"\"\"Elegir la accion a tomar en el estado actual y el espacio de acciones\n",
    "            - estado_anterior: el estado desde que se empezó\n",
    "            - estado_siguiente: el estado al que se llegó\n",
    "            - accion: la acción que llevo al agente desde estado_anterior a estado_siguiente\n",
    "            - recompensa: la recompensa recibida en la transicion\n",
    "            - terminado: si el episodio terminó\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def aprender(self, estado_anterior, estado_siguiente, accion, recompensa, terminado):\n",
    "        \"\"\"Aprender a partir de la tupla \n",
    "            - estado_anterior: el estado desde que se empezó\n",
    "            - estado_siguiente: el estado al que se llegó\n",
    "            - accion: la acción que llevo al agente desde estado_anterior a estado_siguiente\n",
    "            - recompensa: la recompensa recibida en la transicion\n",
    "            - terminado: si el episodio terminó en esta transición\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def fin_episodio(self):\n",
    "        \"\"\"Actualizar estructuras al final de un episodio\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c61e38-f7d3-40bf-af19-c7fefd143ffd",
   "metadata": {},
   "source": [
    "Para un agente aleatorio, la implementación sería:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3adcedd-b300-4e30-9cb1-19d5ec96fb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class AgenteAleatorio(Agente):\n",
    "    def elegir_accion(self, estado, max_accion, explorar = True) -> int:\n",
    "        # Elige una acción al azar\n",
    "        return random.randrange(max_accion)\n",
    "\n",
    "    def aprender(self, estado_anterior, estado_siguiente, accion, recompensa, terminado):\n",
    "        # No aprende\n",
    "        pass\n",
    "\n",
    "    def fin_episodio(self):\n",
    "        # Nada que actualizar\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19498b0d-eb74-431e-ac7f-612497ca07f3",
   "metadata": {},
   "source": [
    "Luego podemos definir una función para ejecutar un episodio con un agente dado:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408a5109",
   "metadata": {},
   "source": [
    "#### Función para ejecutar episodio en entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa15ab0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ejecutar_episodio_1(agente, aprender = True, render = None, max_iteraciones=500):\n",
    "    entorno = gym.make('LunarLander-v2', render_mode=render).env\n",
    "    \n",
    "    iteraciones = 0\n",
    "    recompensa_total = 0\n",
    "\n",
    "    termino = False\n",
    "    truncado = False\n",
    "    estado_anterior, info = entorno.reset()\n",
    "    while iteraciones < max_iteraciones and not termino and not truncado:\n",
    "        # Le pedimos al agente que elija entre las posibles acciones (0..entorno.action_space.n)\n",
    "        accion = agente.elegir_accion(estado_anterior, entorno.action_space.n, aprender)\n",
    "        # Realizamos la accion\n",
    "        estado_siguiente, recompensa, termino, truncado, info = entorno.step(accion)\n",
    "        # Le informamos al agente para que aprenda\n",
    "        if (aprender):\n",
    "            agente.aprender(estado_anterior, estado_siguiente, accion, recompensa, termino)\n",
    "\n",
    "        estado_anterior = estado_siguiente\n",
    "        iteraciones += 1\n",
    "        recompensa_total += recompensa\n",
    "    if (aprender):\n",
    "        agente.fin_episodio()\n",
    "    entorno.close()\n",
    "    return recompensa_total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2356d7",
   "metadata": {},
   "source": [
    "#### Función para ejecutar episodios y guardar los entornos para poder reproducirlos a la hora del análisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d06b7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ejecutar_episodio_2(agente, aprender=True, max_iteraciones=500):\n",
    "    entorno = gym.make('LunarLander-v2').env\n",
    "    semilla = entorno.np_random.bit_generator._seed_seq.entropy\n",
    "    iteraciones = 0\n",
    "    recompensa_total = 0\n",
    "    episodio = []\n",
    "\n",
    "    termino = False\n",
    "    truncado = False\n",
    "    estado_anterior, info = entorno.reset(seed=semilla)\n",
    "    while iteraciones < max_iteraciones and not termino and not truncado:\n",
    "        # Le pedimos al agente que elija entre las posibles acciones (0..entorno.action_space.n)\n",
    "        accion = agente.elegir_accion(estado_anterior, entorno.action_space.n, aprender)\n",
    "        # Realizamos la accion\n",
    "        estado_siguiente, recompensa, termino, truncado, info = entorno.step(accion)\n",
    "        # Le informamos al agente para que aprenda\n",
    "        if aprender:\n",
    "            agente.aprender(estado_anterior, estado_siguiente, accion, recompensa, termino)\n",
    "\n",
    "        # Almacenar el estado, acción y recompensa\n",
    "        episodio.append((estado_anterior, accion, recompensa))\n",
    "\n",
    "        estado_anterior = estado_siguiente\n",
    "        iteraciones += 1\n",
    "        recompensa_total += recompensa\n",
    "\n",
    "    if aprender:\n",
    "        agente.fin_episodio()\n",
    "    entorno.close()\n",
    "    return recompensa_total, episodio, semilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a104779-f5e3-4bfa-b88b-fb44de195d49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(-287.40263757335816)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nota: hay que transformar esta celda en código para ejecutar (Esc + y)\n",
    "\n",
    "# Ejecutamos un episodio con el agente aleatorio y modo render 'human', para poder verlo\n",
    "ejecutar_episodio_1(AgenteAleatorio(), render = 'human')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b9cb2a-37b0-4115-afe8-bb89ce49f605",
   "metadata": {},
   "source": [
    "Podemos ejecutar este ambiente muchas veces y tomar métricas al respecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "536e81ee-6038-44c7-b887-35db442815ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasa de éxito: 0.0. Se obtuvo -179.65198126971825 de recompensa, en promedio\n"
     ]
    }
   ],
   "source": [
    "agente = AgenteAleatorio()\n",
    "recompensa_episodios = []\n",
    "\n",
    "exitos = 0\n",
    "num_episodios = 100\n",
    "for i in range(num_episodios):\n",
    "    recompensa = ejecutar_episodio_1(agente)\n",
    "    # Los episodios se consideran exitosos si se obutvo 200 o más de recompensa total\n",
    "    if (recompensa >= 200):\n",
    "        exitos += 1\n",
    "    recompensa_episodios += [recompensa]\n",
    "\n",
    "import numpy\n",
    "print(f\"Tasa de éxito: {exitos / num_episodios}. Se obtuvo {numpy.mean(recompensa_episodios)} de recompensa, en promedio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086cb250-7bc9-4bd4-a4cd-43cfd561facb",
   "metadata": {},
   "source": [
    "### **5. Programando un agente que aprende**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0827e3e2-a84a-462f-8b86-e03425cfc645",
   "metadata": {},
   "source": [
    "La tarea a realizar consiste en programar un agente de aprendizaje por refuerzos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3f37011-ffaa-4d08-b832-c45d0b9060da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "class AgenteRL(Agente):\n",
    "    # Agregar código aqui\n",
    "\n",
    "    # Pueden agregar parámetros al constructor\n",
    "    def __init__(self, max_accion, bins, initial_epsilon: float, epsilon_decay: float, final_epsilon: float, discount_factor: float = 0.95):\n",
    "        super().__init__()\n",
    "\n",
    "        self.q_table = defaultdict(lambda: np.zeros(max_accion))\n",
    "        self.visitas = defaultdict(lambda: np.zeros(max_accion))\n",
    "\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "        self.epsilon = initial_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.final_epsilon = final_epsilon\n",
    "\n",
    "        self.bins = bins\n",
    "    \n",
    "    def elegir_accion(self, estado, max_accion, explorar = True) -> int:\n",
    "\n",
    "        estado_discreto = discretize_state(estado, self.bins)\n",
    "        if (explorar and np.random.random() < self.epsilon):\n",
    "            return random.randrange(max_accion)\n",
    "        else:\n",
    "            return int(np.argmax(self.q_table[estado_discreto]))\n",
    "    \n",
    "    def aprender(self, estado_anterior, estado_siguiente, accion, recompensa, terminado):\n",
    "        estado_anterior_discreto = discretize_state(estado_anterior, self.bins)\n",
    "        estado_siguiente_discreto = discretize_state(estado_siguiente, self.bins)\n",
    "\n",
    "        self.visitas[estado_anterior_discreto][accion] += 1\n",
    "\n",
    "        future_q_value = (not terminado) * np.max(self.q_table[estado_siguiente_discreto])\n",
    "        temporal_difference = (\n",
    "            recompensa + self.discount_factor * future_q_value - self.q_table[estado_anterior_discreto][accion]\n",
    "        )\n",
    "\n",
    "        alpha = 1 / self.visitas[estado_anterior_discreto][accion]\n",
    "\n",
    "        self.q_table[estado_anterior_discreto][accion] = (\n",
    "            self.q_table[estado_anterior_discreto][accion] + alpha * temporal_difference\n",
    "        )\n",
    "    def fin_episodio(self):\n",
    "        self.epsilon = max(self.final_epsilon, self.epsilon - self.epsilon_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ee51f5-aff8-4d18-934e-92acdcb617c0",
   "metadata": {},
   "source": [
    "Y ejecutar con el muchos episodios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e0524d5-0d12-46a8-8437-984b981fbae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100000 [00:00<?, ?it/s]/home/julio/.local/lib/python3.10/site-packages/numpy/_core/fromnumeric.py:3904: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/julio/.local/lib/python3.10/site-packages/numpy/_core/_methods.py:147: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "  0%|          | 9/100000 [00:00<26:52, 61.99it/s, promedio=-248, recompensa=-332] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo nan de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1013/100000 [00:16<26:09, 63.07it/s, promedio=-178, recompensa=-221] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -178.30962215145314 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2011/100000 [00:32<27:45, 58.85it/s, promedio=-178, recompensa=-387] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -177.14323287314554 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3011/100000 [00:49<25:41, 62.92it/s, promedio=-174, recompensa=-479]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -166.63590724152357 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4010/100000 [01:06<26:41, 59.93it/s, promedio=-169, recompensa=-21.1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -156.0996278873348 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5011/100000 [01:23<28:34, 55.40it/s, promedio=-166, recompensa=-195] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -153.5853381574535 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6011/100000 [01:39<27:16, 57.42it/s, promedio=-164, recompensa=-189] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -150.40977335661918 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 7012/100000 [01:57<26:39, 58.15it/s, promedio=-163, recompensa=16.9] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -157.02781958066942 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8008/100000 [02:13<30:08, 50.88it/s, promedio=-161, recompensa=-178] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -149.16058700894416 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 9012/100000 [02:31<25:44, 58.92it/s, promedio=-159, recompensa=-111] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -146.51300494302993 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 10011/100000 [02:48<25:25, 58.99it/s, promedio=-158, recompensa=-129] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -143.32178123647273 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 11012/100000 [03:06<26:23, 56.20it/s, promedio=-156, recompensa=-93]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -140.31638485021008 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 12011/100000 [03:24<27:28, 53.37it/s, promedio=-154, recompensa=-166]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -135.74039565860357 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 13010/100000 [03:43<27:04, 53.56it/s, promedio=-153, recompensa=-94.2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -137.30188149479312 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 14010/100000 [04:02<26:01, 55.06it/s, promedio=-152, recompensa=-14]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -135.49286414232364 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 15011/100000 [04:21<26:09, 54.15it/s, promedio=-151, recompensa=-32.6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -140.08643124734877 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 16009/100000 [04:41<29:06, 48.08it/s, promedio=-150, recompensa=-221] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -132.53071696733386 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 17010/100000 [05:02<27:08, 50.96it/s, promedio=-149, recompensa=-82.9] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -137.19108100593633 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 18008/100000 [05:24<29:48, 45.85it/s, promedio=-148, recompensa=-133]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -131.37739460247 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 19011/100000 [05:46<29:09, 46.30it/s, promedio=-147, recompensa=-229]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -128.46887439789867 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 20009/100000 [06:07<28:27, 46.84it/s, promedio=-146, recompensa=-61.2] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -121.71547971191798 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 21010/100000 [06:31<29:20, 44.87it/s, promedio=-144, recompensa=-25.6] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -113.4655534435826 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 22008/100000 [06:55<28:22, 45.81it/s, promedio=-143, recompensa=-130]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -111.14026376048139 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 23008/100000 [07:20<31:21, 40.91it/s, promedio=-141, recompensa=-131]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -107.27149117879766 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 24007/100000 [07:46<33:57, 37.30it/s, promedio=-140, recompensa=-121]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -110.05343784924872 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 25008/100000 [08:12<34:01, 36.73it/s, promedio=-139, recompensa=-51.8] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -105.77891106449705 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 26009/100000 [08:40<30:21, 40.62it/s, promedio=-137, recompensa=3.39]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -100.02385759035646 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 27006/100000 [09:08<36:08, 33.66it/s, promedio=-136, recompensa=-120]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -103.30193406936641 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 28008/100000 [09:37<32:40, 36.71it/s, promedio=-135, recompensa=-74.1] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -102.81607474033586 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 29005/100000 [10:07<41:56, 28.21it/s, promedio=-133, recompensa=-194]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -98.01344443975684 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 30005/100000 [10:37<32:12, 36.21it/s, promedio=-132, recompensa=-185]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -99.35332268997865 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 31006/100000 [11:07<33:32, 34.28it/s, promedio=-131, recompensa=-72.7] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -99.03522713612155 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 32005/100000 [11:40<39:14, 28.87it/s, promedio=-130, recompensa=-121]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -106.19527616506349 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 33005/100000 [12:14<34:14, 32.61it/s, promedio=-130, recompensa=-259] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -107.32993237159262 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 34008/100000 [12:48<30:49, 35.68it/s, promedio=-129, recompensa=-64.7]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -104.64184874034699 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 35005/100000 [13:22<50:57, 21.26it/s, promedio=-128, recompensa=-88.1] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -104.12168416939953 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 36004/100000 [13:58<36:57, 28.86it/s, promedio=-128, recompensa=-171]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -102.24980681222938 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 37006/100000 [14:35<36:13, 28.98it/s, promedio=-127, recompensa=31]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -105.82493413987395 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 38005/100000 [15:13<43:25, 23.79it/s, promedio=-127, recompensa=-270]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -112.9561264472749 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 39005/100000 [15:54<49:51, 20.39it/s, promedio=-126, recompensa=-147]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -98.39909838609363 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 40005/100000 [16:35<39:48, 25.12it/s, promedio=-125, recompensa=-129]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -103.98777224882609 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 41005/100000 [17:16<37:25, 26.28it/s, promedio=-125, recompensa=-34.4] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -93.84822762658023 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 42004/100000 [18:03<54:21, 17.78it/s, promedio=-124, recompensa=-225]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -83.71563309931257 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 43004/100000 [18:51<41:58, 22.63it/s, promedio=-122, recompensa=85.9]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -69.62934458094564 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 44004/100000 [19:41<46:47, 19.94it/s, promedio=-121, recompensa=-59.4]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -62.159525559162496 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 45004/100000 [20:31<48:27, 18.91it/s, promedio=-120, recompensa=-154]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -58.30332279090997 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 46004/100000 [21:21<45:50, 19.63it/s, promedio=-118, recompensa=-71.8]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -59.46256538466249 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 47004/100000 [22:11<41:54, 21.07it/s, promedio=-117, recompensa=-170]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -57.36069342688745 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 48003/100000 [23:04<48:42, 17.79it/s, promedio=-116, recompensa=-187]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -46.92348417600283 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 49003/100000 [23:56<41:23, 20.53it/s, promedio=-114, recompensa=22.5]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -36.48115307616393 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 50003/100000 [24:49<49:21, 16.88it/s, promedio=-112, recompensa=-7.96] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -29.73041884665729 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 51002/100000 [25:43<47:04, 17.35it/s, promedio=-110, recompensa=-12.2]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -13.19079167834172 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 52003/100000 [26:37<46:04, 17.36it/s, promedio=-108, recompensa=-93.2]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -9.042904136161809 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 53004/100000 [27:31<45:16, 17.30it/s, promedio=-106, recompensa=-29.1] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 4.478410877203989 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 54005/100000 [28:24<39:09, 19.58it/s, promedio=-104, recompensa=234]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 14.114342894390575 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 55004/100000 [29:17<39:40, 18.90it/s, promedio=-102, recompensa=-11.7] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 18.22848054035371 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 56003/100000 [30:10<38:35, 19.00it/s, promedio=-99.6, recompensa=0.897]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 20.600086248073122 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 57003/100000 [31:03<44:33, 16.08it/s, promedio=-97.3, recompensa=-16.1] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 28.328707494162366 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 58004/100000 [31:58<37:05, 18.87it/s, promedio=-95.3, recompensa=54.8]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 21.72510346483375 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 59003/100000 [32:53<43:05, 15.85it/s, promedio=-93.1, recompensa=254]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 33.86432355525344 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 60003/100000 [33:47<32:59, 20.21it/s, promedio=-91, recompensa=246]      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 33.464359398009385 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 61004/100000 [34:41<36:30, 17.80it/s, promedio=-88.9, recompensa=279]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 37.12068765598172 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 62003/100000 [35:35<36:15, 17.46it/s, promedio=-86.7, recompensa=-199]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 49.46232674103394 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 63004/100000 [36:28<36:31, 16.88it/s, promedio=-84.7, recompensa=81.6]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 33.158454343147326 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 64003/100000 [37:20<30:03, 19.96it/s, promedio=-82.9, recompensa=234]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 36.0210111190184 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 65005/100000 [38:12<29:40, 19.65it/s, promedio=-81, recompensa=61.3]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 39.645029074010466 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 66003/100000 [39:05<36:03, 15.71it/s, promedio=-79.1, recompensa=-117]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 41.67608284970137 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 67004/100000 [39:58<30:38, 17.95it/s, promedio=-77.2, recompensa=-12.1]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 49.022077954992874 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 68003/100000 [40:51<28:31, 18.70it/s, promedio=-75.3, recompensa=197]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 50.603372023506324 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 69003/100000 [41:46<28:11, 18.32it/s, promedio=-73.4, recompensa=-41.9] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 54.9872267154965 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 70003/100000 [42:39<27:16, 18.33it/s, promedio=-71.6, recompensa=207]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 55.87549012454367 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 71004/100000 [43:34<24:25, 19.78it/s, promedio=-69.9, recompensa=29.5]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 51.042670932483304 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 72004/100000 [44:26<24:03, 19.39it/s, promedio=-68.3, recompensa=8.79]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 44.64311132765219 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 73005/100000 [45:20<21:22, 21.05it/s, promedio=-66.6, recompensa=-98.2] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 52.756313754607106 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 74003/100000 [46:13<21:19, 20.31it/s, promedio=-64.8, recompensa=-167]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 68.582319128094 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 75004/100000 [47:08<24:47, 16.80it/s, promedio=-63, recompensa=37.2]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 69.07649858181915 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 76003/100000 [48:02<21:33, 18.55it/s, promedio=-61.3, recompensa=-96.5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 65.99510982407513 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 77003/100000 [48:56<19:50, 19.31it/s, promedio=-59.3, recompensa=297]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 90.14030554157458 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 78004/100000 [49:50<19:21, 18.95it/s, promedio=-57.4, recompensa=-31.9] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 89.58594231585174 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 79002/100000 [50:42<19:45, 17.72it/s, promedio=-55.7, recompensa=178]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 81.08363932901422 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 80003/100000 [51:34<17:55, 18.59it/s, promedio=-53.9, recompensa=240]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 86.36519056973343 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 81003/100000 [52:26<15:36, 20.29it/s, promedio=-52.3, recompensa=35]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 77.57750456213954 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 82003/100000 [53:18<16:37, 18.04it/s, promedio=-50.6, recompensa=24.4]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 89.11683460284235 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 83003/100000 [54:11<15:16, 18.55it/s, promedio=-48.9, recompensa=228]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 82.86088180303446 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 84005/100000 [55:05<13:16, 20.09it/s, promedio=-47.3, recompensa=255]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 88.88671999634231 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 85003/100000 [55:56<13:54, 17.98it/s, promedio=-45.8, recompensa=-228]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 81.54989970218577 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 86004/100000 [56:51<12:55, 18.05it/s, promedio=-44.3, recompensa=284]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 83.30713633330633 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 87002/100000 [57:43<11:21, 19.09it/s, promedio=-42.8, recompensa=264]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 82.8744817405847 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 88002/100000 [58:36<09:15, 21.61it/s, promedio=-41.3, recompensa=-62]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 93.09298153280264 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 89004/100000 [59:29<09:05, 20.17it/s, promedio=-39.9, recompensa=32.2]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 78.54661370765002 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 90003/100000 [1:00:23<08:46, 18.99it/s, promedio=-38.5, recompensa=-60.9] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 86.68024645509469 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 91003/100000 [1:01:17<09:21, 16.03it/s, promedio=-37.2, recompensa=282]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 81.60658800655231 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 92003/100000 [1:02:11<06:52, 19.41it/s, promedio=-35.9, recompensa=-52]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 82.07130160213107 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 93003/100000 [1:03:04<05:49, 20.00it/s, promedio=-34.8, recompensa=-148]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 69.79688910161485 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 94005/100000 [1:03:59<04:38, 21.54it/s, promedio=-33.5, recompensa=56.9] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 81.92814693317246 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 95005/100000 [1:04:53<04:04, 20.39it/s, promedio=-32.3, recompensa=-33.7]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 80.02574581310691 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 96005/100000 [1:05:48<03:00, 22.17it/s, promedio=-31.1, recompensa=-8.3]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 83.94038223473132 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 97002/100000 [1:06:41<02:37, 19.03it/s, promedio=-30, recompensa=117]      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 78.8724879608361 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 98004/100000 [1:07:34<01:49, 18.22it/s, promedio=-28.9, recompensa=-27.7] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 79.23889499410207 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 99004/100000 [1:08:28<00:51, 19.45it/s, promedio=-27.7, recompensa=194]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo 85.7562164600896 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [1:09:21<00:00, 24.03it/s, promedio=-26.7, recompensa=161]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasa de éxito: 0.15302. Se obtuvo -26.69627197753364 de recompensa, en promedio\n"
     ]
    }
   ],
   "source": [
    "# Nota: hay que transformar esta celda en código para ejecutar (Esc + y)\n",
    "# Advertencia: este bloque es un loop infinito si el agente se deja sin implementar\n",
    "from tqdm import tqdm \n",
    "entorno = gym.make('LunarLander-v2').env\n",
    "num_episodios = 100000\n",
    "max_accion = entorno.action_space.n\n",
    "initial_epsilon = 1.0\n",
    "epsilon_decay = initial_epsilon / (num_episodios / 2)\n",
    "final_epsilon = 0.05\n",
    "discount_factor = 0.95\n",
    "\n",
    "agente = AgenteRL(max_accion = max_accion, bins = bins, initial_epsilon = initial_epsilon, epsilon_decay = epsilon_decay, final_epsilon = final_epsilon, discount_factor = discount_factor)\n",
    "exitos = 0\n",
    "recompensa_episodios = []\n",
    "promedio_recompensa = []\n",
    "with tqdm(total=num_episodios) as pbar:\n",
    "    for i in range(num_episodios):\n",
    "        recompensa = ejecutar_episodio_1(agente)\n",
    "        if i % 1000 == 0:\n",
    "            ultimas_1000_recompensas = recompensa_episodios[-1000:]\n",
    "            promedio_ultimas_1000 = np.mean(ultimas_1000_recompensas)\n",
    "            promedio_recompensa.append(promedio_ultimas_1000)\n",
    "            print(f\" Se obtuvo {promedio_ultimas_1000} de recompensa, en promedio en los últimos 1000 episodios\")\n",
    "        if (recompensa >= 200):\n",
    "            exitos += 1\n",
    "        recompensa_episodios += [recompensa]\n",
    "        pbar.set_postfix(recompensa=recompensa, promedio=numpy.mean(recompensa_episodios))\n",
    "        pbar.update(1)\n",
    "print(f\"Tasa de éxito: {exitos / num_episodios}. Se obtuvo {numpy.mean(recompensa_episodios)} de recompensa, en promedio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b7079e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 0 exitoso, recompensa: 239.1143573649777\n",
      "Episodio 5 exitoso, recompensa: 244.5046974385059\n",
      "Episodio 8 exitoso, recompensa: 246.384386971004\n",
      "Episodio 11 exitoso, recompensa: 222.0098044187236\n",
      "Episodio 12 exitoso, recompensa: 208.36103912228089\n",
      "Episodio 15 exitoso, recompensa: 290.4044094758466\n",
      "Episodio 20 exitoso, recompensa: 213.97376633068927\n",
      "Episodio 25 exitoso, recompensa: 265.27254166587943\n",
      "Episodio 26 exitoso, recompensa: 247.1759790171863\n",
      "Episodio 30 exitoso, recompensa: 214.4352864593349\n",
      "Episodio 31 exitoso, recompensa: 281.76244658944626\n",
      "Episodio 41 exitoso, recompensa: 234.40188599570072\n",
      "Episodio 42 exitoso, recompensa: 220.8549133318283\n",
      "Episodio 43 exitoso, recompensa: 207.78471747519706\n",
      "Episodio 44 exitoso, recompensa: 229.1056302530063\n",
      "Episodio 45 exitoso, recompensa: 227.33896596897617\n",
      "Episodio 50 exitoso, recompensa: 230.32615302714956\n",
      "Episodio 51 exitoso, recompensa: 248.2356895366254\n",
      "Episodio 52 exitoso, recompensa: 268.70051718795287\n",
      "Episodio 54 exitoso, recompensa: 268.8076237073334\n",
      "Episodio 62 exitoso, recompensa: 286.6281064002038\n",
      "Episodio 65 exitoso, recompensa: 212.35190682672302\n",
      "Episodio 67 exitoso, recompensa: 219.41082711523364\n",
      "Episodio 71 exitoso, recompensa: 278.0171042861017\n",
      "Episodio 78 exitoso, recompensa: 223.98428149527905\n",
      "Episodio 81 exitoso, recompensa: 265.6854458266432\n",
      "Episodio 86 exitoso, recompensa: 249.8743463352761\n",
      "Episodio 89 exitoso, recompensa: 224.50250783187226\n",
      "Episodio 91 exitoso, recompensa: 253.30782758856614\n",
      "Episodio 93 exitoso, recompensa: 277.711356513022\n",
      "Episodio 97 exitoso, recompensa: 242.73953785717575\n",
      "Episodio 98 exitoso, recompensa: 270.75316150612304\n",
      "Episodio 100 exitoso, recompensa: 233.42909427599824\n",
      "Episodio 103 exitoso, recompensa: 278.5094908042952\n",
      "Episodio 107 exitoso, recompensa: 240.80103035829384\n",
      "Episodio 108 exitoso, recompensa: 232.86523474622078\n",
      "Episodio 110 exitoso, recompensa: 258.4617669978029\n",
      "Episodio 117 exitoso, recompensa: 203.78870124913612\n",
      "Episodio 118 exitoso, recompensa: 218.87322091886836\n",
      "Episodio 119 exitoso, recompensa: 294.85667847530624\n",
      "Episodio 121 exitoso, recompensa: 283.0699459612467\n",
      "Episodio 124 exitoso, recompensa: 233.3811301474153\n",
      "Episodio 126 exitoso, recompensa: 250.49463076217228\n",
      "Episodio 129 exitoso, recompensa: 284.0393364869311\n",
      "Episodio 130 exitoso, recompensa: 229.2069330096378\n",
      "Episodio 136 exitoso, recompensa: 265.1162978407909\n",
      "Episodio 137 exitoso, recompensa: 247.58683415376228\n",
      "Episodio 143 exitoso, recompensa: 308.4683306948657\n",
      "Episodio 144 exitoso, recompensa: 215.21711686542614\n",
      "Episodio 148 exitoso, recompensa: 249.31155036160848\n",
      "Episodio 155 exitoso, recompensa: 270.9961437311186\n",
      "Episodio 157 exitoso, recompensa: 221.65729087389997\n",
      "Episodio 160 exitoso, recompensa: 270.0017793305761\n",
      "Episodio 162 exitoso, recompensa: 272.325616606528\n",
      "Episodio 163 exitoso, recompensa: 235.72459636906018\n",
      "Episodio 165 exitoso, recompensa: 228.2385437684149\n",
      "Episodio 166 exitoso, recompensa: 250.0581000832016\n",
      "Episodio 169 exitoso, recompensa: 265.95432348804957\n",
      "Episodio 170 exitoso, recompensa: 262.23172223748355\n",
      "Episodio 177 exitoso, recompensa: 294.3961812837721\n",
      "Episodio 178 exitoso, recompensa: 257.3628538043031\n",
      "Episodio 183 exitoso, recompensa: 205.2115475157913\n",
      "Episodio 189 exitoso, recompensa: 228.71453169415352\n",
      "Episodio 194 exitoso, recompensa: 237.7851050738507\n",
      "Episodio 196 exitoso, recompensa: 261.87754871009645\n",
      "Episodio 201 exitoso, recompensa: 221.40786878839836\n",
      "Episodio 202 exitoso, recompensa: 208.4781506192598\n",
      "Episodio 205 exitoso, recompensa: 244.00497520586293\n",
      "Episodio 210 exitoso, recompensa: 225.972009369948\n",
      "Episodio 211 exitoso, recompensa: 229.89845595874087\n",
      "Episodio 213 exitoso, recompensa: 272.7071948084009\n",
      "Episodio 214 exitoso, recompensa: 242.41476730886933\n",
      "Episodio 215 exitoso, recompensa: 228.62065266093313\n",
      "Episodio 220 exitoso, recompensa: 229.04212094743448\n",
      "Episodio 228 exitoso, recompensa: 254.60792020674654\n",
      "Episodio 229 exitoso, recompensa: 247.81450278728204\n",
      "Episodio 230 exitoso, recompensa: 228.27634518378386\n",
      "Episodio 232 exitoso, recompensa: 206.0241485982212\n",
      "Episodio 238 exitoso, recompensa: 220.12071047463968\n",
      "Episodio 248 exitoso, recompensa: 229.3134301487938\n",
      "Episodio 249 exitoso, recompensa: 222.62531840178178\n",
      "Episodio 254 exitoso, recompensa: 223.9674033267907\n",
      "Episodio 255 exitoso, recompensa: 231.60590896126715\n",
      "Episodio 258 exitoso, recompensa: 209.46361812488882\n",
      "Episodio 262 exitoso, recompensa: 248.56209556110335\n",
      "Episodio 266 exitoso, recompensa: 225.21243570024285\n",
      "Episodio 267 exitoso, recompensa: 256.45341792418117\n",
      "Episodio 277 exitoso, recompensa: 223.2220736510059\n",
      "Episodio 280 exitoso, recompensa: 272.28574973637114\n",
      "Episodio 281 exitoso, recompensa: 288.42578653887847\n",
      "Episodio 283 exitoso, recompensa: 270.76805450465673\n",
      "Episodio 286 exitoso, recompensa: 263.1179408940028\n",
      "Episodio 289 exitoso, recompensa: 248.54063279772816\n",
      "Episodio 292 exitoso, recompensa: 214.63648581537993\n",
      "Episodio 293 exitoso, recompensa: 258.64496117828685\n",
      "Episodio 294 exitoso, recompensa: 239.24183114820994\n",
      "Episodio 295 exitoso, recompensa: 252.88562663225773\n",
      "Episodio 296 exitoso, recompensa: 214.43450696724568\n",
      "Episodio 297 exitoso, recompensa: 229.21640832106658\n",
      "Episodio 298 exitoso, recompensa: 241.22421550149775\n",
      "Episodio 310 exitoso, recompensa: 288.5258744886729\n",
      "Episodio 312 exitoso, recompensa: 256.0867161372511\n",
      "Episodio 316 exitoso, recompensa: 237.6408457272015\n",
      "Episodio 323 exitoso, recompensa: 234.8867409603768\n",
      "Episodio 324 exitoso, recompensa: 222.90236831240867\n",
      "Episodio 330 exitoso, recompensa: 241.00815452952102\n",
      "Episodio 331 exitoso, recompensa: 213.92014542954172\n",
      "Episodio 332 exitoso, recompensa: 207.89902509436484\n",
      "Episodio 333 exitoso, recompensa: 251.7028825802716\n",
      "Episodio 335 exitoso, recompensa: 267.00034582701653\n",
      "Episodio 342 exitoso, recompensa: 273.24999984146564\n",
      "Episodio 347 exitoso, recompensa: 222.66027263850197\n",
      "Episodio 348 exitoso, recompensa: 220.3139356109015\n",
      "Episodio 350 exitoso, recompensa: 246.30379831112376\n",
      "Episodio 351 exitoso, recompensa: 201.21775509228746\n",
      "Episodio 352 exitoso, recompensa: 275.5319700028251\n",
      "Episodio 358 exitoso, recompensa: 202.24519271759848\n",
      "Episodio 359 exitoso, recompensa: 261.5609115184107\n",
      "Episodio 361 exitoso, recompensa: 257.2955433341309\n",
      "Episodio 364 exitoso, recompensa: 236.8150875296117\n",
      "Episodio 365 exitoso, recompensa: 268.9171495959987\n",
      "Episodio 369 exitoso, recompensa: 304.3717064146523\n",
      "Episodio 372 exitoso, recompensa: 248.19751406875133\n",
      "Episodio 373 exitoso, recompensa: 249.28731338690872\n",
      "Episodio 375 exitoso, recompensa: 255.4973182475745\n",
      "Episodio 378 exitoso, recompensa: 258.93025704419483\n",
      "Episodio 383 exitoso, recompensa: 223.13702582641173\n",
      "Episodio 386 exitoso, recompensa: 221.50845343571694\n",
      "Episodio 389 exitoso, recompensa: 259.22282396066885\n",
      "Episodio 390 exitoso, recompensa: 287.4070808340075\n",
      "Episodio 392 exitoso, recompensa: 277.2086283706082\n",
      "Episodio 393 exitoso, recompensa: 231.1534286474076\n",
      "Episodio 396 exitoso, recompensa: 267.29371760054664\n",
      "Episodio 397 exitoso, recompensa: 282.1600102893558\n",
      "Episodio 399 exitoso, recompensa: 255.09669915544148\n",
      "Episodio 403 exitoso, recompensa: 238.05574712028329\n",
      "Episodio 404 exitoso, recompensa: 256.00220515081475\n",
      "Episodio 405 exitoso, recompensa: 279.2381779442463\n",
      "Episodio 409 exitoso, recompensa: 210.0111903324314\n",
      "Episodio 410 exitoso, recompensa: 237.5071944284354\n",
      "Episodio 413 exitoso, recompensa: 287.0193504008606\n",
      "Episodio 414 exitoso, recompensa: 277.57551586211616\n",
      "Episodio 419 exitoso, recompensa: 280.20690553047723\n",
      "Episodio 422 exitoso, recompensa: 223.1409242189406\n",
      "Episodio 424 exitoso, recompensa: 227.88292500480804\n",
      "Episodio 427 exitoso, recompensa: 227.0809318350794\n",
      "Episodio 428 exitoso, recompensa: 254.29070177226433\n",
      "Episodio 429 exitoso, recompensa: 248.9043046791905\n",
      "Episodio 433 exitoso, recompensa: 247.59867300693674\n",
      "Episodio 435 exitoso, recompensa: 244.5539826853307\n",
      "Episodio 436 exitoso, recompensa: 254.00682250169965\n",
      "Episodio 440 exitoso, recompensa: 235.65092635882107\n",
      "Episodio 441 exitoso, recompensa: 210.49851553029623\n",
      "Episodio 442 exitoso, recompensa: 251.3932854072789\n",
      "Episodio 443 exitoso, recompensa: 261.4285355167284\n",
      "Episodio 447 exitoso, recompensa: 244.9241687211811\n",
      "Episodio 450 exitoso, recompensa: 251.7727546312345\n",
      "Episodio 458 exitoso, recompensa: 245.58889355372716\n",
      "Episodio 462 exitoso, recompensa: 268.27994557699446\n",
      "Episodio 463 exitoso, recompensa: 203.56589011569054\n",
      "Episodio 465 exitoso, recompensa: 202.51188510377685\n",
      "Episodio 466 exitoso, recompensa: 292.5738988167993\n",
      "Episodio 467 exitoso, recompensa: 225.72650189438588\n",
      "Episodio 468 exitoso, recompensa: 249.23148502236242\n",
      "Episodio 473 exitoso, recompensa: 210.15345115800574\n",
      "Episodio 478 exitoso, recompensa: 273.7580728036116\n",
      "Episodio 485 exitoso, recompensa: 221.1934599655666\n",
      "Episodio 486 exitoso, recompensa: 282.4076699136673\n",
      "Episodio 488 exitoso, recompensa: 267.5794562137751\n",
      "Episodio 489 exitoso, recompensa: 221.35853323323195\n",
      "Episodio 492 exitoso, recompensa: 262.842955940315\n",
      "Episodio 493 exitoso, recompensa: 244.91359580889477\n",
      "Episodio 518 exitoso, recompensa: 237.52429054516762\n",
      "Episodio 520 exitoso, recompensa: 233.56808523354175\n",
      "Episodio 521 exitoso, recompensa: 281.5560672988835\n",
      "Episodio 522 exitoso, recompensa: 229.6218307590493\n",
      "Episodio 523 exitoso, recompensa: 274.27673912114176\n",
      "Episodio 524 exitoso, recompensa: 230.355268375771\n",
      "Episodio 526 exitoso, recompensa: 224.20830548077845\n",
      "Episodio 527 exitoso, recompensa: 268.6541297365883\n",
      "Episodio 528 exitoso, recompensa: 228.22767242010048\n",
      "Episodio 533 exitoso, recompensa: 240.03332181715353\n",
      "Episodio 534 exitoso, recompensa: 280.5087549071355\n",
      "Episodio 537 exitoso, recompensa: 240.651012715522\n",
      "Episodio 542 exitoso, recompensa: 234.67503748713588\n",
      "Episodio 543 exitoso, recompensa: 244.50015010932353\n",
      "Episodio 555 exitoso, recompensa: 265.80436820569867\n",
      "Episodio 559 exitoso, recompensa: 247.64052479501464\n",
      "Episodio 563 exitoso, recompensa: 210.8807852300423\n",
      "Episodio 569 exitoso, recompensa: 234.40168098141464\n",
      "Episodio 570 exitoso, recompensa: 246.6467689898286\n",
      "Episodio 571 exitoso, recompensa: 220.33102831491135\n",
      "Episodio 575 exitoso, recompensa: 255.2581560560015\n",
      "Episodio 577 exitoso, recompensa: 254.33499699108012\n",
      "Episodio 579 exitoso, recompensa: 242.04852588653645\n",
      "Episodio 586 exitoso, recompensa: 207.13934970843601\n",
      "Episodio 600 exitoso, recompensa: 250.55669735003104\n",
      "Episodio 603 exitoso, recompensa: 284.52702428616874\n",
      "Episodio 606 exitoso, recompensa: 254.59311410141066\n",
      "Episodio 610 exitoso, recompensa: 257.4302628727189\n",
      "Episodio 612 exitoso, recompensa: 219.31210521639758\n",
      "Episodio 618 exitoso, recompensa: 273.0863220325156\n",
      "Episodio 622 exitoso, recompensa: 278.03975727937814\n",
      "Episodio 624 exitoso, recompensa: 249.25064716774548\n",
      "Episodio 627 exitoso, recompensa: 219.24108431320641\n",
      "Episodio 629 exitoso, recompensa: 222.53061576906387\n",
      "Episodio 632 exitoso, recompensa: 246.46536182232936\n",
      "Episodio 633 exitoso, recompensa: 268.1003837216456\n",
      "Episodio 635 exitoso, recompensa: 258.19381330693204\n",
      "Episodio 637 exitoso, recompensa: 263.01714267631894\n",
      "Episodio 639 exitoso, recompensa: 262.5534680476069\n",
      "Episodio 645 exitoso, recompensa: 256.32709583241797\n",
      "Episodio 646 exitoso, recompensa: 245.32447338617914\n",
      "Episodio 649 exitoso, recompensa: 219.55801552634722\n",
      "Episodio 654 exitoso, recompensa: 250.87349631772912\n",
      "Episodio 655 exitoso, recompensa: 234.7131418840783\n",
      "Episodio 661 exitoso, recompensa: 221.64048906834057\n",
      "Episodio 665 exitoso, recompensa: 229.59593426343628\n",
      "Episodio 680 exitoso, recompensa: 262.8583750125335\n",
      "Episodio 681 exitoso, recompensa: 224.4055688020327\n",
      "Episodio 682 exitoso, recompensa: 206.14465827319336\n",
      "Episodio 690 exitoso, recompensa: 206.85033526950872\n",
      "Episodio 691 exitoso, recompensa: 262.3577346150963\n",
      "Episodio 692 exitoso, recompensa: 278.4405157303749\n",
      "Episodio 694 exitoso, recompensa: 290.2574192321389\n",
      "Episodio 697 exitoso, recompensa: 269.9910722180098\n",
      "Episodio 702 exitoso, recompensa: 221.86235312112038\n",
      "Episodio 703 exitoso, recompensa: 270.8718765023142\n",
      "Episodio 705 exitoso, recompensa: 211.80418099900524\n",
      "Episodio 710 exitoso, recompensa: 288.012397233392\n",
      "Episodio 714 exitoso, recompensa: 249.70910394354974\n",
      "Episodio 716 exitoso, recompensa: 257.7740537825643\n",
      "Episodio 724 exitoso, recompensa: 222.09121710638567\n",
      "Episodio 729 exitoso, recompensa: 236.68145950420976\n",
      "Episodio 730 exitoso, recompensa: 241.6962397710276\n",
      "Episodio 731 exitoso, recompensa: 232.5774894578643\n",
      "Episodio 732 exitoso, recompensa: 224.06619297193936\n",
      "Episodio 733 exitoso, recompensa: 240.15388622252814\n",
      "Episodio 736 exitoso, recompensa: 288.06557465510866\n",
      "Episodio 738 exitoso, recompensa: 245.88930880796897\n",
      "Episodio 739 exitoso, recompensa: 231.52810029524102\n",
      "Episodio 747 exitoso, recompensa: 200.95242345298135\n",
      "Episodio 751 exitoso, recompensa: 289.4195302408267\n",
      "Episodio 752 exitoso, recompensa: 257.6134070022304\n",
      "Episodio 754 exitoso, recompensa: 260.5509957222432\n",
      "Episodio 758 exitoso, recompensa: 214.4747955535523\n",
      "Episodio 761 exitoso, recompensa: 235.74309521012026\n",
      "Episodio 762 exitoso, recompensa: 229.88071885820546\n",
      "Episodio 764 exitoso, recompensa: 261.7520141741973\n",
      "Episodio 768 exitoso, recompensa: 262.8766329591464\n",
      "Episodio 779 exitoso, recompensa: 252.16695946523222\n",
      "Episodio 780 exitoso, recompensa: 276.88412300192124\n",
      "Episodio 781 exitoso, recompensa: 264.6830589489754\n",
      "Episodio 786 exitoso, recompensa: 220.74251740075286\n",
      "Episodio 789 exitoso, recompensa: 208.9739810178756\n",
      "Episodio 790 exitoso, recompensa: 234.68493853650398\n",
      "Episodio 792 exitoso, recompensa: 256.510845879955\n",
      "Episodio 793 exitoso, recompensa: 256.5573434107028\n",
      "Episodio 794 exitoso, recompensa: 259.7398947585068\n",
      "Episodio 796 exitoso, recompensa: 270.1112665744696\n",
      "Episodio 800 exitoso, recompensa: 284.5728504358367\n",
      "Episodio 802 exitoso, recompensa: 252.72235114682962\n",
      "Episodio 804 exitoso, recompensa: 253.53987338776855\n",
      "Episodio 805 exitoso, recompensa: 254.88904885413066\n",
      "Episodio 806 exitoso, recompensa: 204.8801174952838\n",
      "Episodio 807 exitoso, recompensa: 215.74942557580243\n",
      "Episodio 809 exitoso, recompensa: 230.9144943618023\n",
      "Episodio 818 exitoso, recompensa: 290.6187209201124\n",
      "Episodio 821 exitoso, recompensa: 249.17386616315238\n",
      "Episodio 822 exitoso, recompensa: 227.96852548105758\n",
      "Episodio 823 exitoso, recompensa: 221.47439194191685\n",
      "Episodio 825 exitoso, recompensa: 227.55899373418208\n",
      "Episodio 830 exitoso, recompensa: 246.39463640772948\n",
      "Episodio 831 exitoso, recompensa: 204.2435010020999\n",
      "Episodio 834 exitoso, recompensa: 240.38211252268235\n",
      "Episodio 841 exitoso, recompensa: 225.3282536664303\n",
      "Episodio 846 exitoso, recompensa: 238.44439291328104\n",
      "Episodio 848 exitoso, recompensa: 225.74406046502804\n",
      "Episodio 853 exitoso, recompensa: 251.27237087252342\n",
      "Episodio 856 exitoso, recompensa: 255.1921250230419\n",
      "Episodio 857 exitoso, recompensa: 227.64873190764047\n",
      "Episodio 862 exitoso, recompensa: 225.3310351922575\n",
      "Episodio 864 exitoso, recompensa: 260.3109146572549\n",
      "Episodio 866 exitoso, recompensa: 210.53405329140975\n",
      "Episodio 867 exitoso, recompensa: 266.81019756776334\n",
      "Episodio 868 exitoso, recompensa: 268.01508870377376\n",
      "Episodio 870 exitoso, recompensa: 215.12666615649303\n",
      "Episodio 873 exitoso, recompensa: 278.7282056805831\n",
      "Episodio 875 exitoso, recompensa: 258.2399298053221\n",
      "Episodio 879 exitoso, recompensa: 280.32060502415095\n",
      "Episodio 883 exitoso, recompensa: 268.2915378510978\n",
      "Episodio 887 exitoso, recompensa: 257.67754349465883\n",
      "Episodio 888 exitoso, recompensa: 228.62325740640617\n",
      "Episodio 890 exitoso, recompensa: 217.2979441827274\n",
      "Episodio 892 exitoso, recompensa: 233.34697657298096\n",
      "Episodio 896 exitoso, recompensa: 262.1239547469927\n",
      "Episodio 897 exitoso, recompensa: 224.6501021648715\n",
      "Episodio 899 exitoso, recompensa: 287.8350175015588\n",
      "Episodio 909 exitoso, recompensa: 227.6354791305027\n",
      "Episodio 920 exitoso, recompensa: 285.5296811871001\n",
      "Episodio 924 exitoso, recompensa: 225.31891747627114\n",
      "Episodio 925 exitoso, recompensa: 206.2285906594858\n",
      "Episodio 930 exitoso, recompensa: 251.7234889011171\n",
      "Episodio 931 exitoso, recompensa: 208.34110621868425\n",
      "Episodio 934 exitoso, recompensa: 246.00854349839688\n",
      "Episodio 937 exitoso, recompensa: 276.69505227806565\n",
      "Episodio 938 exitoso, recompensa: 237.87081222969454\n",
      "Episodio 940 exitoso, recompensa: 213.48512811442373\n",
      "Episodio 943 exitoso, recompensa: 201.3990422875372\n",
      "Episodio 946 exitoso, recompensa: 227.16290292017783\n",
      "Episodio 947 exitoso, recompensa: 267.7154210672538\n",
      "Episodio 950 exitoso, recompensa: 229.38762667590805\n",
      "Episodio 953 exitoso, recompensa: 290.9343842642927\n",
      "Episodio 954 exitoso, recompensa: 200.11157056153775\n",
      "Episodio 956 exitoso, recompensa: 250.98123712221258\n",
      "Episodio 961 exitoso, recompensa: 239.6551539815326\n",
      "Episodio 968 exitoso, recompensa: 267.4112378035678\n",
      "Episodio 969 exitoso, recompensa: 287.86699564807543\n",
      "Episodio 970 exitoso, recompensa: 271.68833041545264\n",
      "Episodio 981 exitoso, recompensa: 222.202965226079\n",
      "Episodio 982 exitoso, recompensa: 247.07725770678738\n",
      "Episodio 987 exitoso, recompensa: 258.3405718955056\n",
      "Episodio 990 exitoso, recompensa: 277.24791857032164\n",
      "Episodio 991 exitoso, recompensa: 215.79074947633677\n",
      "Episodio 998 exitoso, recompensa: 233.65408164084255\n",
      "Tasa de éxito: 0.325. Se obtuvo 78.82602724331406 de recompensa, en promedio\n"
     ]
    }
   ],
   "source": [
    "# Nota: hay que transformar esta celda en código para ejecutar (Esc + y)\n",
    "# Advertencia: este bloque es un loop infinito si el agente se deja sin implementar\n",
    "\n",
    "exitos = 0\n",
    "recompensa_episodios = []\n",
    "num_episodios = 1000\n",
    "episodios_exitosos = []\n",
    "episodios_no_exitosos = []\n",
    "semillas_exitosas = []\n",
    "semillas_no_exitosas = []\n",
    "\n",
    "for i in range(num_episodios):\n",
    "    recompensa, episodio, semilla = ejecutar_episodio_2(agente, aprender=False)\n",
    "    # Los episodios se consideran exitosos si se obtuvo 200 o más de recompensa total\n",
    "    if recompensa >= 200:\n",
    "        print(f\"Episodio {i} exitoso, recompensa: {recompensa}\")\n",
    "        exitos += 1\n",
    "        episodios_exitosos.append(episodio)\n",
    "        semillas_exitosas.append(semilla)\n",
    "    else:\n",
    "        episodios_no_exitosos.append(episodio)\n",
    "        semillas_no_exitosas.append(semilla)\n",
    "    recompensa_episodios.append(recompensa)\n",
    "\n",
    "print(f\"Tasa de éxito: {exitos / num_episodios}. Se obtuvo {np.mean(recompensa_episodios)} de recompensa, en promedio\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a680260",
   "metadata": {},
   "source": [
    "# Análisis de resultados\n",
    "\n",
    "En la siguiente sección estaremos realizando un análisis de los resultados obtenidos tras la implementación y el entrenamiento de un agente de RL basado en Q Learning. En esta sección explicaremos cómo construimos el agente, en qué nos basamos, qué parámetros tuvimos en cuenta y por qué. También analizaremos algunas situaciones particulares del agente para ver su comportamiento en detalle, algunos casos de éxito y de falla, cómo evolucionó la recompensa promedio a medida que avanzaba el entrenamiento y qué limitantes vemos en el agente y cómo se podrían mitigar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7858dc3d",
   "metadata": {},
   "source": [
    "## Análisis de implementación y decisiones de diseño\n",
    "\n",
    "A la hora de construir nuestro agente RL basado en Q Learning, nos basamos fuertemente en el teórico del curso, utilizando la metodología vista en clase, y también en la documentación de gymnasium, en la cual explican, sobre otro ejemplo, como entrenarlo con Q Learning utilizando la librería. El link a la documentación utilizada es el siguiente: https://gymnasium.farama.org/tutorials/training_agents/blackjack_tutorial/.\n",
    "\n",
    "### Estrategia de acción a tomar\n",
    "\n",
    "Lo primero que decidimos sobre el modelo es cómo debíamos elegir una acción, dado que si siempre elegimos la mejor acción nos perdemos de encontrar acciones que no fueron descubiertas previamente, por lo que hay que encontrar un balance entre tomar decisiones azarosas y las mejores decisiones. Ante esto teníamos 3 opciones: \n",
    "- Soft Max: En esta estrategia, se asignan probabilidades a las diferentes acciones usando una función. Las acciones con valores de Q más altos tendrán mayores probabilidades de ser seleccionadas, pero incluso las acciones con valores menores todavía tienen una probabilidad no nula de ser escogidas, lo que genera que se suela elegir las acciones con valores de Q más altos, pero que con cierta probabilidad no nula se tomen otras acciones que pueden resultar beneficiosas.\n",
    "- Epsilon-Greedy: En esta estrategia, se toma un valor epsilon, que viene dado como parámetro, y se toma la mejor acción (mayor valor de la tabla Q para ese estado) con probabilidad 1-epsilon, y una decisión azarosa con probabilidad epsilon. De esta forma se asegura que tomemos acciones azarosas e informadas, lo que contribuye a un mejor entrenamiento.\n",
    "- Epsilon-Greedy con decaimiento: Esta estrategia es una variante de la estrategia Epsilon-Greedy que aparte del valor epsilon toma un valor de epsilon minimo y un valor de decaimiento de epsilon. Por cada episodio que pasa se reduce el epsilon restandole el valor de decaimiento, hasta que en algún episodio se llega al valor mínimo de epsilon, el cual se utiliza durante el resto de entrenamiento. Esto lo que genera es que al principio del entrenamiento tomemos más decisiones azarosas, ya que no tenemos tanto conocimiento del entorno aún, y a medida que pasa el tiempo y se asume que tenemos más información y podemos tomar más decisiones informadas, la probabilidad de tomar decisiones azarosas se reduce y en su lugar se toman decisiones informadas con respecto a los valores de la tabla Q.\n",
    "\n",
    "Ante estas tres opciones decidimos inclinarnos por la tercera opción: Epsilon-Greedy con decaimiento. Nos pareció una estrategia coherente, un poco mejor que el Epsilon-Greedy sin esta variante, y creímos que en términos de implementación no era muy compleja. Decidimos probar esta estrategia y en caso de notar un bajo desempeño, cambiar a Soft Max. Cómo se verá más adelante en este informe, tras ver que los resultados fueron óptimos, se mantuvo la decisión.\n",
    "\n",
    "### Hiperparámetros a utilizar y valores utilizados\n",
    "\n",
    "#### Epsilon, Epsilon Mínimo y Decaimiento de Epsilon\n",
    "\n",
    "Tras la decisión de la estrategia a utilizar, debíamos decidir qué valores utilizar para dichos parámetros (epsilon, decaimiento de epsilon y epsilon mínimo). Para el epsilon se tomó la decisión del valor de 1, ya que entendimos que cuando recién se comienza el entrenamiento queremos explorar lo más que se pueda, ya que no tenemos ningún tipo de información del entorno aún, por lo que la probabilidad conviene que sea 1, y que luego esta vaya disminuyendo linealmente.\n",
    "\n",
    "Cómo epsilon mínimo tomamos el valor de 0.05. Esta decisión se basó mayormente en experiencia. Sabíamos que queríamos un valor bajo de epsilon mínimo ya que una vez que tengamos la tabla Q bastante llena, queremos tomar decisiones informadas la mayor parte de las veces. Al principio probamos con un valor de 0.1, de 0.2, de 0.05, y así fuimos probando con distintos valores, llegando a que los mejores resultados se daban con el valor de 0.05, aunque la diferencia no era tanta, sí había una diferencia.\n",
    "\n",
    "Por último, debimos decidir sobre el valor del decaimiento de epsilon.\n",
    "Sobre este valor nos encontramos con un tema a la hora de la implementación, y fue que al principio utilizamos el decaimiento de epsilon como factor que se le multiplicaba al valor de epsilon hasta llegar al epsilon mínimo. Por lo que poniamos un valor fijo alto de decaimiento de epsilon (ej: 0.995). Vimos que los resultados no eran buenos y decidimos probar con valores paramétricos, utilizando la cantidad de episodios para decidir sobre el valor. Para esto se nos ocurrió la fórmula: decaimiento de epsilon = (epsilon - epsilon min) ^ (1/episodios), con la idea de que esto haría que el epsilon fuera decayendo y en el último episodio se llegara al valor mínimo de epsilon.\n",
    "\n",
    "Esta opción tampoco nos trajo buenos resultados, por lo que tras revisar la documentación brindada por gymnasium, vimos que estaba bien poner un valor paramétrico en función de la cantidad de episodios, pero que la disminución del epsilon con el factor de decaimiento era a través de una resta. Epsilon = Epsilon - decaimiento_epsilon. En este caso vimos que en la documentación utilizaban un valor de decaimiento descripto por la fórmula: decaimiento_epsilon = epsilon / (num_episodios / 2). Tras utilizar esta fórmula nuestros resultados mejoraron bastante. Probamos luego variar el valor de 2 en la fórmula por distintos múltiplos de 2, ya que entendimos que por cómo es su funcionamiento, cuanto más grande dicho valor, durante menos tiempo se mantiene el epsilon en un número alto, pero vimos que la fórmula detallada por la documentación fue la que nos dió mejores resultados en la práctica.\n",
    "\n",
    "#### Tasa de aprendizaje en ambiente no determinista alpha\n",
    "\n",
    "Dado que nos encontramos en el problema de Lunar Lander, este es un problema continuo, donde el estado está definido por los ejes horizontal, vertical y de rotación, sus velocidades y 2 booleanos que indican si las patas de la nave están en contacto con el suelo o no. Por más de que en un principio discretizamos el estado en una cantidad de bins, esto no quita que el problema siga siendo continuo, por lo que al trabajar con un problema continuo, nos va a suceder muchas veces que al tomar una misma acción desde un mismo estado caigamos en estados distintos. Esto tiene un problema si no lo tomamos en cuenta, y es que a la hora de llenar la tabla Q con los valores de recompensa, las recompensas no van a ser las mismas desde un estado y una acción ya que los resultados posteriores dependen del estado en el que se caiga posteriormente, y estos varían cada vez que los tomamos. \n",
    "Para esto se utiliza el hiperparámetro tasa de aprendizaje.\n",
    "El cual se agrega en la etapa de aprendizaje en la actualización de la tabla Q:\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0fc44224",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "self.q_table[estado_anterior][accion] = (\n",
    "            self.q_table[estado_anterior][accion] + tasa_aprendizaje * (recompensa + np.max(self.q_table[estado_siguiente]) - self.q_table[estado_anterior][accion])\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de17e49",
   "metadata": {},
   "source": [
    "Aquí vemos que se multiplica a la tasa de aprendizaje por la recompensa del movimiento que se realizó, más la recompensa máxima del estado al que se llegó, menos el valor de la tabla Q en el par estado-acción que se está llenando.\n",
    "\n",
    "Vemos aquí como la tasa de aprendizaje determina cuánto peso se le da a la nueva información en comparación con la información previamente aprendida. Cuanto más grande sea esta tasa, mayor será el peso que se le otorga.\n",
    "\n",
    "Para definir un valor para la tasa de aprendizaje, primero probamos con valores fijos chicos, como 0.1 y 0.01. Los resultados con valores fijos no fueron muy favorables, y nuestro agente no lograba desempeñarse bien, obteniendo muy bajos porcentajes de éxito. \n",
    "Luego se utilizó la fórmula vista en clase, de definir la tasa de aprendizaje en función del estado acción que se está llenando. La tasa de aprendizaje se define como 1/visitas[estado][acción], donde visitas[estado][acción] es la cantidad de veces que se pasó por el dicho estado y acción en el proceso. Esto tiene sentido ya que a medida que vamos teniendo más información de ese entorno, el impacto de un nuevo valor debería ser menos, mientras que al principio cuando no hay tanta información, debería tener más impacto.\n",
    "\n",
    "Al utilizar dicho valor variable, el desempeño mejoró notoriamente. Este cambio fue el que nos hizo pasar de un desempeño sin entrenamiento de aproximadamente 1 éxito en 1000, a tener más de un 10% de éxito en 1000 episodios. \n",
    "\n",
    "### Factor de descuento gamma\n",
    "\n",
    "El último hiperparámetro utilizado en este caso es el parámetro de descuento gamma. Este hiperparámetro es mayormente utilizado cuando se trata con tareas posiblemente infinitas, como el caso de un avión que debe mantenerse volando indefinidamente. Es un valor que en la fórmula de actualización de la tabla Q se multiplica por el valor máximo que se encuentra en el estado futuro del par estado-acción tomado, lo que genera que se de más valor a las recompensas inmediatas que a los valores futuros.\n",
    "\n",
    "La decisión de utilizar este hiperparámetro por nuestra parte fue completamente empírica. Sin utilizar dicho parámetro el desempeño del agente decae drásticamente, y al utilizarlo mejora. No tenemos claro la razón por la cual esto sucede, ya que la tarea que estamos intentando aprender es una tarea finita de aterrizaje. Creemos que puede llegar a tener que ver con el hecho de que para aterrizar la nave debe mantenerse en el aire de forma controlada primero, por lo cual puede que se desempeñe mejor si tiene más en cuenta la recompensa de los movimientos actuales que la mantengan en el aire, recta y con velocidades bajas, antes que las recompensas de los valores Q de estados futuros.\n",
    "\n",
    "El valor utilizado para gamma fue de 0.95, el cual también fue decidido empíricamente. Variamos los valores de gamma y este fue el que tuvo mayor desempeño.\n",
    "\n",
    "### Cantidad de bins\n",
    "\n",
    "Por último, si bien no es un parámetro de la función, cabe destacar nuestro cambio sobre la cantidad de bins utilizados. Por defecto en la tarea venía puesto como 20 bins, lo que significaba que los ejes continuos que representan el estado serían particionados en 20 valores cada uno. Notamos que el desempeño en este caso no era muy bueno por lo cual al ya tener implementado el resto de parámetros, variamos el valor de bins y vimos que mejoraba al cambiarlo. Tras probar con varios valores, mayores y menores a éste, vimos que en 15 bins nos daba resultados muy buenos, y esta es la razón por la cual decidimos cambiar ese valor y documentarlo aquí."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab48702f",
   "metadata": {},
   "source": [
    "## Análisis de \"cortes\" de la matriz Q y la política"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c54cce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[ 3.48045771  1.64230029 -1.00700493  1.69349329]\n",
      "[9.78310153 9.75045357 9.20702841 9.73376249]\n",
      "[10.31395321  9.99421894  9.7291172   9.9717998 ]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[1.24615113 1.24594584 0.33009363 1.24692192]\n"
     ]
    }
   ],
   "source": [
    "# Estado:\n",
    "# (x, y, x_vel, y_vel, theta, theta_vel, pie_izq_en_contacto, pie_derecho_en_contacto)\n",
    "# Nave cayendo rapidamente hacia abajo sin rotacion en la matriz Q\n",
    "estado = discretize_state([0, 1.5, 0, -5, 0, 0, 0, 0], bins)\n",
    "print(agente.q_table[estado])\n",
    "# Nave cayendo rapidamente hacia abajo sin rotacion en la matriz Q\n",
    "estado = discretize_state([0, 1.5, 0, -4, 0, 0, 0, 0], bins)\n",
    "print(agente.q_table[estado])\n",
    "# Nave cayendo rapidamente hacia abajo sin rotacion en la matriz Q\n",
    "estado = discretize_state([0, 0.5, 0, -2, 0, 0, 0, 0], bins)\n",
    "print(agente.q_table[estado])\n",
    "estado_final = discretize_state([0, 0, 0, 0, 0, 0, 1, 1], bins)\n",
    "print(agente.q_table[estado_final])\n",
    "\n",
    "estado_2 = discretize_state([0, 0, 0, 0, 0, 0, 0, 1], bins)\n",
    "print(agente.q_table[estado_2])\n",
    "\n",
    "estado_3 = discretize_state([0, 0, 0, 0, 0, 0, 1, 0], bins)\n",
    "print(agente.q_table[estado_3])\n",
    "\n",
    "# Aterriza pero no en el centro der\n",
    "estado_4 = discretize_state([1.5, 0, 0, 0, 0, 0, 0, 0], bins)\n",
    "print(agente.q_table[estado_4])\n",
    "\n",
    "# Aterriza en el centro izq\n",
    "estado_5 = discretize_state([-1.5, 0, 0, 0, 0, 0, 0, 0], bins)\n",
    "print(agente.q_table[estado_5])\n",
    "\n",
    "# Estado inicial\n",
    "estado_6 = discretize_state([0, 1.5, 0, 0, 0, 0, 0, 0], bins)\n",
    "print(agente.q_table[estado_6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ea8bde",
   "metadata": {},
   "source": [
    "####  Nave cayendo rapidamente hacia abajo sin rotacion en la matriz Q\n",
    "En este caso, el estado representa:\n",
    "Posición: El agente se encuentra en el eje Y a una altura de 1.5, pero con una velocidad negativa de -5, lo que implica una caída rápida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47f836ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "estado = discretize_state([0, 1.5, 0, -5, 0, 0, 0, 0], bins)\n",
    "print(agente.q_table[estado])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331792e7",
   "metadata": {},
   "source": [
    "Es posible que el agente nunca haya visitado ese estado durante el proceso de entrenamiento. Si un estado particular no se ha explorado, no tendrá valor asociado en la matriz Q."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1461a8",
   "metadata": {},
   "source": [
    "#### Nave cayendo cayendo hacia abajo en la matriz Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b24e6cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "estado = discretize_state([0, 1.5, 0, -1.9, 0, 0, 0, 0], bins)\n",
    "print(agente.q_table[estado])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836eba9e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "84543c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Nave cayendo rapidamente hacia abajo sin rotacion en la matriz Q\n",
    "estado = discretize_state([0, 0.5, 0, -2, 0, 0, 0, 0], bins)\n",
    "print(agente.q_table[estado])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d10205",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "458039a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.48045771  1.64230029 -1.00700493  1.69349329]\n"
     ]
    }
   ],
   "source": [
    "estado_final = discretize_state([0, 0, 0, 0, 0, 0, 1, 1], bins)\n",
    "print(agente.q_table[estado_final])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c28b857",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4a162d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.78310153 9.75045357 9.20702841 9.73376249]\n"
     ]
    }
   ],
   "source": [
    "estado_2 = discretize_state([0, 0, 0, 0, 0, 0, 0, 1], bins)\n",
    "print(agente.q_table[estado_2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30499a70",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a846a09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10.31395321  9.99421894  9.7291172   9.9717998 ]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "estado_3 = discretize_state([0, 0, 0, 0, 0, 0, 1, 0], bins)\n",
    "print(agente.q_table[estado_3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f867a028",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "766a71af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Aterriza pero no en el centro der\n",
    "estado_4 = discretize_state([1.5, 0, 0, 0, 0, 0, 0, 0], bins)\n",
    "print(agente.q_table[estado_4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9631aa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cc10958f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Aterriza en el centro izq\n",
    "estado_5 = discretize_state([-1.5, 0, 0, 0, 0, 0, 0, 0], bins)\n",
    "print(agente.q_table[estado_5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a49a74",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f80c726c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.24615113 1.24594584 0.33009363 1.24692192]\n"
     ]
    }
   ],
   "source": [
    "# Estado inicial\n",
    "estado_6 = discretize_state([0, 1.5, 0, 0, 0, 0, 0, 0], bins)\n",
    "print(agente.q_table[estado_6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb5d4b1",
   "metadata": {},
   "source": [
    "## Análisis de evolución de recompensa promedio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44482154",
   "metadata": {},
   "source": [
    "A continuación se grafica la recompensa promedio en intervalos de 1000 episodios. Se entrenó con 100.000 episodios y cada 1000 de ellos, se separaron y se calculó su recompensa promedio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "091b1c5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxYAAAGGCAYAAADmRxfNAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAACTrUlEQVR4nOzddVyT2x8H8M+o0WKCKCq2KCZ2J3a3XrG7sOvaih3X9noVr3Ftr9cW89rdYte1W0CQ2vn9cX6bTMJNBiM+79drL7ZnZ8/zfbazse9OKYQQAkRERERERPFgYuwAiIiIiIgo+WNiQURERERE8cbEgoiIiIiI4o2JBRERERERxRsTCyIiIiIiijcmFkREREREFG9MLIiIiIiIKN6YWBARERERUbwxsSACsHLlSixbtszYYRARERElW0wsyOgUCgXGjx+fYPuvUqUKqlSpEuv9mzdvxoABA1CyZMkEiyEqX19fKBQKPH782CD7O3r0KBQKBY4ePWqQ/RElFzG9l370fqekLaH/H+jK0J/TFJ0xXuuOHTsiR44cRo8jJWNiQQC+fYjGdjlz5oyxQ0wQ9+7dQ8+ePbFp0yYUL17c2OEkCepERX0xNTVFpkyZ0Lx5c/j7+xs7PPqB718/c3Nz5MyZEx06dMDDhw+NHR79BIVCgb59+/7UY9evX4958+YZNiD6KVOmTEHDhg3h6Oj4wy+zz58/R8uWLeHg4AB7e3s0atQo1vfvH3/8gQIFCsDS0hJ58uTBggUL4r3P7+XIkSPW7we1a9fWaR+UOpgZOwBKWiZOnAhXV9do23Pnzm2EaAzjwIEDsd539epVrFq1CnXq1EnEiJKH/v37o2TJkggPD8e1a9ewdOlSHD16FDdu3ICTk5Oxw6MfiPr6Xbp0CcuXL8fu3btx/fp1ODs7Gzu8BBPX+z01Wr9+PW7cuIGBAwcaO5RUb8yYMXByckKxYsWwf//+WMsFBQWhatWq+Pz5M0aNGgVzc3PMnTsXlStXxpUrV5A+fXpN2WXLlqFnz55o1qwZBg0ahOPHj6N///4IDg7G8OHDf2qfsSlatCgGDx4cbfvPfp6EhITAzMz4X0OTShwpBZ9J0lKnTh14eHgYOwyDsrCwiPW+5s2bJ2IkyUvFihW1np98+fKhV69e+PPPPzFs2DAjRka6iPr6derUCXnz5kX//v2xevVqjBw5MsbHfPnyBTY2NokZpsHF9X5Pqr5+/QoLCwuYmCSPTgQqlQphYWGwtLQ0dijJyqNHj5AjRw68e/cOGTNmjLXc4sWLce/ePZw7d07TRbdOnTooVKgQZs+ejalTpwKQX4hHjx6NevXqYcuWLQCAbt26QaVSYdKkSejevTvSpk2r1z7jkiVLFrRv3z5ez0FUSaX+JJU4Uork8SlGSUJ4eDjSpUuHTp06RbsvICAAlpaWGDJkiGbbmzdv0KVLFzg6OsLS0hJFihTB6tWrf3icmPpAAsD48eOhUCiibV+7di1KlSoFa2trpE2bFpUqVdL61TKmPte6xPb48WMoFArMmjULy5cvR65cuaBUKlGyZEmcP3/+h+cBADdv3kS1atVgZWWFrFmzYvLkyVCpVDGW3bt3LypWrAgbGxvY2dmhXr16uHnzpk7H+d7x48fRokULZMuWDUqlEi4uLvD29kZISMhP7Q+QX1QB4MGDB1rbnz9/js6dO8PR0RFKpRIFCxbEypUroz3+69evGD9+PPLmzQtLS0tkzpwZTZs21drfly9fMHjwYLi4uECpVCJfvnyYNWsWhBBa+1J3Ddm8eTPc3NxgZWWFsmXL4vr16wDkr3i5c+eGpaUlqlSpEq2fdJUqVVCoUCFcvHgR5cqVg5WVFVxdXbF06dJocYeGhmLcuHHInTu35rkcNmwYQkNDY4zp77//RqFChTTPxb59+7TKBQYGYuDAgciRIweUSiUyZcqEmjVr4tKlS5oyCfH6VatWDYD8cgN8ez/dunULbdu2Rdq0aVGhQgUAQEREBCZNmqSp8zly5MCoUaOinXOOHDlQv359HD16FB4eHrCysoK7u7tmvM+2bdvg7u4OS0tLlChRApcvX44W1+3bt9G8eXOkS5cOlpaW8PDwwD///BOtnK7vpZ99v8dGfY4HDhxA0aJFYWlpCTc3N2zbti1a2YcPH6JFixZIly4drK2tUaZMGezevVurjLqr2oYNGzBmzBhkyZIF1tbWCAgI0CmeqPvYtGkTpkyZgqxZs8LS0hLVq1fH/fv3tZ6L3bt348mTJ5puK1E/W/Wt2+vWrUPBggWhVCqxc+dOnf8fhIWFYezYsShRogTSpEkDGxsbVKxYEUeOHNHpfC9fvow6derA3t4etra2qF69erTuueHh4ZgwYQLy5MkDS0tLpE+fHhUqVICfn98P959Yn9Mx/V+LyZYtW1CyZEmtcX/58+dH9erVsWnTJs22I0eO4P379+jdu7fW4/v06YMvX75o1T1d9xlfHTt2hK2tLR4+fAhPT0/Y2NjA2dkZEydOjPFzPGp3MF0+GwE5LrJEiRKwsrJChgwZ0L59ezx//jxaLOrPYktLSxQqVAjbt2+PMeaYuqUldJ1LydhiQVo+f/6Md+/eaW1TKBRInz49zM3N0aRJE2zbtg3Lli3T+mXw77//RmhoKFq3bg1A/pJSpUoV3L9/H3379oWrqys2b96Mjh074tOnTxgwYIBB4p0wYQLGjx+PcuXKYeLEibCwsMDZs2dx+PBh1KpVK8bH6Bvb+vXrERgYiB49ekChUGDGjBlo2rQpHj58CHNz81hje/XqFapWrYqIiAiMGDECNjY2WL58OaysrKKVXbNmDby8vODp6Ynp06cjODgYS5YsQYUKFXD58mWd/yGpbd68GcHBwejVqxfSp0+Pc+fOYcGCBXj27Bk2b96s177U1F/O1b+AAcDr169RpkwZzRePjBkzYu/evejSpQsCAgI03S8iIyNRv359HDp0CK1bt8aAAQMQGBgIPz8/3LhxA7ly5YIQAg0bNsSRI0fQpUsXFC1aFPv378fQoUPx/PlzzJ07Vyue48eP459//kGfPn0AAD4+Pqhfvz6GDRuGxYsXo3fv3vj48SNmzJiBzp074/Dhw1qP//jxI+rWrYuWLVuiTZs22LRpE3r16gULCwt07twZgPxltmHDhjhx4gS6d++OAgUK4Pr165g7dy7u3r2Lv//+W2ufJ06cwLZt29C7d2/Y2dnht99+Q7NmzfD06VNNV4OePXtiy5Yt6Nu3L9zc3PD+/XucOHEC/v7+mnE+CfH6qRO477s8tGjRAnny5MHUqVM1//i7du2K1atXo3nz5hg8eDDOnj0LHx8f+Pv7R/vnfP/+fbRt2xY9evRA+/btMWvWLDRo0ABLly7FqFGjNF96fHx80LJlS9y5c0fzy/zNmzdRvnx5ZMmSRfMe2bRpExo3boytW7eiSZMmAPR7L33PEJ9F9+7dQ6tWrdCzZ094eXlh1apVaNGiBfbt24eaNWsCkO+FcuXKITg4GP3790f69OmxevVqNGzYEFu2bNGci9qkSZNgYWGBIUOGIDQ09KdaWqZNmwYTExMMGTIEnz9/xowZM9CuXTucPXsWADB69Gh8/vwZz54907x/bG1tAehftw8fPoxNmzahb9++yJAhA/LkyaPz/4OAgACsWLECbdq0Qbdu3RAYGIg//vgDnp6eOHfuHIoWLRrrOd68eRMVK1aEvb09hg0bBnNzcyxbtgxVqlTBsWPHULp0aQAyUfbx8UHXrl1RqlQpBAQE4MKFC7h06ZLmNYqJMT+nY6JSqXDt2jXNZ1BUpUqVwoEDBxAYGAg7OztNov59L4MSJUrAxMQEly9fRvv27fXaZ1zCw8OjfT8AABsbG63nKzIyErVr10aZMmUwY8YM7Nu3D+PGjUNERAQmTpwY6/51+Wz09fVFp06dULJkSfj4+OD169eYP38+Tp48icuXL8PBwQGA7BLZrFkzuLm5wcfHB+/fv0enTp2QNWvWOM8RSPg6l+IJIiHEqlWrBIAYL0qlUlNu//79AoDYuXOn1uPr1q0rcubMqbk9b948AUCsXbtWsy0sLEyULVtW2NraioCAAM12AGLcuHGa215eXiJ79uzRYhw3bpyIWmXv3bsnTExMRJMmTURkZKRWWZVKpbleuXJlUblyZb1je/TokQAg0qdPLz58+KApu2PHjhifg+8NHDhQABBnz57VbHvz5o1IkyaNACAePXokhBAiMDBQODg4iG7dumk9/tWrVyJNmjTRtn/vyJEjAoA4cuSIZltwcHC0cj4+PkKhUIgnT57otL+VK1eKt2/fihcvXoh9+/aJ3LlzC4VCIc6dO6cp26VLF5E5c2bx7t07rX20bt1apEmTRhPHypUrBQAxZ86caMdTv1Z///23ACAmT56sdX/z5s2FQqEQ9+/f12xT10v1cyiEEMuWLRMAhJOTk1b9GjlypNbzLYSsEwDE7NmzNdtCQ0NF0aJFRaZMmURYWJgQQog1a9YIExMTcfz4ca2Yli5dKgCIkydPasVkYWGhFefVq1cFALFgwQLNtjRp0og+ffpEex6iMvTrt3v3bpEjRw6hUCjE+fPnhRDf3k9t2rTRevyVK1cEANG1a1et7UOGDBEAxOHDhzXbsmfPLgCIU6dOabapPyOsrKy0YlW/PlHrafXq1YW7u7v4+vWrZptKpRLlypUTefLk0WzT9b0kxM+/32OjPsetW7dqtn3+/FlkzpxZFCtWLFqMUetKYGCgcHV1FTly5NB8Rqlfn5w5c8b4OscEgFadUe+jQIECIjQ0VLN9/vz5AoC4fv26Zlu9evVi/DzVt26bmJiImzdvapXV9f9BRESEVpxCCPHx40fh6OgoOnfuHO1co/4/aNy4sbCwsBAPHjzQbHvx4oWws7MTlSpV0mwrUqSIqFevXrTz/JHE+pyO6u3bt9HO8/v7Jk6cGO2+RYsWCQDi9u3bQggh+vTpI0xNTWM8RsaMGUXr1q313mds1O+DmC4+Pj6acl5eXgKA6Nevn2abSqUS9erVExYWFuLt27ea7d8/Bz/6bAwLCxOZMmUShQoVEiEhIZrtu3btEgDE2LFjNduKFi0qMmfOLD59+qTZduDAAQEg2vshsetcSseuUKRl0aJF8PPz07rs3btXc3+1atWQIUMGbNy4UbPt48eP8PPzQ6tWrTTb9uzZAycnJ7Rp00azzdzcHP3790dQUBCOHTsW71j//vtvqFQqjB07Nlrf5Ji6TP1sbK1atdL6lV7dJehHs2ns2bMHZcqUQalSpTTbMmbMiHbt2mmV8/Pzw6dPn9CmTRu8e/dOczE1NUXp0qV17i4QVdRfj758+YJ3796hXLlyEELE2B0lJp07d0bGjBnh7OyM2rVr4/Pnz1izZo2mKV0Iga1bt6JBgwYQQmjF7unpic+fP2uasLdu3YoMGTKgX79+0Y6jfq327NkDU1NT9O/fX+v+wYMHQwihVQ8BoHr16lq/EKp/RWrWrJnWL2/q7d+/XmZmZujRo4fmtoWFBXr06IE3b97g4sWLAGTLQYECBZA/f36t81N3K/r+talRowZy5cqluV24cGHY29trHdvBwQFnz57Fixcvoj0XaoZ+/erVq4cvX75g9erV0X7d7Nmzp9btPXv2AAAGDRqktV09aPP7rj1ubm4oW7as5rb6+a5WrRqyZcsWbbv6ufjw4QMOHz6Mli1bIjAwUPPcvn//Hp6enrh3756me4Ou76WYGOKzyNnZWavFwd7eHh06dMDly5fx6tUrzXFKlSql6U4GyNaB7t274/Hjx7h165bWPr28vHRqcYlLp06dtFoKdP1sAvSv25UrV4abm5vWNl3/H5iammriVKlU+PDhAyIiIuDh4RGtm0tUkZGROHDgABo3boycOXNqtmfOnBlt27bFiRMnNF3IHBwccPPmTdy7d++H5x6VMT+nY6Lu7qhUKqPdpx4LoC4TEhISa0uXpaWlVjld9xmX0qVLR/t+4Ofnp/XeUos6i5m6RTssLAwHDx6Mdf8/+my8cOEC3rx5g969e2uNi6hXrx7y58+v+Wx6+fIlrly5Ai8vL6RJk0ZTrmbNmtHq8PcSo86ldOwKRVpKlSoV5+BtMzMzNGvWDOvXr0doaCiUSiW2bduG8PBwrX8kT548QZ48eaJ94S9QoIDm/vh68OABTExMfvhB8T19Y4v65Qj41hXo48ePPzyO+stUVPny5dO6rf5QUv9D/569vX2cx4nJ06dPMXbsWPzzzz/R4vz8+bNO+xg7diwqVqyIoKAgbN++HRs2bNB6zt6+fYtPnz5h+fLlWL58eYz7ePPmDQD5WuXLly/OmTeePHkCZ2fnaM3xur4u6n8gLi4uMW7//nlwdnaONlA5b968AGS3rzJlyuDevXvw9/ePdaCl+vxiiwmQ9SXqsWfMmAEvLy+4uLigRIkSqFu3Ljp06KD1T8yQr5+pqSkyZMiAAgUKxPj8fz8L3JMnT2BiYhJtJjgnJyc4ODgY7HW4f/8+hBD49ddf8euvv8Z4Dm/evEGWLFl0fi/FxBCfRblz5472Y0XUuuLk5BRrjFGPU6hQIc32mGbf09fPfjYB0LtuxxSvrv8PAGD16tWYPXs2bt++jfDw8Dj3q/b27VsEBwfH+DoXKFAAKpUK//33HwoWLIiJEyeiUaNGyJs3LwoVKoTatWvjl19+QeHCheN8Hoz5OR0TdbL5/TgXQI5Ti1rGysoKYWFhMe7n69evWuV03WdcMmTIgBo1avywnImJidbnGaD9fonNjz4b1e/VmOpD/vz5ceLECa1yefLkiVYuX758cSaziVHnUjomFqS31q1bY9myZdi7dy8aN26MTZs2IX/+/ChSpIhB9h9ba0NkZKRB9q8vU1PTGLeL7wai/Sz1IME1a9bEOI2rvtPgRUZGombNmvjw4QOGDx+O/Pnzw8bGBs+fP0fHjh1jHZT4PXd3d80/kcaNGyM4OBjdunVDhQoV4OLiotlP+/bt4eXlFeM+EvIDNrbXxZCvl0qlgru7O+bMmRPj/d9/edbl2C1btkTFihWxfft2HDhwADNnzsT06dOxbds21KlTJ0Fev7jE9oUirla/qH72dVCfx5AhQ+Dp6Rlj2eQ8zfWPxLe1AohfXde3bscWry7/D9auXYuOHTuicePGGDp0KDJlygRTU1P4+PhEmwziZ1WqVAkPHjzAjh07cODAAaxYsQJz587F0qVL0bVr13jv39Cf07FJly4dlEolXr58Ge0+9Tb19K6ZM2dGZGQk3rx5g0yZMmnKhYWF4f3795py+uzTmH702ZjUJHSdS66YWJDeKlWqhMyZM2Pjxo2oUKECDh8+jNGjR2uVyZ49O65duwaVSqX1S+Ht27c198cmbdq0+PTpU7Tt3/+ymCtXLqhUKty6dSvOwX/fi09s+siePXuMTaR37tzRuq3uOpMpUyadvgj+yPXr13H37l2sXr0aHTp00GyP70wV06ZNw/bt2zFlyhQsXboUGTNmhJ2dHSIjI38Yd65cuXD27FmEh4fHOuA9e/bsOHjwYLRBhIZ+XdRevHgRbXrVu3fvAvg2e0uuXLlw9epVVK9eXecv2rrInDkzevfujd69e+PNmzcoXrw4pkyZgjp16iTY66er7NmzQ6VS4d69e5pf2wE5OPnTp08Gex3Uv0Kam5v/sP7o+l6K7bHxfb+rW1ei1oHv60r27NljjCeh6q+uYqu3hqrbuvw/2LJlC3LmzIlt27ZpHWvcuHFx7jtjxoywtraO9Xk1MTHRSoDUs1R16tQJQUFBqFSpEsaPHx/nlzxjfU7HxsTEBO7u7rhw4UK0+86ePYucOXNqPh/V//cuXLiAunXraspduHABKpVKc78++zQElUqFhw8falopgOjvl9jE9dmofg/duXMnWsvRnTt3NPer//7MZ0Zi1LmUjmMsSG8mJiZo3rw5du7ciTVr1iAiIiJas3fdunXx6tUrrb63ERERWLBgAWxtbVG5cuVY958rVy58/vwZ165d02x7+fJltNloGjduDBMTE0ycODHar7hx/WIXn9j0UbduXZw5cwbnzp3TbHv79i3WrVunVc7T0xP29vaYOnWqVheBqI/Rh/pXzKjPgRAC8+fP12s/38uVKxeaNWsGX19fvHr1CqampmjWrBm2bt2KGzduxBl3s2bN8O7dOyxcuDBaOXWcdevWRWRkZLQyc+fOhUKhMPgvVhEREVi2bJnmdlhYGJYtW4aMGTOiRIkSAOQvaM+fP8fvv/8e7fEhISH48uWLXseMjIyM1pUpU6ZMcHZ21nRTSKjXT1fqLyjfr9as/mW7Xr16BjlOpkyZUKVKFSxbtizGX1Kj1h9d30sxMcT7/cWLF1qfPwEBAfjzzz9RtGhRza/XdevWxblz53D69GlNuS9fvmD58uXIkSOH3l02DcXGxibG7nOGqtu6/D+IqU6fPXtW67mKiampKWrVqoUdO3ZodaF5/fo11q9fjwoVKmi6IL1//17rsba2tsidO3eM3X+iMtbndFyaN2+O8+fPayUCd+7cweHDh9GiRQvNtmrVqiFdunRYsmSJ1uOXLFkCa2trrfeqrvs0lKif40IILFy4EObm5qhevXqM5XX5bPTw8ECmTJmwdOlSrdd179698Pf315xv5syZUbRoUaxevVprn35+ftHGOn0vMepcSscWC9Kyd+9ezS9sUZUrV06rz2SrVq2wYMECjBs3Du7u7lq/bAJA9+7dsWzZMnTs2BEXL15Ejhw5sGXLFpw8eRLz5s2L89eR1q1bY/jw4WjSpIlmBdElS5Ygb968Wn0jc+fOjdGjR2PSpEmoWLEimjZtCqVSifPnz8PZ2Rk+Pj4x7j8+selj2LBhWLNmDWrXro0BAwZopjFU/4KqZm9vjyVLluCXX35B8eLF0bp1a2TMmBFPnz7F7t27Ub58+Ri/kMcmf/78yJUrF4YMGYLnz5/D3t4eW7du1anf9Y8MHToUmzZtwrx58zBt2jRMmzYNR44cQenSpdGtWze4ubnhw4cPuHTpEg4ePIgPHz4AADp06IA///wTgwYNwrlz51CxYkV8+fIFBw8eRO/evdGoUSM0aNAAVatWxejRo/H48WMUKVIEBw4cwI4dOzBw4ECtQdGG4OzsjOnTp+Px48fImzcvNm7ciCtXrmD58uWaVpVffvkFmzZtQs+ePXHkyBGUL18ekZGRuH37NjZt2oT9+/frtaBkYGAgsmbNiubNm6NIkSKwtbXFwYMHcf78ecyePRtAwr5+uihSpAi8vLywfPlyfPr0CZUrV8a5c+ewevVqNG7cGFWrVjXYsRYtWoQKFSrA3d0d3bp1Q86cOfH69WucPn0az549w9WrVwHo/l6KiSHe73nz5kWXLl1w/vx5ODo6YuXKlXj9+jVWrVqlKTNixAj89ddfqFOnDvr374906dJh9erVePToEbZu3Wq0xe9KlCiBjRs3YtCgQShZsiRsbW3RoEEDg9btH/0/qF+/PrZt24YmTZqgXr16ePToEZYuXQo3NzcEBQXFue/JkyfDz88PFSpUQO/evWFmZoZly5YhNDQUM2bM0JRzc3NDlSpVUKJECaRLlw4XLlzQTF0al8T8nF6zZg2ePHmC4OBgAMC///6LyZMnA5CfNepf2nv37o3ff/8d9erVw5AhQ2Bubo45c+bA0dFRa+VrKysrTJo0CX369EGLFi3g6emJ48ePY+3atZgyZQrSpUunKavrPuPy/PlzrF27Ntp2W1tbNG7cWHPb0tIS+/btg5eXF0qXLo29e/di9+7dGDVqVKxjenT5bDQ3N8f06dPRqVMnVK5cGW3atNFMN5sjRw54e3tr9ufj44N69eqhQoUK6Ny5Mz58+IAFCxagYMGCRq9zKV7iTkJFSVVc080CEKtWrdIqr1KphIuLS4zTg6q9fv1adOrUSWTIkEFYWFgId3f3aPsRIvpUb0LIaeEKFSokLCwsRL58+cTatWujTTertnLlSlGsWDGhVCpF2rRpReXKlYWfn5/m/u+nn9Q1NvV0szNnztQp5phcu3ZNVK5cWVhaWoosWbKISZMmiT/++CPaFJlCyCkkPT09RZo0aYSlpaXIlSuX6Nixo7hw4UKcx4hputlbt26JGjVqCFtbW5EhQwbRrVs3zdSnMb0GMe1v8+bNMd5fpUoVYW9vr5nG7/Xr16JPnz7CxcVFmJubCycnJ1G9enWxfPlyrccFBweL0aNHC1dXV0255s2ba03pFxgYKLy9vYWzs7MwNzcXefLkETNnztSaPliI6NNvChH76xXT+VSuXFkULFhQXLhwQZQtW1ZYWlqK7Nmzi4ULF0Y737CwMDF9+nRRsGBBTR0rUaKEmDBhgvj8+XOcMQkhp2n08vISQsgpbYcOHSqKFCki7OzshI2NjShSpIhYvHix1mMS8vVTU7+fok7/qBYeHi4mTJigea1cXFzEyJEjtaaFVZ9bTNMt6vP6PHjwQHTo0EE4OTkJc3NzkSVLFlG/fn2xZcsWrXK6vpd+9v0eG/U57t+/XxQuXFgolUqRP3/+GJ/fBw8eiObNmwsHBwdhaWkpSpUqJXbt2qVVRtfXJ6rvn8/Y9qF+jqOeW1BQkGjbtq1wcHCINtVmfOu22o/+H6hUKjF16lSRPXt2oVQqRbFixcSuXbtinFo8ps/WS5cuCU9PT2Frayusra1F1apVtaY4FkKIyZMni1KlSgkHBwdhZWUl8ufPL6ZMmaKZOjouifE5LcS3aa5jukT9/BZCiP/++080b95c2NvbC1tbW1G/fn1x7969GPe7fPlykS9fPmFhYSFy5col5s6dG+0zU999fi+u6WajvoZeXl7CxsZGPHjwQNSqVUtYW1sLR0dHMW7cuGjTwkd9rXX9bBRCiI0bN2r+56dLl060a9dOPHv2LFq5rVu3igIFCgilUinc3NzEtm3bkkydS8kUQhhoBCoRUTJRpUoVvHv3LsYuXERR5ciRA4UKFcKuXbuMHQpRktexY0ds2bLlh60ClHJxjAUREREREcUbEwsiIiIiIoo3JhZERERERBRvHGNBRERERETxxhYLIiIiIiKKNyYWREREREQUb0wsiIiIiIgo3rjytp5UKhVevHgBOzs7KBQKY4dDRERERJRghBAIDAyEs7MzTEzibpNgYqGnFy9ewMXFxdhhEBERERElmv/++w9Zs2aNswwTCz3Z2dkBkE+uvb19vPYVHh6OAwcOoFatWjA3NzdEeJRMsS4QwHpAEusBqbEuEGD8ehAQEAAXFxfNd+C4JKvE4t9//8XMmTNx8eJFvHz5Etu3b0fjxo019wshMG7cOPz+++/49OkTypcvjyVLliBPnjyaMh8+fEC/fv2wc+dOmJiYoFmzZpg/fz5sbW11ikHd/cne3t4giYW1tTXs7e35gZHKsS4QwHpAEusBqbEuEJB06oEuQwCS1eDtL1++oEiRIli0aFGM98+YMQO//fYbli5dirNnz8LGxgaenp74+vWrpky7du1w8+ZN+Pn5YdeuXfj333/RvXv3xDoFIiIiIqIUKVm1WNSpUwd16tSJ8T4hBObNm4cxY8agUaNGAIA///wTjo6O+Pvvv9G6dWv4+/tj3759OH/+PDw8PAAACxYsQN26dTFr1iw4Ozsn2rkQEREREaUkyarFIi6PHj3Cq1evUKNGDc22NGnSoHTp0jh9+jQA4PTp03BwcNAkFQBQo0YNmJiY4OzZs4keMxERERFRSpGsWizi8urVKwCAo6Oj1nZHR0fNfa9evUKmTJm07jczM0O6dOk0Zb4XGhqK0NBQze2AgAAAsr9beHh4rPFERkYiIiICQohYy0RERMDMzAxBQUEwM0sxLwX9hNReFxQKBczMzGBqamrsUIxK/ZkS12cLpXysB6TGukCA8euBPsdNfd9g9OTj44MJEyZE237gwAFYW1vH+Bg7OzvY2dn9cK5fAHBycsLDhw/jHSclf6m9LqhUKgQGBiIwMNDYoRidn5+fsUOgJID1gNRYFwgwXj0IDg7WuWyKSSycnJwAAK9fv0bmzJk121+/fo2iRYtqyrx580brcREREfjw4YPm8d8bOXIkBg0apLmtnnKrVq1aMc4K9fr1awQEBCBjxoywtraOcwS9EAJfvnyBjY0NF9tL5VJ7XRBCIDg4GG/fvkXevHmjtTymFuHh4fDz80PNmjU5A0wqxnpAaqwLBBi/Hqh76+gixSQWrq6ucHJywqFDhzSJREBAAM6ePYtevXoBAMqWLYtPnz7h4sWLKFGiBADg8OHDUKlUKF26dIz7VSqVUCqV0babm5tHe3EjIyMRGBgIR0dHpE+f/ocxq1QqhIeHw8rKSqfWDUq5WBcAGxsbmJiY4M2bN8icOXOq7hYV0+cLpT6sB6TGukCA8eqBPsdMVolFUFAQ7t+/r7n96NEjXLlyBenSpUO2bNkwcOBATJ48GXny5IGrqyt+/fVXODs7a9a6KFCgAGrXro1u3bph6dKlCA8PR9++fdG6dWuDzAil7oMWWxcpIoqb+r0THh6eqhMLIiKi5ChZJRYXLlxA1apVNbfVXZS8vLzg6+uLYcOG4cuXL+jevTs+ffqEChUqYN++fbC0tNQ8Zt26dejbty+qV6+uWSDvt99+M2icqbErC5Eh8L1DRESUfCWrxKJKlSpxzrKkUCgwceJETJw4MdYy6dKlw/r16xMiPKJU5+jRo6hatSo+fvwIBwcH+Pr6YuDAgfj06ZOxQyMiomTi40fg5EmgXj2Avy8lb6mzMzdF07FjRygUCigUCpibm8PV1RXDhg3TWrWcDOvo0aOa59zU1BR58+ZF8+bNk/XMUK1atcLdu3eNHQYRESUTkZFA3bpAgwbAkiXGjobii4kFadSuXRsvX77Ew4cPMXfuXCxbtgzjxo0zdlgp3p07d/Ds2TOsWrUKt27dQoMGDRAZGRmtnBACERERRohQd1ZWVtHWiiEiIorNsmXAmTPy+pw5gEpl3HgofphYkIZSqYSTkxNcXFzQuHFj1KhRQ2vOZJVKBR8fH7i6usLKygpFihTBli1btPZx8+ZN1K9fH/b29rCzs0PFihXx4MEDzeMnTpyIrFmzQqlUomjRoti3b5/msY8fP4ZCocCmTZtQsWJFWFlZoWTJkrh79y7Onz8PDw8P2Nraok6dOnj79q3mcR07dkTjxo0xYcIEZMyYEfb29ujZsyfCwsJ0jl3denDo0CF4eHjA2toa5cqVw507dzRlrl69iqpVq8LOzg729vYoUaIELly4AAB4//492rRpgyxZssDa2hru7u7466+/dHreM2XKhMyZM6N8+fIYM2YMbt26hfv372ti2rt3L0qUKAGlUokTJ04gNDQU/fv3R6ZMmWBpaYkKFSrg/Pnz0c5l//79KFasGKysrFCtWjW8efMGe/fuRYECBWBvb4+2bdtqzU2ty+u7Z88e5M2bF1ZWVqhatSoeP36sdb+vry8cHBy0ti1ZsgS5cuWChYUF8uXLhzVr1uj0vBARpRQqFfD338DWrcCJE8Ddu8CnT0AcvbtThRcvgJEj5XWFAnjwANi1y7gxUfwkqzEWyZEQQGzriqhUwJcvgKkpkBAzjFpb/3xfxRs3buDUqVPInj27ZpuPjw/Wrl2LpUuXIk+ePPj333/Rvn17ZMyYEZUrV8bz589RqVIlVKlSBYcPH4a9vT1Onjyp+ZV9/vz5mD17NpYtW4ZixYph5cqVaNiwIW7evIk8efJojjNu3DjMmzcP2bJlQ+fOndG2bVvY2dlh/vz5sLa2RsuWLTF27FgsidJmeujQIVhaWuLo0aN4/PgxOnXqhPTp02PKlCk6xa42evRozJ49GxkzZkTPnj3RuXNnnDx5EgDQrl07FCtWDEuWLIGpqSmuXLmimYLt69evKFGiBIYPHw57e3vs3r0bv/zyC3LlyoVSpUrp/LxbWVkBgFZSNGLECMyaNQs5c+ZE2rRpMWzYMGzduhWrV69G9uzZMWPGDHh6euL+/ftIly6d5nHjx4/HwoULNc9Zy5YtoVQqsX79egQFBaFJkyZYsGABhg8frtNz9N9//6Fp06bo06cPunfvjgsXLmDw4MFxns/27dsxYMAAzJs3DzVq1MCuXbvQqVMnZM2aVWsiBiKilGzdOqBDh+jbLSyATJmA0qWBFSuA736XMajPn4H794HixfX7bvD1KxBlDhyDGjAACAgASpUCKlUCZs0C5s4FGjZMmONRIhCkl8+fPwsA4vPnz9HuCwkJEbdu3RIhISGabUFBQsj0IvEvQUG6n5eXl5cwNTUVNjY2QqlUCgDCxMREbNmyRQghxNevX4W1tbU4deqU1uO6dOki2rRpI4QQYuTIkcLV1VWEhYXFeAxnZ2cxZcoUrW0lS5YUvXv3FkII8ejRIwFArFixQnP/X3/9JQCIQ4cOabb5+PiIfPnyacWeLl068eXLF822JUuWCFtbWxEZGalT7EeOHBEAxMGDBzX37969WwDQvJ52dnbC19c3rqdRS7169cTgwYNjvV99zI8fP4rIyEjh7+8vypUrJ7JkySJCQ0M19//999+axwQFBQlzc3Oxbt06zbawsDDh7OwsZsyYEeu5+Pj4CADiwYMHmm09evQQnp6eQgjdX183Nzet+4cPH645ByGEWLVqlUiTJo3m/nLlyolu3bppPaZFixaibt26MT4nMb2HUpOwsDDx999/x/oeotSB9SDladlS/l/Onl2I3LmFsLOL/j+7SRMhVCrtxxmqLoSHC+HuLo8zfHj048QkIECIWrWEsLYW4p9/4nX4GO3cKeMxNRXiyhUhnj6V1wEhLl82/PGSM2N/JsT13fd77ApFGlWrVsWVK1dw9uxZeHl5oVOnTmjWrBkA4P79+wgODkbNmjVha2urufz555+ark5XrlxBxYoVY1xIJSAgAC9evED58uW1tpcvXx7+/v5a2woXLqy5rl6B2d3dXWvb9yuoFylSRGv9kLJlyyIoKAj//fefTrHHdGz1Cu7qYw0aNAhdu3ZFjRo1MG3aNK3HRkZGYtKkSXB3d0e6dOlga2uL/fv34+nTpzE+11FlzZoVdnZ2KFCgAL58+YKtW7fCwsJCc7+Hh4fm+oMHDxAeHq71PJqbm6NUqVI/fB6tra2RM2fOGJ9HXZ4jf3//aAtJli1bNs5z8/f31+k1JyJKqVQq4NAheX3tWuDePfkrfXAw8OQJ8M8/suVi+3bAwLPfa/zxB3D9urw+fToweHDc3bDevweqVwcOHJBxtm4N/L/nr0EEBQF9+sjrgwYBRYoALi5AixZy27x5hjsWJS52hUpg1tbyDRQTlUqFgIAA2NvbJ8hqy/qu02djY4PcuXMDAFauXIkiRYrgjz/+QJcuXRD0/5PYvXs3smTJovU49crk6m488RU1MVGva/D9NpUeo7t0iT2uY6uPNX78eLRt2xa7d+/G3r17MW7cOGzYsAFNmjTBzJkzMX/+fMybNw/u7u6wsbHBwIEDtbo0xeb48eOwtbWFpaUlsmTJEq0u2NjY6HyucZ3L9wlf1OdRn+eIiIh0d/Wq/KJuayu7PKlZWQHZssnL7NlAv37A0KFA2bKya1BcTpwA/P2Bzp1ld+q4BAYCY8fK656ewP79srtRWJhMZL7/+vHyJVCrFnDjBpA+PVCggDxe/fpykHWOHHo/BdGMGwc8fQpkzy6vq3l7Axs2AH/9BUybBjg5xb6PDRuAhQuB0aOBOnXiH1NSFhgIfP5s8eOCSQBbLBKYQgHY2BjnEp+5oE1MTDBq1CiMGTMGISEhcHNzg1KpxNOnT5E7d26ti4uLCwD5C/nx48c1K5BHZW9vD2dnZ814BbWTJ0/Czc3t5wP9v6tXryIkJERz+8yZM7C1tYWLi4tOsesqb9688Pb2xoEDB9C0aVOsWrVKcx6NGjVC+/btUaRIEeTMmVPnaVddXV2RK1cu2NnZ/bCsehB01OcxPDwc58+fj9fzqMtzVKBAAZw7d07rcWfUU3nEokCBAgn2mhMRJQcHD8q/lSsDMTToA5C/3rdoAYSHAy1bAh8+xFxOCGDmTDkeoXt34P/DCOM0Ywbw5g2QJ49sHfn9d/n9YNEioFcv7VmYHj8GKlaUSUXmzMC//wJ79sgWhdev5Rf4jx/1Ov1oLl/+1iKxZIn8vqJWqpRMrMLCgMWLY9/H6dNyzIp67YvJk1PubFJBQUDDhqYYM6Y8Xr82djQ/xsSCYtWiRQuYmppi0aJFsLOzw5AhQ+Dt7Y3Vq1fjwYMHuHTpEhYsWIDVq1cDAPr27YuAgAC0bt0aFy5cwL1797BmzRrNzEpDhw7F9OnTsXHjRty5cwcjRozAlStXMGDAgHjHGhYWhi5duuDWrVvYs2cPxo0bh759+8LExESn2H8kJCQEffv2xdGjR/HkyROcPHkS58+fR4ECBQAAefLkgZ+fH06dOgV/f3/06NEDrxPgE8DGxga9evXC0KFDsW/fPty6dQvdunVDcHAwunTp8tP71eU56tmzJ+7du4ehQ4fizp07WL9+PXx9fePc79ChQ+Hr64slS5bg3r17mDNnDrZt24YhQ4b8dKxERMmJOrGoUSP2MgqF/MKfK5fsHtWpU/SuSl+/Al5ewLBh3+6bMAE4fjz2/T57JltDANkFysIC6NoVWLVKHnP5cqBLF7mWhL8/UKGCnJnJ1VW2Uri5AXZ2wO7dQNaswO3bQJMmQGjozz0XkZEyIVKpZAIVU0uDt7f8u2SJPOfvvXoFNG8ukzBXV/lc/Por0LSpHKCeVOjQYeGHgoPl+h4nT5rg/XsrvHgR/30muEQY85Gi6Dt4Oy6RkZGagbvG5uXlJRo1ahRtu4+Pj8iYMaMICgoSKpVKzJs3T+TLl0+Ym5uLjBkzCk9PT3Hs2DFN+atXr4patWoJa2trYWdnJypWrKgZMBwZGSnGjx8vsmTJIszNzUWRIkXE3r17NY9VD96+HGXUVtQBzmrfDxBWxz527FiRPn16YWtrK7p16ya+fv2qKfOj2GM6zuXLlwUA8ejRIxEaGipat24tXFxchIWFhXB2dhZ9+/bVvNbv378XjRo1Era2tiJTpkxizJgxokOHDjE+pzGdW0x1IaaYhJD1rF+/fiJDhgxCqVSK8uXLi3Pnzun1nAkhxLhx40SRIkV0fo6EEGLnzp0id+7cQqlUiooVK4qVK1fGOXhbCCEWL14scubMKczNzUXevHnFn3/+GetzwsHbHLRLrAcpSUiIEFZWckDy9es/Ln/xohAWFrL87Nnf6sKTJ2GiTJlvg51/+02IDh3kbRcXId6/j3l/HTvKMhUqRB+wvW6dECYm8v6GDYXIkEFeL1BAiGfPou/r2jUh7O1lmbZthYjpq8vXr0IcPizEokVC/PmnELt2CXHypBC3bgnx6pUQc+bIx6dJI8SLFzHHHB4uB7kDQkSZy0UIIURYmBCVKn2LMyBAiD/++Pac5c0rj2VML14I0aaNfJ369hUiNPTn9hMcLET16vK87OxUYsaMY8li8DYTCz2l1MQiOYstKUpOWBckJhb8QkmsBynJ4cPyi6Gjo24zMQkhxOLF8jFmZkIcPx4uZs06IrJkUQlACAcHIfz8ZLmAADnDVGwzSl2+LIRCIe8/cybmY23aJI+jnpmqRAkh3r6NPTY/v2/lR42Sx/T3F2L+fCHq1RPCxka3WSuXLIn7OZg1S5YrWFD7vAYOVH/RFuL27W/bz50TImtWeZ+trRBbt2rvT6WSM2U+fSrE48dxH/tnhYfLhE+dfKkv5cvHnkTFJiREzsilPp9jx8I5KxQRERFRaqaeDapGDd3HPfbsCbRqBUREAC1amGLUqAp4/lyB/PmBc+e+damys5MDmM3N5YxSy5Z924cQwJAh8m/r1tqDxqNq0QLYvFkOLK9eXcabIUPssdWoIbtsAcDUqbJ7VIECcj2K3bvl2lyZMsnuO9WryzUzXF3l+hzq869eXXaHikuXLnLsxc2b37qS/fXXt7EZq1cD+fJ9K1+yJHDxIlClihyT0KyZHKvh7g44O8uB8ra2cqB8jhyyK5iufH3l48uXB3x85Oxa33dTO3tWjg/p31/O+FWypBxYbm8vx4GUKAGcOqXb8UJDZfwHDshJePbsAcqWjWMKr6QmERKdFIUtFkkPWyxSDrZY8JdqYj1ISUqXlr86r1ql3+M+fxYiT55vv3rXrh0pPn2Kuezs2bKMpeW37lZ79shtFhZCPHz44+NF6Tmsk3HjvsVmYSG77EyfLtejiO3fWESEEB8/6t5y06+f3H/dukJcvfqtS9moUbE/JjxcCG/v2FtK1OtkZM4sRJSlr2L1+bMQ6dNH30+2bEL07i3X4ujR41vLkIODbI2JiJCPv3tXtroAQpibyy5icZ1/aKgQDRrI8lZWQhw5Ircb+zOBXaESEBMLSgisCxITC36hJNaDlOLjx29jGJ4+1f/x164J4eERKVq0uC1CQmKvC5GRQtSpI4/j5ia7SLm5ydtDhvx8/HFRqYTYtk0mMLp8Qf8Z9+9/+8Ku7uZUq9a3L+1xOXNGiA0bhDhwQI5befxYiMBAmUCpx2/MnPnj/Ywd+23sxtKlQtSvLxO4mJKWDh2EeP06+j4CA78tkAjIcS/BwTIJ+vBBiEePZEL2779CNG78LUmMssat0T8T9EksuI4FERERkYEdPSpnP8qXTy7+pi93d+DUqUjs2XMbpqY5Yy1nYiK76xQpAty6Jbs9+fsD6dIBo0b9dPhxUijk7FAJKVcuoGFDYMcOObtV9uzA+vU/XrcDkM9BbN2/xo+Xs275+MguWfb2MZd7+xaYM0denzJFzkTVo4ecqenIEWDXLmDfPtn1a8YMOZ1wTGxtZZe1kiWB4cPla7V+feyzRimV8pyrV//xeSZFHGNBREREZGC6TDNrKJkyAWvWyC/8/v5y29ixQNq0CX/shKSeetbSEti2TS7YF1/t2wP588u1QubOjb3c1KlyvEaJEnLMg5q1tVw7Y8kS4NEjOb4itqRCTaGQY14OHJBjWKImFZaWgKOjXGekQgWZsNSqFb9zNCYmFglAfD+qh4h0wvcOEaUUiZlYqI8zfLi8niuXXPwuuatcGdiyRbYQFC9umH2amQETJ8rrs2fLVdG/9+TJtwX6pk6N34LDUVWvDvz3H3D/vmwRCQ0FQkLk2hx378o1SRKrviQUJhYGZP7/JTWDg4ONHAlR8qR+75jHtjwtEVEy8N9/wJ07sptSlSqJd9xJk+SMSfv2ycXwUoJmzYAyZQy/z2LFgMBAuXDg9yZMkK0KVaoANWsa9tiWljLxy5Ah5bxGUXGMhQGZmprCwcEBb968AQBYW1tDEUeaq1KpEBYWhq9fv8LEhDleapba64IQAsHBwXjz5g0cHBxgqksnWiKiJEo9zayHh5xqNbGYmQEdOiTe8ZIrExNg8mTZpWnBAmDgQDktLSC7kq1eLa/7+BiutSK1YGJhYE5OTgCgSS7iIoRASEgIrKys4kxAKOVjXZAcHBw07yEiouQqsbtBkf7q1JFrU5w8KQdnL1okt//6qxx037Ch4VtKUgMmFgamUCiQOXNmZMqUCeHh4XGWDQ8Px7///otKlSqx60cqx7oguz+xpYKIkjshtBfGo6RJoZDjJypXBpYvl4Or378Htm6V902ZYuwIkycmFgnE1NT0h1+STE1NERERAUtLy1T7ZZIk1gUiopTh1i05GNfKSq7+TElXpUpyBqYDB+S4ihcv5Pb27YFChYwbW3KV+jpzExERESUQdTeoihXlQF1K2tQtE6tXA35+gLm5TDLo5zCxICIiIjIQjq9IXjw8tBf7694dcHU1XjzJHbtCEREREX3n/HnZRebdu2+X9+/l36AgoHZtYORIoECBb48JD5crbgNMLJKTSZOAf/6RLUxjxhg7muSNiQURERFRFIGBMjEICIi9zJo1wNq1QNOmwOjRcl2Ec+dk0pE+PVCkSOLFS/FTsCBw+rQcF8OJCeOHiQURERFRFOvXy6Qia1agXTu5mJn6kj69bJmYPx/Ytk3OIrR1q5y+VL1mRfXqcq0ESj5KljR2BCkDEwsiIiKi/xMCWLZMXh80CPD2jrlcpUrAzZtyEbW//gL27v12H7tBUWrFfJqIiIjo/86fBy5fBpRKwMsr7rIFC8ruUHfvAt26yRmFbG1l6wVRasTEgoiIiOj/li6Vf1u2BNKl0+0xuXLJRdaePZPrWGTNmnDxESVl7ApFREREBODTJ2DDBnm9Z0/9H58pk0HDIUp22GJBREREBDnTU0iIXHWZq2YT6Y+JBREREaV6QnzrBtWzJ6BQGDceouSIiQURERGleidPyvER1tZA+/bGjoYoeWJiQURERKmeurWiTRsgTRrjxkKUXDGxICIiolTt3TtgyxZ5/WcGbRORxMSCiIiIUrXVq4HQUKB4ccDDw9jRECVfnG6WiIiIkrQvX4CHD4EHD+Qld26gYUPDDLCOutI2WyuI4oeJBRERESUp168Dv/0G3LkD3L8PvHwZvUyrVsDvvwN2dvE71pEjwL17cj9t2sRvX0SpHRMLIiIiShIiI4GZM4GxY4HwcO370qaVLRXOzsDu3cDGjcDly8DmzUDhwj9/TPWg7fbtAVvbn98PEaWwMRbjx4+HQqHQuuTPn19z/9evX9GnTx+kT58etra2aNasGV6/fm3EiImIiAiQrQYVKwIjR8qkon59YP164Nw54P174MMHef3vv4F//wWyZgXu3gVKlwZWrJBdmvT16hWwfbu83qOHQU+HKFVKUYkFABQsWBAvX77UXE6cOKG5z9vbGzt37sTmzZtx7NgxvHjxAk2bNjVitERERKmbEMDixUDRosDp07JL0sqVwD//yK5JJUsC6dJpP6ZsWdlaUacO8PUr0K0b0KEDEBSk37F//RWIiADKlAGKFDHYKRGlWikusTAzM4OTk5PmkiFDBgDA58+f8ccff2DOnDmoVq0aSpQogVWrVuHUqVM4c+aMkaMmIiJKfZ4+BTw9gT59gOBgoEoVOb6iU6cfD8zOkAHYtQvw8QFMTYG1a2UScveubsdesUJeFApg0qR4nwoRIQUmFvfu3YOzszNy5syJdu3a4enTpwCAixcvIjw8HDVq1NCUzZ8/P7Jly4bTp08bK1wiIqJU4907YNs2oH9/2UKQIwfg5wdYWgLz5gGHDgHZs+u+PxMTYMQIOQDb2Rm4fRuoXFn+jcu5czKZAWRSEeWrARHFQ4oavF26dGn4+voiX758ePnyJSZMmICKFSvixo0bePXqFSwsLODg4KD1GEdHR7x69SrWfYaGhiI0NFRzOyAgAAAQHh6O8O9HlulJ/fj47oeSP9YFAlgPSDJkPXj3Dqhb1wxFiwosXx4Z7/39jGfPgNmzTXDkiAlu3YreDFG+vApLlkQif345eDvyJ8IsU0YmC3XqmOH6dQWqVhU4cCACUYZZarx5AzRrZoawMAUaNFBhyJDIaAPFkwp+JhBg/Hqgz3EVQvzMcKfk4dOnT8iePTvmzJkDKysrdOrUSStJAIBSpUqhatWqmD59eoz7GD9+PCZMmBBt+/r162FtbZ0gcRMRERmCr68b/v47DwBgwYJDcHHRcxBCPAUEWGDEiIp48eLbdEvZsgWgYMH3KFjwHQoWfI+0aUPj2IP+xxs7thweP06DtGm/YtKkk8ia9ds5R0YqMH58WVy/nhHOzkGYOfMYbGwiDHZ8opQoODgYbdu2xefPn2Fvbx9n2RSdWABAyZIlUaNGDdSsWRPVq1fHx48ftVotsmfPjoEDB8Lb2zvGx8fUYuHi4oJ379798Mn9kfDwcPj5+aFmzZowNzeP174oeWNdIID1gCRD1YMXL4D8+c3w9atsJfD2jsT06SpDhflDX78CtWub4tQpE2TLJjBzZiQqVBDImDFhj/vuHeDpKVsuHB0F/Py+tVyMGGGCOXNMYWMjcPJkBNzcEjaW+OJnAgHGrwcBAQHIkCGDTolFiuoK9b2goCA8ePAAv/zyC0qUKAFzc3McOnQIzZo1AwDcuXMHT58+RdmyZWPdh1KphFKpjLbd3NzcYC+uIfdFyRvrAgGsByTFtx7MmCG/3GfIIL9sr1ljCh8fU8TwL83gVCqgSxfg1CkgTRpgzx4FChZMnK8cmTMDhw/LcRNXrypQs6Y5jh4Frl0D5syRZXx9FShSJPm8x/iZQIDx6oE+x0xRg7eHDBmCY8eO4fHjxzh16hSaNGkCU1NTtGnTBmnSpEGXLl0waNAgHDlyBBcvXkSnTp1QtmxZlClTxtihExERGczjx3JVagD46y85sPndOzmFa2IYMUIuXGduLteJKFgwcY6rliEDcPCgHCD++rWcbapzZ3nfsGFA8+aJGw9RapGiEotnz56hTZs2yJcvH1q2bIn06dPjzJkzyPj/dte5c+eifv36aNasGSpVqgQnJyds27bNyFETEREZ1oQJcpG56tXlL/fqL9XqZCMhLVkiV88G5HoUVasm/DFj8n1y8eWLfD6mTDFOPESpQYrqCrVhw4Y477e0tMSiRYuwaNGiRIqIiIgocd25A/z5p7yu/hLduTMwebKc2vXRI8DVNWGOvWsX0LevvD5pEtC+fcIcR1cZMsgpbFu0AAIDZeuNWYr65kOUtKSoFgsiIqLUbtw4OcahQQOgdGm5zdUVqFlTXl+5MmGOe/Ei0KrVt/EVo0cnzHH0lT69HHNx7hwSfOA4UWrHxIKIiCiFuHoV2LhRXv9+NemuXeXflSuBCAPNsBoYKI/XurVcmC44GKhVS3aH+tHK2YktqcVDlBKxQZCIiCiF+PVX+bdVKzm2IKpGjWTXoBcvgH37gPr1f+4Yb9/KQeDbt8uuVWFh3+4rXfrboG0iSn3YYkFERJQCnDkD7NwJmJgA48dHv1+pBLy85PWfGcT97Jl8vJOTbP3YvVsmFXnyAMOHy+OfOgXEc4knIkrG2GJBRESUAqhbKzp0gGZBuO916QLMni2Tghcv5DS0P/Lli5zlacYMICREbiteHGjSRF7c3NjNiIgktlgQERElc4cOyalVzc3l4O3YFCgAVKgAREYCvr5x71OlkrNL5csnp68NCQHKlwfOnpUDtceMketTMKkgIjUmFkRERMlUQAAwciRQt6683a0bkCNH3I9RD+L+4w+ZPMTk2DE5XsLLC3j+XO5z40bg+HGgVClDRU9EKQ0TCyIiomQmMlImBnnzAtOmybEONWpEnwkqJi1ayHEQDx8CR4582x4RIQdely0rV6q+cAGwswN8fAB/f6BlS7ZOEFHcmFgQERElIUIAAQEWCAmR17937BhQsqRseXj9Wg6e/ucf4MABIF26H+/f2hpo105e//13OWXsvHlyPy1bykHYFhZAjx7AvXvAiBGApaVBT5GIUigO3iYiIkpC+vQxwYoVdQDIL/gODt8uJibyiz8ApEkDjB0rV7q2sNDvGN26ybUmtm0D9u6VXaoAuZhc795Anz6Ao6OhzoiIUoufSiyOHTuGWbNmwd/fHwDg5uaGoUOHomLFigYNjoiIKDW5dg34449vnQnCwoA3b+RFzcQE6NlTTin7sytJFysmZ3a6dAkID5cDtAcNAn75BbCyit85EFHqpXdisXbtWnTq1AlNmzZF//79AQAnT55E9erV4evri7Zt2xo8SCIiotRg9GhACAXKl3+Of/7JhC9fzPHpEzSXz59lN6h8+eJ/rN9/B5YuBRo2lIO/Tdg5mojiSe/EYsqUKZgxYwa8vb012/r37485c+Zg0qRJTCyIiIh+wsmTwK5dgKmpQNu2/rCzy4R06QAXl4Q5XvHiwPLlCbNvIkqd9P594uHDh2jQoEG07Q0bNsSjR48MEhQREVFqIgQwapS87uUlkCXLF+MGRET0E/ROLFxcXHDo0KFo2w8ePAiXhPpZhYiIKBl6/Fiu/xAREXe5/fuBf/8FlEpgzJjIRImNiMjQ9O4KNXjwYPTv3x9XrlxBuXLlAMgxFr6+vpg/f77BAyQiIkqOnj6Vi8y9eQO0aQOsWQOYmkYvp1J9a63o0wfImlUO4iYiSm70Tix69eoFJycnzJ49G5s2bQIAFChQABs3bkSjRo0MHiAREVFyExQkB0WrZ3P66y/ZGvHHH9EHSW/ZAly+LBejGzky8WMlIjKUn5putkmTJmjSpImhYyEiIkr2VCqgQwfg6lUgUyZgzBjA2xvw9ZXJxZIl31awjogAfv1VXh88GMiQQU7/SkSUHHGBPCIiIgMaNw7Yvl0uWrd9O1CunFx4rn17YNkyuYr13LkyufD1Be7elQnFoEHGjpyIKH50SizSpUuHu3fvIkOGDEibNi0U6p9aYvDhwweDBUdERJSc/PUXMHmyvL58uUwqAKBtWyA0FOjcGZg/X7ZcTJggL4Bcv8LOzjgxExEZik6Jxdy5c2H3/0+8efPmJWQ8REREydL58zJxAIChQwEvL+37O3WSK2n37AnMmAEcOQI8eybXqejZM/HjJSIyNJ0SC68on45e339SEhERpXLPnwONGgFfvwL16wM+PjGX69FDlhk4UCYiADB+vOweRUSU3OmUWAQEBOi8Q3t7+58OhoiIKDkRQiYIvXsDL18CBQsC69bFPK2s2oABslvU8OGAm5sc6E1ElBLolFg4ODjEOa4iqshILuxDREQp27Nncl2KP/8Ebt+W29KnB/75B9Dl97Vhw4AqVYDs2QEzTqNCRCmETh9nR44c0Vx//PgxRowYgY4dO6Js2bIAgNOnT2P16tXwia3tl4iIKJkLCZFrTvz5J3DokGytAAArK6BpU7kGRc6cuu+vVKmEiZOIyFh0SiwqV66suT5x4kTMmTMHbdq00Wxr2LAh3N3dsXz5co7BICKiFGf/fjk+4smTb9sqV5YDtJs1062VgogopTP5cRFtp0+fhoeHR7TtHh4eOHfunEGCIiIiSgrevZNjIGrXlklF1qxyitiHD4GjR+VMT0wqiIgkvRMLFxcX/P7779G2r1ixAi4uLgYJioiIyJiEkGtSuLnJsRQKhZzJyd8fGDsWcHU1doREREmP3kPG5s6di2bNmmHv3r0oXbo0AODcuXO4d+8etm7davAAiYiIEtPTp0CvXsCePfJ2oULAihXA///lERFRLPRusahbty7u3r2LBg0a4MOHD/jw4QMaNGiAu3fvom7dugkRIxERUaK4fFkmEnv2ABYWwKRJwMWLTCqIiHTxU5Pcubi4YOrUqYaOhYiIyGjCwuR4isBAOWOTry9QoICxoyIiSj70brEAgOPHj6N9+/YoV64cnj9/DgBYs2YNTpw4YdDgiIiIEsukScCNG0DGjMCuXUwqiIj0pXdisXXrVnh6esLKygqXLl1CaGgoAODz589sxSAiomTp0iVAvRTT4sUyuSAiIv3onVhMnjwZS5cuxe+//w5zc3PN9vLly+PSpUsGDY6IiCihhYUBHTsCkZFAixZA8+bGjoiIKHnSO7G4c+cOKlWqFG17mjRp8OnTJ0PERERElGimTAGuXwcyZAAWLjR2NEREyZfeiYWTkxPu378fbfuJEyeQM2dOgwRFRESUGC5fBtS9eBctAjJlMm48RETJmd6JRbdu3TBgwACcPXsWCoUCL168wLp16zBkyBD06tUrIWIkIiIyuLAwuXJ2RATQrJnsBkVERD9P7+lmR4wYAZVKherVqyM4OBiVKlWCUqnEkCFD0K9fv4SIkYiIyOB8fICrV4H06WVrhUJh7IiIiJI3vRMLhUKB0aNHY+jQobh//z6CgoLg5uYGW1vbhIiPiIjI4K5cASZPltcXLQIcHY0aDhFRivBTC+QBgIWFBdzc3AwZCxERUYITAujRQ3aBatoUaNnS2BEREaUMeo+x+Pr1K2bOnIm6devCw8MDxYsX17okF4sWLUKOHDlgaWmJ0qVL49y5c8YOiYiIEsHBg8C5c4CVFbtAEREZkt4tFl26dMGBAwfQvHlzlCpVCopk+Im8ceNGDBo0CEuXLkXp0qUxb948eHp64s6dO8jEKUGIiFI09UJ43boBTk7GjYWIKCXRO7HYtWsX9uzZg/LlyydEPIlizpw56NatGzp16gQAWLp0KXbv3o2VK1dixIgRRo6OiIgSypkzwJEjgJkZMHiwsaMhIkpZ9E4ssmTJAjs7u4SIJVGEhYXh4sWLGDlypGabiYkJatSogdOnT0crHxoaitDQUM3tgIAAAEB4eDjCw8PjFYv68fHdDyV/rAsEsB4khilTTAGYoF07FTJnjkRSfKpZD0iNdYEA49cDfY6rd2Ixe/ZsDB8+HEuXLkX27Nn1fbjRvXv3DpGRkXD8bgoQR0dH3L59O1p5Hx8fTJgwIdr2AwcOwNra2iAx+fn5GWQ/lPyxLhDAepBQnjyxw65d1aBQCJQqdQR79gQZO6Q4sR6QGusCAcarB8HBwTqX1Tux8PDwwNevX5EzZ05YW1vD3Nxc6/4PHz7ou8skbeTIkRg0aJDmdkBAAFxcXFCrVi3Y29vHa9/h4eHw8/NDzZo1oz2PlLqwLhDAepDQOnY0BQA0bizQrVslI0cTO9YDUmNdIMD49UDdW0cXeicWbdq0wfPnzzF16lQ4Ojomu8HbGTJkgKmpKV6/fq21/fXr13CKYRSfUqmEUqmMtt3c3NxgL64h90XJG+sCAawHCeHRI2DjRnl99GgTmJvrPSliomM9IDXWBQKMVw/0OabeicWpU6dw+vRpFClSRN+HJgkWFhYoUaIEDh06hMaNGwMAVCoVDh06hL59+xo3OCIiShCzZgGRkUCtWkCJEsaOhogoZdI7scifPz9CQkISIpZEM2jQIHh5ecHDwwOlSpXCvHnz8OXLF80sUURElHK8egX88Ye8HmXeDiIiMjC9E4tp06Zh8ODBmDJlCtzd3aM1j8R33EFiaNWqFd6+fYuxY8fi1atXKFq0KPbt2xdtQDcRESV/8+YBoaFAmTJA5crGjoaIKOXSO7GoXbs2AKB69epa24UQUCgUiIyMNExkCaxv377s+kRElMJ9+gQsXiyvjxrFVbaJiBKS3onFkSNHEiIOIiIig1u8GAgMBAoVAurVM3Y0REQpm96JRWW2IxMRUTLw5YvsBgUAI0YAJkl/IigiomRN78QCAD59+oQ//vgD/v7+AICCBQuic+fOSJMmjUGDIyIi+hknTwJdugBv3wKurkCrVsaOiIgo5dP795sLFy4gV65cmDt3Lj58+IAPHz5gzpw5yJUrFy5dupQQMRIREekkMBDo1w+oWBG4cwdwcgJ8fQGzn/oZjYiI9KH3R623tzcaNmyI33//HWb//6SOiIhA165dMXDgQPz7778GD5KIiOhH9u0DevQAnj6Vt7t0AWbOBNKmNW5cRESphd6JxYULF7SSCgAwMzPDsGHD4OHhYdDgiIiIfuT9e8DbG1izRt52dQWWLwdq1DBuXEREqY3eXaHs7e3xVP1zUBT//fcf7OzsDBIUERGRLoQA6tSRSYWJCTBoEHD9OpMKIiJj0LvFolWrVujSpQtmzZqFcuXKAQBOnjyJoUOHok2bNgYPkIiIKDZnzwLnzwNWVsCRI0Dp0saOiIgo9dI7sZg1axYUCgU6dOiAiIgIAIC5uTl69eqFadOmGTxAIiKi2Pzxh/zbogWTCiIiY9MrsYiMjMSZM2cwfvx4+Pj44MGDBwCAXLlywdraOkECJCIiiklQELBhg7zepYtxYyEiIj0TC1NTU9SqVQv+/v5wdXWFu7t7QsVFREQUp02bZHKRJ4+cXpaIiIxL78HbhQoVwsOHDxMiFiIiIp2pu0F17gwoFMaNhYiIfiKxmDx5MoYMGYJdu3bh5cuXCAgI0LoQERElNH9/4NQpwNQU8PIydjRERAT8xODtunXrAgAaNmwIRZSfiIQQUCgUiIyMNFx0REREMVi5Uv6tVw/InNm4sRARkaR3YnHkyJGEiIOIiEgn4eHAn3/K6xy0TUSUdOiVWAgh4OzsjLCwMOTLl09r9W0iIqLEsGsX8OYN4OQE/L8RnYiIkgCdx1g8evQIhQsXRv78+VG4cGHkypULFy5cSMjYiIiIolmxQv718gL4+xYRUdKhc2IxdOhQREREYO3atdiyZQuyZs2KHj16JGRsREREWp4/B/btk9c7dzZuLEREpE3n33pOnDiBLVu2oEKFCgCAMmXKIGvWrPjy5QtsbGwSLEAiIiI1X19ApZLrVuTNa+xoiIgoKp1bLN68eYM8efJobmfOnBlWVlZ48+ZNggRGREQUlUr1bTYoDtomIkp6dG6xUCgUCAoKgpWVlWabiYkJAgMDtdavsLe3N2yEREREAI4dAx4+BOzsgObNjR0NERF9T+fEQgiBvN+1OwshUKxYMc11rmNBREQJRb3Sdtu2AHvgEhElPTonFly/goiIjOXTJ2DrVnmd3aCIiJImnROLypUrJ2QcREREMVKpgH79gK9fAXd3wMPD2BEREVFMdB68TURElNiEAPr0AdauBUxNgRkzAIXC2FEREVFMmFgQEVGSJAQwfDiwdKlMJtasAWrXNnZUREQUGyYWRESUJE2ZAsycKa8vWwa0aWPceIiIKG5MLIiIKMmZNw/49Vd5fc4coFs3o4ZDREQ6YGJBRERJyh9/AN7e8vqECd+uExFR0qbzrFBRXbhwAZs2bcLTp08RFhamdd+2bdsMEhgREaU+Gzd+a50YMuRbqwURESV9erdYbNiwAeXKlYO/vz+2b9+O8PBw3Lx5E4cPH0aaNGkSIkYiIkoFHj8GOnWSg7Z79uQMUEREyY3eicXUqVMxd+5c7Ny5ExYWFpg/fz5u376Nli1bIlu2bAkRIxERpQL9+wMhIUClSsCiRUwqiIiSG70TiwcPHqBevXoAAAsLC3z58gUKhQLe3t5Yvny5wQMkIqKUb8cOYOdOwMwMWLIEMOEIQCKiZEfvj+60adMiMDAQAJAlSxbcuHEDAPDp0ycEBwcbNjoiIkrxvnyRrRWAHFfh5mbceIiI6OfoPXi7UqVK8PPzg7u7O1q0aIEBAwbg8OHD8PPzQ/Xq1RMiRiIiSsEmTgSePgWyZ+dgbSKi5EzvxGLhwoX4+vUrAGD06NEwNzfHqVOn0KxZM4wZM8bgARIRUcp144ZcpwIAFiwArK2NGw8REf08vROLdOnSaa6bmJhgxIgRBg2IiIhSByGA3r2BiAigUSOgQQNjR0RERPGh9xiLS5cu4fr165rbO3bsQOPGjTFq1Khoa1oQERHFZvVq4Phx2Urx22/GjoaIiOJL78SiR48euHv3LgDg4cOHaNWqFaytrbF582YMGzbM4AESEVHK8+EDMHSovD5uHMDZyomIkj+9E4u7d++iaNGiAIDNmzejcuXKWL9+PXx9fbF161ZDx0dERCnQyJHAu3dAwYKAt7exoyEiIkPQO7EQQkClUgEADh48iLp16wIAXFxc8O7dO8NGp6ccOXJAoVBoXaZNm6ZV5tq1a6hYsSIsLS3h4uKCGTNmGClaIqLU6cwZQL3s0ZIlgLm5ceMhIiLD0HvwtoeHByZPnowaNWrg2LFjWLJkCQDg0aNHcHR0NHiA+po4cSK6deumuW1nZ6e5HhAQgFq1aqFGjRpYunQprl+/js6dO8PBwQHdu3c3RrhERKlKRATQs6e83rEjULGiUcMhIiID0juxmDdvHtq1a4e///4bo0ePRu7cuQEAW7ZsQbly5QweoL7s7Ozg5OQU433r1q1DWFgYVq5cCQsLCxQsWBBXrlzBnDlzmFgQESWCBQuAq1eBtGkBNhgTEaUseicWhQsX1poVSm3mzJkwNTU1SFDxMW3aNEyaNAnZsmVD27Zt4e3tDTMzeZqnT59GpUqVYGFhoSnv6emJ6dOn4+PHj0ibNm20/YWGhiI0NFRzOyAgAAAQHh6O8PDweMWqfnx890PJH+sCASm/Hjx7BowdawZAgalTI+DgIJBCTzVeUno9IN2xLhBg/Hqgz3H1TizUwsLC8ObNG814C7VsRpzao3///ihevDjSpUuHU6dOYeTIkXj58iXm/H/1pVevXsHV1VXrMeruW69evYoxsfDx8cGECROibT9w4ACsDbSSk5+fn0H2Q8kf6wIBKbcezJjhgaCgLMiX7wMcHY9jzx5jR5S0pdR6QPpjXSDAePUgODhY57IKIYTQZ+d3795Fly5dcOrUKa3tQggoFApERkbqs7sfGjFiBKZPnx5nGX9/f+TPnz/a9pUrV6JHjx4ICgqCUqlErVq14OrqimXLlmnK3Lp1CwULFsStW7dQoECBaPuIqcVCPVDd3t4+HmcmM0A/Pz/UrFkT5hy9mKqxLhCQsuvB/v0KNGhgBlNTgbNnI1C4sLEjSrpScj0g/bAuEGD8ehAQEIAMGTLg8+fPP/zuq3eLRadOnWBmZoZdu3Yhc+bMUCgUPx2oLgYPHoyOHTvGWSZnzpwxbi9dujQiIiLw+PFj5MuXD05OTnj9+rVWGfXt2MZlKJVKKJXKaNvNzc0N9uIacl+UvLEuEJDy6kFICDBggLw+YIACJUqknHNLSCmtHtDPY10gwHj1QJ9j6p1YXLlyBRcvXoyxhSAhZMyYERkzZvypx165cgUmJibIlCkTAKBs2bIYPXo0wsPDNU+Sn58f8uXLF2M3KCIiij8fH+DhQyBLFmD8eGNHQ0RECUXvdSzc3NyMvl5FTE6fPo158+bh6tWrePjwIdatWwdvb2+0b99ekzS0bdsWFhYW6NKlC27evImNGzdi/vz5GDRokJGjJyJKme7cAdS9WefPB6LMAE5ERCmM3i0W06dPx7BhwzB16lS4u7tHax6J77iDn6VUKrFhwwaMHz8eoaGhcHV1hbe3t1bSkCZNGhw4cAB9+vRBiRIlkCFDBowdO5ZTzRIRJQAhgN69gbAwoE4doGlTY0dEREQJSe/EokaNGgCA6tWra21PqMHbuipevDjOnDnzw3KFCxfG8ePHEyEiIqLUbd064PBhwNISWLgQSOAheUREZGR6JxZHjhxJiDiIiCgJe/YMeP4ccHOLuzvTw4fA1q3Ali3AuXNy2+jRQCxzbBARUQqid2JRuXLlhIiDiIiSoKAgYNIkYM4cICJCbsuVCyhcWF6KFAFcXAA/P5lMXLr07bEKBdCkCTB0qHFiJyKixPVTC+QdP34cy5Ytw8OHD7F582ZkyZIFa9asgaurKypUqGDoGImIKJEJIROFQYNkawUAZMgAvHsHPHggL9u3R3+ciQlQtSrQvDnQuDEQy0zeRESUAuk9K9TWrVvh6ekJKysrXLp0SbN43OfPnzF16lSDB0hERInrzh2gVi2gZUuZVLi6Ajt3Am/fysvhw8C8eUCnTkCJEkDatICnJ/D778CrV8DBg0DPnkwqiIhSG71bLCZPnoylS5eiQ4cO2LBhg2Z7+fLlMXnyZIMGR0REiefrV2DiRGDWLCA8HFAqgREjgOHDASsrWSZDBtkiUbWqcWMlIqKkR+/E4s6dO6hUqVK07WnSpMGnT58MERMRESWyyEigdWtgxw55u25d4Lff5HgKIiIiXejdFcrJyQn379+Ptv3EiRPIyWk/iIiSpaFDZVKhVMpZnXbtYlJBRET60Tux6NatGwYMGICzZ89CoVDgxYsXWLduHYYMGYJevXolRIxERJSAFi8G5s6V11evlgvZcc0JIiLSl95doUaMGAGVSoXq1asjODgYlSpVglKpxJAhQ9CvX7+EiJGIiBLI3r2A+qN7yhSgVSvjxkNERMmX3omFQqHA6NGjMXToUNy/fx9BQUFwc3ODra1tQsRHREQJ5OpVOfOTSgV07AiMHGnsiIiIKDn7qXUsAMDCwgJ2dnaws7NjUkFElMy8eAHUry8XwKtaFVi2jN2fiIgofvQeYxEREYFff/0VadKkQY4cOZAjRw6kSZMGY8aMQXh4eELESEREBvTlC9CggVyjIn9+OVjbwsLYURERUXKnd4tFv379sG3bNsyYMQNly5YFAJw+fRrjx4/H+/fvsWTJEoMHSUREhiEE8MsvwKVLck2K3bvlAndERETxpXdisX79emzYsAF16tTRbCtcuDBcXFzQpk0bJhZEREnYiRPA9u2yhWLHDoCzhBMRkaHo3RVKqVQiR44c0ba7urrCgm3pRERJ2syZ8m/HjkC5ckYNhYiIUhi9E4u+ffti0qRJCA0N1WwLDQ3FlClT0LdvX4MGR0REhnP7NrBzpxykPWiQsaMhIqKURu+uUJcvX8ahQ4eQNWtWFClSBABw9epVhIWFoXr16mjatKmm7LZt2wwXKRERxcucOfJvw4ZAvnzGjYWIiFIevRMLBwcHNGvWTGubi4uLwQIiIiLDe/0a+PNPeX3IEOPGQkREKZPeicWqVasSIg4iIkpACxcCoaFAmTJA+fLGjoaIiFIivcdYEBFR8vLlC7B4sbw+ZAgXwiMiooShd4vF+/fvMXbsWBw5cgRv3ryBSqXSuv/Dhw8GC46IiOJv1SrgwwcgVy6gcWNjR0NERCmV3onFL7/8gvv376NLly5wdHSEgj99ERElWZGR3wZtDxoEmJoaNx4iIkq59E4sjh8/jhMnTmhmhCIioqRr+3bg0SMgfXq5dgUREVFC0XuMRf78+RESEpIQsRARkQEJ8W1BvD59AGtr48ZDREQpm96JxeLFizF69GgcO3YM79+/R0BAgNaFiIiShhMngHPnAKVSJhZEREQJ6afWsQgICEC1atW0tgshoFAoEBkZabDgiIjo56lbK7y8gEyZjBsLERGlfHonFu3atYO5uTnWr1/PwdtEREnU7dvAzp1yatlBg4wdDRERpQZ6JxY3btzA5cuXkS9fvoSIh4iIDGDMGPm3YUOAH9dERJQY9B5j4eHhgf/++y8hYiEiIgP45x9g61Y5teyECcaOhoiIUgu9Wyz69euHAQMGYOjQoXB3d4e5ubnW/YULFzZYcEREpJ/AwG8DtYcMATgzOBERJRa9E4tWrVoBADp37qzZplAoOHibiCgJGDMGePYMcHUFxo41djRERJSa6J1YPHr0KCHiICKieDp3DliwQF5fupTrVhARUeLSO7HInj17QsRBRETxEB4OdO8uF8Vr3x6oVcvYERERUWqjd2IBAA8ePMC8efPg7+8PAHBzc8OAAQOQK1cugwZHRES6mTsXuHoVSJcOmDPH2NEQEVFqpPesUPv374ebmxvOnTuHwoULo3Dhwjh79iwKFiwIPz+/hIiRiIji8OABMH68vD5nDpAxo1HDISKiVErvFosRI0bA29sb06ZNi7Z9+PDhqFmzpsGCIyKiuAkB9OoFhIQA1aoBHToYOyIiIkqt9G6x8Pf3R5cuXaJt79y5M27dumWQoIiISDfr1wN+foBSKQdsKxTGjoiIiFIrvROLjBkz4sqVK9G2X7lyBZkyZTJETEREpIP374GBA+X1sWOBPHmMGg4REaVyeneF6tatG7p3746HDx+iXLlyAICTJ09i+vTpGDRokMEDJCKimA0ZArx7BxQsKK8TEREZk96Jxa+//go7OzvMnj0bI0eOBAA4Oztj/Pjx6N+/v8EDJCKi6A4fBnx9Zden338HLCyMHREREaV2eneFUigU8Pb2xrNnz/D582d8/vwZz549w4ABA6BIwM69U6ZMQbly5WBtbQ0HB4cYyzx9+hT16tWDtbU1MmXKhKFDhyIiIkKrzNGjR1G8eHEolUrkzp0bvr6+CRYzEVFCCAkBevSQ13v1AsqWNW48REREwE8kFo8ePcK9e/cAAHZ2drCzswMA3Lt3D48fPzZocFGFhYWhRYsW6NWrV4z3R0ZGol69eggLC8OpU6ewevVq+Pr6YuzYsVqx16tXD1WrVsWVK1cwcOBAdO3aFfv370+wuInox86ckTMa1agBjBoFbN8OPH9u7KgS3969Cty4kf6H5aZMAe7fB5ydgalTEyEwIiIiHejdFapjx47o3Lkz8nw3SvDs2bNYsWIFjh49aqjYtEyYMAEAYm1hOHDgAG7duoWDBw/C0dERRYsWxaRJkzB8+HCMHz8eFhYWWLp0KVxdXTF79mwAQIECBXDixAnMnTsXnp6eCRI3EcVOCLnuwogRgLpx8dChb/dnzgyUKgXkywd8/QoEBn67BAXJbe3bfxvAnJxduwY0amQGoALMzSMxdGjMMzzduAFMny6vL1wIpEmTqGESERHFSu/E4vLlyyhfvny07WXKlEHfvn0NEtTPOH36NNzd3eHo6KjZ5unpiV69euHmzZsoVqwYTp8+jRo1amg9ztPTEwPj+FYSGhqK0NBQze2AgAAAQHh4OMLDw+MVs/rx8d0PJX/JrS6oVMDEiSa4cEGBfv1UqFVL6D3N6YcPQJcupti9WzacNmumQvXqKly4YILz5xW4eRN4+VKBHTvi3s/Fi4BKFYl+/VQ/eTZJw5w5plA3Ig8fboonTyIxe7YKpqbfyqhUQLdupoiIMEGDBirUrx+JZFJlSA/J7fOAEg7rAgHGrwf6HFfvxEKhUCAwMDDa9s+fPyMyMlLf3RnMq1evtJIKAJrbr169irNMQEAAQkJCYGVlFW2/Pj4+mtaSqA4cOABra2uDxM4Vy0ktOdSF8HATzJtXHCdPZgEAHDhggvz536Nt29soXPidTvu4cyctZs3ywNu35jAzi0SXLjdQu/ZjKBRAw4by8vWrKR4+TIP79x3w5o01LC0jYWUVAUvLCFhZycu9e2mxbVseDBligpcvL6FChRcJeeoJ5tMnJdatk4uL1qz5GH5+ObB4sSkuXHiDQYMuwtJSfrbu3ZsDZ84UgaVlBBo1OoQ9e74aM2xKYMnh84ASB+sCAcarB8HBwTqX1TuxqFSpEnx8fPDXX3/B9P8/pUVGRsLHxwcVKlTQa18jRozAdHWbfiz8/f2RP39+fcM0mJEjR2pNoxsQEAAXFxfUqlUL9vb28dp3eHg4/Pz8ULNmTZibm8c3VErGkktd+PwZaNHCFCdPmsDcXKB5c4Ht2xW4fTs9xo4tj8qVVRg/XoXy5UWMjxcCmDfPBKNHmyAiQoFcuQTWr1ehWDE3AG56xyMEMHBgJJYsMcVvv3mgVq1IVKkS87EN7eNH4MABBf77T4GWLVXIlu3n9zVpkgkiIkxRsmQk+vS5Ci+vzOjSxQLnzmXGrFl1sX17JCIigA4d5Ef2tGkKdOhQzUBnQklNcvk8oITHukCA8euBureOLvROLKZPn45KlSohX758qFixIgDg+PHjCAgIwOHDh/Xa1+DBg9GxY8c4y+TMmVOnfTk5OeHcuXNa216/fq25T/1XvS1qGXt7+xhbKwBAqVRCqVRG225ubm6wF9eQ+6LkLSnXhZcvgTp1gKtXAVtbYPt2BWrUUODFC8DHB1i+HDh2zARVq5qgVi2gUCG5gNu7d9/+vnsHfPok99eyJfD77wrY28fvfBcsAN68AbZuVaB5czMcPw4ULhz/8/2eEHJ8w+7d8nLqlOyaBAATJ5pi0CA5VuT/81no7OtXYNkyeX3AAJkUtWxpghw5FGjUCLhwwQSVKpkgZ04gIAAoXRro29dU88MOpVxJ+fOAEhfrAgHGqwf6HFPvxMLNzQ3Xrl3DwoULcfXqVVhZWaFDhw7o27cv0qVLp9e+MmbMiIwZM+obQozKli2LKVOm4M2bN5oVwP38/GBvbw83NzdNmT179mg9zs/PD2U5VyNRnO7cATw9gSdPAEdHYM8eoHhxeZ+zs/xyP3SonK1o5UrgwAF5iYmlpRyw3bNnzIOT9WVqCqxdK5OL48dl8nP6NPRuQVCpgMePgdevZQL09u23y+vXwLFjwH//aT+mYEGZSJw5I2dnWrECmDAB6NoVMNPx0/Wvv2TsWbMCTZoIqFu6y5eX51GnDvDgAfDokdzn8uUAcwoiIkqK9E4sALkg3tREnuPw6dOn+PDhA54+fYrIyEhcuXIFAJA7d27Y2tqiVq1acHNzwy+//IIZM2bg1atXGDNmDPr06aNpcejZsycWLlyIYcOGoXPnzjh8+DA2bdqE3bt3J+q5ECUnZ88C9erJVofcuYH9+4GYGhKzZZO/vA8fLhdsi4gAMmQA0qfX/ps1q/6/6v+IpSWwYwdQsSJw8yZQuzZw4gSgy28dHz/KheYWL5ZTuMbFykpOi1uvHlC3LpA9u2zJ+OcfYNgw4O5dua7EggXAzJkyKYgreZJdw+T1fv2A738UypNHJhcNG8rkZfjwhGmNISIiMoSfSiyOHz+OZcuW4eHDh9i8eTOyZMmCNWvWwNXVVe9xFroaO3YsVq9erbldrFgxAMCRI0dQpUoVmJqaYteuXejVqxfKli0LGxsbeHl5YeLEiZrHuLq6Yvfu3fD29sb8+fORNWtWrFixglPNEsVACPkL/MCBQHAwULIksGsX8P8GwVjlzCm7RiW2tGmBvXvlYnH+/vKL/4AB8ot43rzRv7RfvQosWgSsWyfPDwCUSjnFbcaM8pIhw7fr7u5A1aoyuYhKoQAaNZLHW7pUtljcuiWTj2bNZItEbK3IR47IaWatrYFu3WIukzEj8O+/cp9MKoiIKCnTO7HYunUrfvnlF7Rr1w6XLl3STMX6+fNnTJ06NVpXI0Px9fX94SrZ2bNn/+Hxq1SpgsuXLxswMqLkITAQ2LpVrgvh9oNx0k+fyu486m45tWsDmzfLsRVJmYsLsG+fbLk4exZo21Zut7AA8uf/lmQcOCBbNNTc3YE+fYB27X7+HM3NZavDL7/ILmG//Saf765dZYtITC0Xc+fKvx07ysQothn9zM2BIkV+Li4iIqLEovfK25MnT8bSpUvx+++/aw3mKF++PC5dumTQ4Igo/oQANm0CChQAOnWS4wJq1ZIDkFWq6GV//10OvPbzk12MZs+WLRVJPalQK1RI/sLfowdQrpzsdhUWJlsG1q4Fxo6VSYWZmRxA/u+/svWiRw/DnKODg+wGtXWrHAvx559yNfHv3bsnn1dAtqwQEREld3q3WNy5cweVKlWKtj1NmjT4pJ7uhYiShDt35K/o6paHTJnkwGQ/P3nJk0fe37GjHGvQrdu3QdflysmB2PnyGS38n+buLrslATJZevJEJhbXr8tuUnnyyHN1dk64GOrXlwOtu3QBpk2TXaz69/92//z538rlzZtwcRARESUWvRMLJycn3L9/Hzly5NDafuLECZ2nhiWihBUcDEyeDMyaJbvXKJVyKtThw+UMRwsXyvET9+7JL7tjxsgv4IGBspViyhT5K3pKmH1IoQBy5JCXhg0T99idO8tpeseMkWNVnJxkK8nHj8CqVbLMwIGJGxMREVFC0bsrVLdu3TBgwACcPXsWCoUCL168wLp16zBkyBD06tUrIWIkIh0FBwNr1shuTz4+MqmoW1fOlDR+vBx4nCOHTDiePZMJRt68cn2EwEA58PnKFWDQoJSRVCQFo0YBvXvLxO2XX+SA7RUr5GtVuLCcZYqIiCgl0LvFYsSIEVCpVKhevTqCg4NRqVIlKJVKDBkyBP369UuIGIkoDpGRwNGjMqHYuhUICpLbs2WT3W0aNYp54LCtrRyw3KuX7Bb1/j3QqhUTCkNTKORA7tev5evTuLGcBQqQrRWGWMuDiIgoKdA7sVAoFBg9ejSGDh2K+/fvIygoCG5ubrC1tUVISEisK1gTkWHdvCkHBq9bBzx//m17zpxykLa3N2Bj8+P9mJjIxe8o4agX8Xv7Vg4WDwiQ413atDF2ZERERIbzU+tYAICFhYVmRevQ0FDMmTNHszAdESWs5cvlLEZqDg6yteGXX+Sga/4KnvREXcTvxg3ZWmRpaeyoiIiIDEfnxCI0NBTjx4+Hn58fLCwsMGzYMDRu3BirVq3C6NGjYWpqCm9v74SMlYgg12fo21der1tXzjpUr54coE1Jm4MDcOyYXL28eXNjR0NERGRYOicWY8eOxbJly1CjRg2cOnUKLVq0QKdOnXDmzBnMmTMHLVq0gCk7ZxMlqLdv5RfS8HC5qvPmzWydSG7SpWMXKCIiSpl0Tiw2b96MP//8Ew0bNsSNGzdQuHBhRERE4OrVq1Dwmw1RgouMlCtJP3sm15ZYuZJJBRERESUdOk83++zZM5QoUQIAUKhQISiVSnh7ezOpIEokY8cCBw/KGYW2bgXs7Y0dEREREdE3OicWkZGRsLCw0Nw2MzODra1tggRFRNp27gSmTpXXV6wAChY0bjxERERE39O5K5QQAh07doTy/yNEv379ip49e8Lmu/kst23bZtgIiVK5Bw/kbE8A0K8f++cTERFR0qRzYuHl5aV1u3379gYPhoi0BQfLQdqfP8tVsWfNMnZERERERDHTObFYtWpVQsZBRN8RAujZE7h6FciYUc4AFaU3IhEREVGSovMYCyJKPEIAQ4YAa9bIlbE3bACyZDF2VERERESxY2JBlARNmADMmSOvr1gBVKtm3HiIiIiIfoSJBVESM3u2TCwAYP58oFMn48ZDREREpAsmFkRJyIoVCgwZIq9Pngz072/ceIiIiIh0xcSCKIk4diwr+vQxBQAMHw6MGmXkgIiIiIj0wMSCKAnYsUOB+fOLQQgF+vQBfHwALmpPREREyQkTCyIjevcOmDgRaNfOFCqVCdq3V+G335hUEBERUfKj8zoWRGQ49+8Dc+cCq1YBISEAoEDZsi+wfHlGmJgw3yciIqLkh4kFUSI6cwaYORPYvl2uVQEAxYsDAwdGwMbmPMzM6ho3QCIiIqKfxMSCKBF8+gS0aQPs2/dtW926chG8KlWAiAiBPXuMFR0RERFR/DGxINKTSiXXmQgLA8aMAWxs4i7/4gVQuzZw/TpgYQG0bw8MGgQULJg48RIRERElBiYWRHoaPRqYNk1e374d2LABKFo05rJ37wKensDjx4CTk2yxKFIksSIlIiIiSjwcJUqkh1WrviUVGTIAd+4ApUsDv/32bcyE2oULQIUKMqnInRs4dYpJBREREaVcTCwoxRBCtgisXw/899+Py79/L1sbunWTq1x//Rp3+WPHgB495PUxY4Dbt4FGjWSXqAEDgAYNgLdv5f1+fkDVqvJ2iRLAyZOAq2v8zo+IiIgoKWNXKEoR3r0DunYFduz4ts3VFahUSV4qVwZy5JCtCHv3ygTk3DntVoaNG4G//gIKFYq+/7t3gSZNgPBwoGVLOcbCxER2hVq8GBg8GNi9W7ZIdO8OTJ0qy9aoAWzbBtjZJfhTQERERGRUTCwo2Tt4EOjQAXj5EjA3B9zdgatXgUeP5GX1alnOwkK2LkTl7i5nZdq4EbhxA/DwAGbNAvr0+bZI3YcPQP36wMePstuTr69MKgBZpk8foGJFoHVrwN9fJh2AvL16tTwuERERUUrHrlCUbIWGyulaa9aUSUX+/MDZs8DFizIJ2LcPGDUKKF9eJhxhYUCaNECLFsAffwDPngHXrsnxEdeuAXXqyH326wc0bCi7MYWFAU2bAvfuAdmyyRYRK6vosRQuDJw/L7tVmZoCAwcC69YxqSAiIqLUgy0WlCzdvg20bQtcvixv9+wJzJ4NWFvL23Z2cjYmT095OyREDqLOkwcwi6HWOzrKrkwLFwJDhwK7dsnWDA8PObbCzk5uc3SMPSYbG2D5cmDBAkCpNOjpEhERESV5bLGgZOX9e9nVqHhxmVSkTw/8/TewZMm3pCImVlZAgQIxJxVqCoVsrTh3DnBzA16/lsmGiYnsKuXurluMTCqIiIgoNWJiQcnCs2dyUbns2YHx42ULRM2actG5Ro0Me6zCheUg7z59ZEvF4sWymxQRERERxY5doShJu30bmDEDWLtWzrIEyJmXRoyQszOZJFBqbGUlu0UtWPBtEDcRERERxY6JBSVJV64AkybJ6VzVU8JWriwTCk/PxPuyz6SCiIiISDdMLChJuXwZmDhRjptQa9QIGD4cKFvWaGERERER0Q8wsaAk4fJlOShbvcCdQgG0aiVXuC5Y0LixEREREdGPMbEgoxECOH1ajqGImlC0bi0TCjc348ZHRERERLpLNrNCTZkyBeXKlYO1tTUcHBxiLKNQKKJdNmzYoFXm6NGjKF68OJRKJXLnzg1fX9+ED560BAXJ9R6KFZOL1+3YIROKtm2BmzeB9euZVBARERElN8mmxSIsLAwtWrRA2bJl8ccff8RabtWqVahdu7bmdtQk5NGjR6hXrx569uyJdevW4dChQ+jatSsyZ84MT/VKapRgbt2S6038+ScQECC3WVoCbdoAw4bJlbOJiIiIKHlKNonFhAkTAOCHLQwODg5wcnKK8b6lS5fC1dUVs2fPBgAUKFAAJ06cwNy5c5lYJKDPn2XysHfvt2158gC9egFeXkC6dMaLjYiIiIgMI9kkFrrq06cPunbtipw5c6Jnz57o1KkTFP+fM/T06dOoUaOGVnlPT08MHDgw1v2FhoYiNDRUczvg/z+1h4eHI1y9sMJPUj8+vvtJTJ8+AevWmaBYMYFy5cQPy0dGAm3bmmLvXhOYmAjUry/Qs6cK1aoJzRoUyej0E0xyrAtkeKwHBLAe0DesCwQYvx7oc9wUlVhMnDgR1apVg7W1NQ4cOIDevXsjKCgI/fv3BwC8evUKjo6OWo9xdHREQEAAQkJCYGVlFW2fPj4+mtaSqA4cOABra2uDxO3n52eQ/SSkwEBz7NyZC7t25URwsCnMzCIxatR5FC/+Js7HrV1bAHv25IWFRSQmTz6BvHk/ISwM2LcvkQJPZpJDXaCEx3pAAOsBfcO6QIDx6kFwcLDOZY2aWIwYMQLTp0+Ps4y/vz/y69j5/tdff9VcL1asGL58+YKZM2dqEoufMXLkSAwaNEhzOyAgAC4uLqhVqxbs7e1/er+AzAD9/PxQs2ZNmJubx2tfCeX9e2DePBMsXmyCwEDZ8pM2rcDHj6aYMaMMduyIRNWqMbdcbNqkwJYtsootXy7Qtm25RIs7uUkOdYESHusBAawH9A3rAgHGrwfq3jq6MGpiMXjwYHTs2DHOMjlz5vzp/ZcuXRqTJk1CaGgolEolnJyc8Pr1a60yr1+/hr29fYytFQCgVCqhVCqjbTc3NzfYi2vIfRnK69fAvHnAwoVyFicAKFwYGDsWqF9fgRYtgJ07FWjSxAz79wMVKmg//vJloFs3eX3oUMDLK0U1jiWYpFgXKPGxHhDAekDfsC4QYLx6oM8xjfptL2PGjMiYMWOC7f/KlStImzatJjEoW7Ys9uzZo1XGz88PZbmkMwA5HuLAAWDFCuCff4CICLm9aFFg3DigYUNoxkVs3ixXxN6/H6hbFzh4EChVSt735g3QuDEQEgLUrg34+BjjbIiIiIgoMSWbn5GfPn2KDx8+4OnTp4iMjMSVK1cAALlz54atrS127tyJ169fo0yZMrC0tISfnx+mTp2KIUOGaPbRs2dPLFy4EMOGDUPnzp1x+PBhbNq0Cbt37zbSWSUNT58CK1fKy3//fdtepgwwciTQoIFcZyIqpRLYtg2oXx84cgTw9AQOH5arZDdvLveZNy/w11+AqWning8RERERJb5kk1iMHTsWq1ev1twuVqwYAODIkSOoUqUKzM3NsWjRInh7e0MIgdy5c2POnDnopu6PA8DV1RW7d++Gt7c35s+fj6xZs2LFihWpdqrZwECgY0dg+3a5CjYApE0L/PIL0LUr4O4e9+OtrWXLRu3awMmTQM2aQJUqwPHjgL29XPgulrUMiYiIiCiFSTaJha+vb5xrWNSuXVtrYbzYVKlSBZcvXzZgZMnXmDGy1QGQCUG3bkDTpnLROl3Z2gJ79gA1agDnzwNbt8rWjfXrueAdERERUWqSbBILMqwrV+TAbADYvVuOk/hZ9vZyrEW1anK/Pj5AvXqGiJKIiIiIkgsmFqmQSiVXvVapgJYt45dUqKVNC5w9Czx4ABQoEP/9EREREVHyYmLsACjxrVwJnDkjuzHNmWO4/VpYMKkgIiIiSq2YWKQy794Bw4fL6xMnAlmyGDceIiIiIkoZmFikMiNHAh8+yBmf+vUzdjRERERElFIwsUhFzpyRi98BwOLFgBlH2BARERGRgTCxSCUiIuSAbUCuXVGhglHDISIiIqIUholFKrF4sZwKNm1aYMYMY0dDRERERCkNE4tU4OVL4Ndf5XUfHyBjRuPGQ0REREQpDxOLVGDIECAgAChZEuja1djREBEREVFKxMQihTt8GFi/HlAoZHcoU1NjR0REREREKRETixQsLAzo3Vte790b8PAwbjxERERElHIxsUjBZs8G7twBMmUCJk82djRERERElJIxsUihHj8GJk2S12fPBhwcjBkNEREREaV0TCxSqAEDgJAQoHJloF07Y0dDRERERCkdE4sU6J9/5MXMTA7YViiMHRERERERpXRMLFKY4GCgf395ffBgwM3NuPEQERERUerAxCKFmTIFePIEcHH5tigeEREREVFCY2KRgty+DcycKa//9htgY2PceIiIiIgo9TAzdgAUf0IAz57JtSrCw4F69YBGjYwdFRERERGlJkwskpmQEODmTeDqVeDatW9/P36U91taytYKDtgmIiIiosTExCKZyZkTePUq+nYzMyB/fmD8eFmGiIiIiCgxMbFIZgoWBCIigCJF5KVwYfm3QAFAqTR2dERERESUWjGxSGZ27ACsrdnViYiIiIiSFiYWyQxneiIiIiKipIjTzRIRERERUbwxsSAiIiIionhjYkFERERERPHGxIKIiIiIiOKNiQUREREREcUbEwsiIiIiIoo3JhZERERERBRvTCyIiIiIiCjemFgQEREREVG8MbEgIiIiIqJ4MzN2AMmNEAIAEBAQEO99hYeHIzg4GAEBATA3N4/3/ij5Yl0ggPWAJNYDUmNdIMD49UD9nVf9HTguTCz0FBgYCABwcXExciRERERERIkjMDAQadKkibOMQuiSfpCGSqXCixcvYGdnB4VCEa99BQQEwMXFBf/99x/s7e0NFCElR6wLBLAekMR6QGqsCwQYvx4IIRAYGAhnZ2eYmMQ9ioItFnoyMTFB1qxZDbpPe3t7fmAQANYFklgPCGA9oG9YFwgwbj34UUuFGgdvExERERFRvDGxICIiIiKieGNiYURKpRLjxo2DUqk0dihkZKwLBLAekMR6QGqsCwQkr3rAwdtERERERBRvbLEgIiIiIqJ4Y2JBRERERETxxsSCiIiIiIjijYmFES1atAg5cuSApaUlSpcujXPnzhk7JEpAPj4+KFmyJOzs7JApUyY0btwYd+7c0Srz9etX9OnTB+nTp4etrS2aNWuG169fGyliSgzTpk2DQqHAwIEDNdtYD1KH58+fo3379kifPj2srKzg7u6OCxcuaO4XQmDs2LHInDkzrKysUKNGDdy7d8+IEVNCiIyMxK+//gpXV1dYWVkhV65cmDRpEqIOgWVdSHn+/fdfNGjQAM7OzlAoFPj777+17tflNf/w4QPatWsHe3t7ODg4oEuXLggKCkrEs4iOiYWRbNy4EYMGDcK4ceNw6dIlFClSBJ6ennjz5o2xQ6MEcuzYMfTp0wdnzpyBn58fwsPDUatWLXz58kVTxtvbGzt37sTmzZtx7NgxvHjxAk2bNjVi1JSQzp8/j2XLlqFw4cJa21kPUr6PHz+ifPnyMDc3x969e3Hr1i3Mnj0badOm1ZSZMWMGfvvtNyxduhRnz56FjY0NPD098fXrVyNGToY2ffp0LFmyBAsXLoS/vz+mT5+OGTNmYMGCBZoyrAspz5cvX1CkSBEsWrQoxvt1ec3btWuHmzdvws/PD7t27cK///6L7t27J9YpxEyQUZQqVUr06dNHczsyMlI4OzsLHx8fI0ZFienNmzcCgDh27JgQQohPnz4Jc3NzsXnzZk0Zf39/AUCcPn3aWGFSAgkMDBR58uQRfn5+onLlymLAgAFCCNaD1GL48OGiQoUKsd6vUqmEk5OTmDlzpmbbp0+fhFKpFH/99VdihEiJpF69eqJz585a25o2bSratWsnhGBdSA0AiO3bt2tu6/Ka37p1SwAQ58+f15TZu3evUCgU4vnz54kW+/fYYmEEYWFhuHjxImrUqKHZZmJigho1auD06dNGjIwS0+fPnwEA6dKlAwBcvHgR4eHhWvUif/78yJYtG+tFCtSnTx/Uq1dP6/UGWA9Si3/++QceHh5o0aIFMmXKhGLFiuH333/X3P/o0SO8evVKqx6kSZMGpUuXZj1IYcqVK4dDhw7h7t27AICrV6/ixIkTqFOnDgDWhdRIl9f89OnTcHBwgIeHh6ZMjRo1YGJigrNnzyZ6zGpmRjtyKvbu3TtERkbC0dFRa7ujoyNu375tpKgoMalUKgwcOBDly5dHoUKFAACvXr2ChYUFHBwctMo6Ojri1atXRoiSEsqGDRtw6dIlnD9/Ptp9rAepw8OHD7FkyRIMGjQIo0aNwvnz59G/f39YWFjAy8tL81rH9H+C9SBlGTFiBAICApA/f36YmpoiMjISU6ZMQbt27QCAdSEV0uU1f/XqFTJlyqR1v5mZGdKlS2fUesHEgsgI+vTpgxs3buDEiRPGDoUS2X///YcBAwbAz88PlpaWxg6HjESlUsHDwwNTp04FABQrVgw3btzA0qVL4eXlZeToKDFt2rQJ69atw/r161GwYEFcuXIFAwcOhLOzM+sCJTvsCmUEGTJkgKmpabRZXl6/fg0nJycjRUWJpW/fvti1axeOHDmCrFmzarY7OTkhLCwMnz590irPepGyXLx4EW/evEHx4sVhZmYGMzMzHDt2DL/99hvMzMzg6OjIepAKZM6cGW5ublrbChQogKdPnwKA5rXm/4mUb+jQoRgxYgRat24Nd3d3/PLLL/D29oaPjw8A1oXUSJfX3MnJKdqEPxEREfjw4YNR6wUTCyOwsLBAiRIlcOjQIc02lUqFQ4cOoWzZskaMjBKSEAJ9+/bF9u3bcfjwYbi6umrdX6JECZibm2vVizt37uDp06esFylI9erVcf36dVy5ckVz8fDwQLt27TTXWQ9SvvLly0ebbvru3bvInj07AMDV1RVOTk5a9SAgIABnz55lPUhhgoODYWKi/XXM1NQUKpUKAOtCaqTLa162bFl8+vQJFy9e1JQ5fPgwVCoVSpcunegxaxht2Hgqt2HDBqFUKoWvr6+4deuW6N69u3BwcBCvXr0ydmiUQHr16iXSpEkjjh49Kl6+fKm5BAcHa8r07NlTZMuWTRw+fFhcuHBBlC1bVpQtW9aIUVNiiDorlBCsB6nBuXPnhJmZmZgyZYq4d++eWLdunbC2thZr167VlJk2bZpwcHAQO3bsENeuXRONGjUSrq6uIiQkxIiRk6F5eXmJLFmyiF27dolHjx6Jbdu2iQwZMohhw4ZpyrAupDyBgYHi8uXL4vLlywKAmDNnjrh8+bJ48uSJEEK317x27dqiWLFi4uzZs+LEiRMiT548ok2bNsY6JSGEEEwsjGjBggUiW7ZswsLCQpQqVUqcOXPG2CFRAgIQ42XVqlWaMiEhIaJ3794ibdq0wtraWjRp0kS8fPnSeEFTovg+sWA9SB127twpChUqJJRKpcifP79Yvny51v0qlUr8+uuvwtHRUSiVSlG9enVx584dI0VLCSUgIEAMGDBAZMuWTVhaWoqcOXOK0aNHi9DQUE0Z1oWU58iRIzF+J/Dy8hJC6Paav3//XrRp00bY2toKe3t70alTJxEYGGiEs/lGIUSUpR2JiIiIiIh+AsdYEBERERFRvDGxICIiIiKieGNiQURERERE8cbEgoiIiIiI4o2JBRERERERxRsTCyIiIiIiijcmFkREREREFG9MLIiIiIiIKN6YWBARGdDjx4+hUChw5cqVBDtGx44d0bhxY83tKlWqYODAgQl2PAC4c+cOnJycEBgYmKDHSWwKhQJ///13oh0vMepHYrp16xayZs2KL1++GDsUIkoCmFgQEf1fx44doVAool1q166t8z5cXFzw8uVLFCpUKAEj1bZt2zZMmjQpQY8xcuRI9OvXD3Z2dgCAo0ePQqFQ4NOnTzrv4/uEiH7O169f0bFjR7i7u8PMzCzW5/To0aMoXrw4lEolcufODV9f32hlFi1ahBw5csDS0hKlS5fGuXPnoh2rT58+SJ8+PWxtbdGsWTO8fv1ac7+bmxvKlCmDOXPmGPIUiSiZYmJBRBRF7dq18fLlS63LX3/9pfPjTU1N4eTkBDMzswSMUlu6dOk0X/gTwtOnT7Fr1y507NgxwY6hj7CwMGOHYFSRkZGwsrJC//79UaNGjRjLPHr0CPXq1UPVqlVx5coVDBw4EF27dsX+/fs1ZTZu3IhBgwZh3LhxuHTpEooUKQJPT0+8efNGU8bb2xs7d+7E5s2bcezYMbx48QJNmzbVOlanTp2wZMkSREREJMwJE1GywcSCiCgKpVIJJycnrUvatGk19ysUCixZsgR16tSBlZUVcubMiS1btmju/76ry8ePH9GuXTtkzJgRVlZWyJMnD1atWqUpf/36dVSrVg1WVlZInz49unfvjqCgIM39kZGRGDRoEBwcHJA+fXoMGzYMQgitmL/vCvXx40d06NABadOmhbW1NerUqYN79+5p7n/y5AkaNGiAtGnTwsbGBgULFsSePXtifU42bdqEIkWKIEuWLLGW8fX1hYODA/bv348CBQrA1tZWk6QBwPjx47F69Wrs2LFD0xJ09OhRAMB///2Hli1bwsHBAenSpUOjRo3w+PFjzb7VLR1TpkyBs7Mz8uXLh1GjRqF06dLR4ihSpAgmTpwIADh//jxq1qyJDBkyIE2aNKhcuTIuXboU6zkAP349jh49ilKlSsHGxgYODg4oX748njx5Euv+zp07h2LFisHS0hIeHh64fPlytDI3btxAnTp1YGtrC0dHR/zyyy949+5drPu0sbHBkiVL0K1bNzg5OcVYZunSpXB1dcXs2bNRoEAB9O3bF82bN8fcuXM1ZebMmYNu3bqhU6dOcHNzw9KlS2FtbY2VK1cCAD5//ow//vgDc+bMQbVq1VCiRAmsWrUKp06dwpkzZzT7qVmzJj58+IBjx47F/sQSUarAxIKISE+//vormjVrhqtXr6Jdu3Zo3bo1/P39Yy1769Yt7N27F/7+/liyZAkyZMgAAPjy5Qs8PT2RNm1anD9/Hps3b8bBgwfRt29fzeNnz54NX19frFy5EidOnMCHDx+wffv2OOPr2LEjLly4gH/++QenT5+GEAJ169ZFeHg4AKBPnz4IDQ3Fv//+i+vXr2P69OmwtbWNdX/Hjx+Hh4fHD5+X4OBgzJo1C2vWrMG///6Lp0+fYsiQIQCAIUOGoGXLllotQuXKlUN4eDg8PT1hZ2eH48eP4+TJk5qkJGrLxKFDh3Dnzh34+flh165daNeuHc6dO4cHDx5oyty8eRPXrl1D27ZtAQCBgf9r7/5joq7/OIA/Qf0cB8jpASJlwrQLsAwQ04gt1g9iudjJPzW78GR1BVagtNjoD3VZrrW2co5ausmaYSor+mWTmjs35Q6rI7gUPCBIzMCGIOMS8YDn94/GZ3yAO1L2zdLXY/v8ce/36/P+vD/vz2fjXrw/7/sMwGq14sSJE6irq4PJZMKaNWv8rhOZ7noMDw9j7dq1yMzMhNvthtPpxPPPP4+goKAp2/N6vXjiiSewbNkyuFwubNu2TR2PMZcuXcLDDz+M1NRU/Pjjjzhy5AguXLiAJ598ctrxDsTpdE6azcjOzobT6QTw16yPy+XSxAQHB+PRRx9VY1wuF3w+nyYmMTERixcvVmMAQFEUpKSk4Pjx4zPqsxDiJkAhhBAkSavVylmzZjEsLEyzvfnmm2oMABYUFGj2W716NQsLC0mSHR0dBMCffvqJJJmTk8P8/Pwpj7d7927Onz+fXq9XLTt8+DCDg4PZ3d1NkoyNjeXbb7+t1vt8Pi5atIhms1kty8zMZHFxMUmypaWFAFhbW6vW9/T0UK/X89ChQyTJ5cuXc9u2bX97XJKTk/n6669ryux2OwGwr6+PJFlRUUEAbGtrU2PKy8sZExOjfrZarZp+k+S+ffuYkJDA0dFRtWxoaIh6vZ41NTXqfjExMRwaGgrYr7KyMq5evdrveYyMjHDu3Ln86quv1DIArK6uJjn99bh48SIB8NixY36PMd6HH37IyMhIDg4OqmUffPCB5v7Yvn07H3vsMc1+586dIwB6PJ5pjzHVmJKkyWTijh07NGWHDx8mAF6+fJnnz58nADocDk3Mq6++ylWrVpEkKysrqSjKpLbvu+8+lpaWaspyc3O5YcOGafsrhLi5yYyFEEKMM/ZM+vitoKBAE5Oenj7ps78Zi8LCQhw4cAApKSkoLS2Fw+FQ65qbm5GcnIywsDC1LCMjA6Ojo/B4POjv70dXV5fmkZ/Zs2cHnD1obm7G7NmzNftERkYiISFB7WNRURHeeOMNZGRkYOvWrXC73QHHZHBwECEhIQFjACA0NBRLly5VP8fGxmqe159KY2Mj2traMHfuXISHhyM8PBxGoxFXrlzRzEYsX74ciqJo9rVYLNi/fz8AgCQ++eQTWCwWtf7ChQuw2WwwmUwwGAyIiIiA1+tFZ2fnlH2Z7noYjUZs2LAB2dnZyMnJwc6dO9VHvfy1d++992rGbuK909jYCLvdrp57eHg4EhMTAUBz/v92er0ely9fvtHdEELcYJJYCCHEOGFhYbjzzjs1m9FovO72Hn/8cZw9exabN2/G77//jkceeWTS4zD/tOeeew7t7e3Iy8vDzz//jJUrV2LXrl1+46OiotDX1zdtu3PmzNF8DgoKmrQeZCKv14u0tLRJyVxLS4v6SBMAzZf9MevWrYPH40F9fT0cDgfOnTuHp556Sq23Wq1oaGjAzp074XA40NDQgMjIyBkt/q6oqIDT6cQDDzyAgwcP4q677tKsN7hWXq8XOTk5k86/tbUVDz744HW3u3DhQs2vNwF/JVoRERHQ6/WIiorCrFmzpowZW7excOFCXL16ddIvf42PGdPb24vo6Ojr7q8Q4uYgiYUQQlyjiV8k6+rqkJSU5Dc+OjoaVqsVH3/8Md577z3s3r0bAJCUlITGxkbNOwBqa2sRHByMhIQEGAwGxMbG4uTJk2r98PAwXC6X32MlJSVheHhYs8/Fixfh8XiwbNkyteyOO+5AQUEBPvvsM7zyyivYs2eP3zZTU1PR1NTkt/7vUhQFIyMjmrIVK1agtbUVCxYsmJTQGQyGgO0tWrQImZmZqKysRGVlJbKysrBgwQK1vra2FkVFRVizZg3uvvtu6HS6gIuip7seY1JTU1FWVgaHw4F77rlHnTWZqj23240rV66oZRPvnRUrVuD06dOIj4+fdP5TJVN/V3p6Oo4ePaop++6779QZE0VRkJaWpokZHR3F0aNH1Zi0tDTMmTNHE+PxeNDZ2Tlp5uXUqVNITU297v4KIW4OklgIIcQ4Q0ND6O7u1mwTv4xWVVVh7969aGlpwdatW/H9999rFlyPt2XLFnzxxRdoa2vD6dOn8fXXX6tJiMViQUhICKxWK06dOgW73Y6XX34ZeXl5iImJAQAUFxfjrbfewueff44zZ85g48aNAd8dYTKZYDabYbPZcOLECTQ2NuKZZ57B7bffDrPZDADYtGkTampq0NHRgfr6etjt9oCJ0dii34lJwbWKj4+H2+2Gx+NBT08PfD4fLBYLoqKiYDabcfz4cXR0dODYsWMoKirCb7/9Nm2bFosFBw4cQFVVleYxqLGx2LdvH5qbm3Hy5ElYLBbo9fqAbQW6Hh0dHSgrK4PT6cTZs2fx7bfforW11e/YPf300wgKCoLNZkNTUxO++eYbvPPOO5qYF198Eb29vVi3bh1++OEH/PLLL6ipqUF+fn7A8W5qakJDQwN6e3vR39+vznSMKSgoQHt7O0pLS3HmzBm8//77OHToEDZv3qzGlJSUYM+ePfjoo4/Q3NyMwsJC/Pnnn8jPzwcAGAwGPPvssygpKYHdbofL5UJ+fj7S09Nx//33q+38+uuvOH/+vN+fvhVC3EJu9CIPIYT4t7BarQQwaUtISFBjALC8vJxZWVnU6XSMj4/nwYMH1fqJi7e3b9/OpKQk6vV6Go1Gms1mtre3q/Fut5sPPfQQQ0JCaDQaabPZODAwoNb7fD4WFxczIiKC8+bNY0lJCdevX+938TZJ9vb2Mi8vjwaDgXq9ntnZ2WxpaVHrX3rpJS5dupQ6nY7R0dHMy8tjT0+P33Hx+Xy87bbbeOTIEbVsqsXbBoNBs191dTXH/5n5448/mJWVxfDwcAKg3W4nSXZ1dXH9+vWMioqiTqfjkiVLaLPZ2N/fr16XqRYok2RfXx91Oh1DQ0M140aS9fX1XLlyJUNCQmgymVhVVcW4uDi+++67agzGLd4mA1+P7u5url27lrGxsVQUhXFxcdyyZQtHRkb8jp3T6WRycjIVRWFKSgo//fRTzf1B/rXgPjc3l/PmzaNer2diYiI3bdqkWdA+UVxc3JT36nh2u50pKSlUFIVLlixhRUXFpHZ27drFxYsXU1EUrlq1inV1dZr6wcFBbty4kfPnz2doaChzc3PZ1dWlidmxYwezs7P99lUIcesIIqd5AFYIIYQqKCgI1dXVt9wbpMvLy/Hll19qXrAmxNWrV2EymbB//35kZGTc6O4IIW6wf+7VsEIIIf6zXnjhBVy6dAkDAwP/17d8i/+Wzs5OvPbaa5JUCCEAADJjIYQQ1+BWnbEQQgghpiMzFkIIcQ3kfzFCCCHE1ORXoYQQQgghhBAzJomFEEIIIYQQYsYksRBCCCGEEELMmCQWQgghhBBCiBmTxEIIIYQQQggxY5JYCCGEEEIIIWZMEgshhBBCCCHEjEliIYQQQgghhJgxSSyEEEIIIYQQM/Y/lzAgCwinzjUAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Datos de recompensa promedio a lo largo de los episodios\n",
    "episodios_ajustados = list(range(1, 101))\n",
    "\n",
    "\n",
    "# Crear la gráfica\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(episodios_ajustados, promedio_recompensa, label='Recompensa Promedio', color='blue')\n",
    "\n",
    "# Añadir etiquetas y título\n",
    "plt.xlabel('Episodios (Intervalos de 1000)')\n",
    "plt.ylabel('Recompensa Promedio')\n",
    "plt.title('Evolución de la Recompensa Promedio por Intervalos de 1000 Episodios')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "# Mostrar la gráfica\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3825b32b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Fase inicial (Episodios 1 - 20.000):\n",
    "En los primeros 20.000 episodios, el agente tiene un desempeño consistentemente bajo, con recompensas promedio que oscilan entre -176 y -122. Esto es típico al inicio del entrenamiento, cuando el agente está explorando y cometiendo errores mientras aprende las dinámicas del entorno. Aquí vemos que el valor de la recompensa promedio mejora lentamente, pero sigue siendo negativo.\n",
    "\n",
    "### Fase de estabilización (Episodios 20.000 - 40.000):\n",
    "Entre los episodios 20,000 y 40,000, el agente muestra una tendencia hacia la estabilización. La recompensa promedio continúa mejorando, y se aproxima a -100 en varios puntos, pero aún se registran caídas, como los episodios en torno a los 35,000, donde la recompensa promedio disminuye brevemente. Esto puede indicar que el agente sigue cometiendo errores significativos, pero empieza a aprender estrategias básicas.\n",
    "\n",
    "### Mejora sostenida (Episodios 40.000 - 60.000):\n",
    "A partir de los 40.000 episodios, se observa una mejora más pronunciada en la recompensa promedio. El agente pasa de recompensas alrededor de -100 a valores cercanos a 0 cuando se acerca a los 60.000. Esto sugiere que el agente ha comenzado a entender mejor cómo aterrizar con éxito en el entorno LunarLander. La recompensa promedio incluso llega a valores positivos por primera vez alrededor de los 57.000 episodios, lo que indica un progreso significativo en el aprendizaje.\n",
    "\n",
    "### Fase de alto rendimiento (Episodios 60.000 - 100.000):\n",
    "A partir de los 60.000 episodios, el agente experimenta un incremento continuo en su desempeño. Las recompensas promedio se mantienen en su mayoría positivas, llegando a valores entre 50 y 60 hacia el final del entrenamiento. Este periodo muestra que el agente ha alcanzado un nivel considerable de competencia, aunque sigue habiendo fluctuaciones, posiblemente debido a exploraciones que el agente realiza esporádicamente para optimizar su política de acciones.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac3d75d",
   "metadata": {},
   "source": [
    "## Análisis de casos (Éxitos y Fallos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08c618c",
   "metadata": {},
   "source": [
    "#### Funcion para reproducir un episodio individual de la prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed685b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para reproducir un episodio específico\n",
    "def reproducir_episodio(agente, episodio, semilla, max_iteraciones=500):\n",
    "    entorno = gym.make('LunarLander-v2', render_mode='human').env\n",
    "    entorno.reset(seed=semilla)\n",
    "    iteraciones = 0\n",
    "    termino = False\n",
    "    truncado = False\n",
    "    estado_anterior, info = entorno.reset(seed=semilla)\n",
    "    while iteraciones < max_iteraciones and not termino and not truncado:\n",
    "        accion = episodio[iteraciones][1]\n",
    "        estado_siguiente, recompensa, termino, truncado, info = entorno.step(accion)\n",
    "        entorno.render()\n",
    "        estado_anterior = estado_siguiente\n",
    "        iteraciones += 1\n",
    "    entorno.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbccc3e1",
   "metadata": {},
   "source": [
    "### Reproducir episodios exitosos y no exitosos de los casos de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e7768b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducir un episodio específico\n",
    "episodio_especifico = 30  # Cambia este valor al episodio que quieras reproducir\n",
    "if episodio_especifico < len(episodios_exitosos):\n",
    "    reproducir_episodio(agente, episodios_exitosos[episodio_especifico], semillas_exitosas[episodio_especifico])\n",
    "else:\n",
    "    print(f\"No hay suficientes episodios exitosos para reproducir el episodio {episodio_especifico}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "addc484e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducir un episodio específico\n",
    "episodio_especifico = 10  # Cambia este valor al episodio que quieras reproducir\n",
    "if episodio_especifico < len(episodios_no_exitosos):\n",
    "    reproducir_episodio(agente, episodios_no_exitosos[episodio_especifico], semillas_no_exitosas[episodio_especifico])\n",
    "else:\n",
    "    print(f\"No hay suficientes episodios no exitosos para reproducir el episodio {episodio_especifico}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a63f6a",
   "metadata": {},
   "source": [
    "### Episodios Exitosos y no exitosos\n",
    "\n",
    "La indeterminación del entorno en el problema del Lunar Lander proviene de la discretización de los estados. Dado que un mismo estado discretizado puede llevar a diferentes resultados debido a pequeñas variaciones que no se capturan en la discretización, el comportamiento del agente no siempre es predecible. El agente ha logrado aprender y adaptarse de manera efectiva en un (porcentaje) de los 1,000 episodios de prueba.\n",
    "\n",
    "#### Episodios Exitosos\n",
    "\n",
    "En los casos exitosos, el agente ha demostrado un control eficiente del propulsor principal, asegurándose de que la nave nunca descienda a una velocidad demasiado alta. Este control es fundamental para permitir que el agente pueda ajustar su orientación horizontal hacia la zona de aterrizaje. Desde el inicio, el agente toma decisiones que mantienen un equilibrio entre el uso del propulsor principal y los propulsores laterales, lo que le permite encadenar acciones efectivas que lo llevan a una recompensa exitosa.\n",
    "\n",
    "Un patrón clave en los episodios exitosos es que, al evitar el uso excesivo de los propulsores laterales en momentos inadecuados, el agente es capaz de mantener una trayectoria controlada hacia la meta. Esto es particularmente importante en las primeras etapas del episodio, donde las decisiones erróneas tienen un mayor impacto negativo en el aterrizaje. Al ejecutar correctamente los propulsores en los momentos críticos, el agente asegura un aterrizaje seguro y controlado.\n",
    "\n",
    "\n",
    "\n",
    "#### Episodios No Exitosos\n",
    "\n",
    "Uno de los principales problemas observados en los casos fallidos es que el agente es muy vulnerable durante la fase de aterrizaje. Si una de las patas de la nave se dobla o toca el suelo de manera incorrecta, el agente no puede recuperar el control, lo que conduce a una falla inmediata. Esta vulnerabilidad también se observa en su sensibilidad a la velocidad y el ángulo de inclinación. Aunque el agente llega muchas veces a una posición correcta para el aterrizaje, una decisión tardía o incorrecta de inclinarse hacia un lado debido a la velocidad provoca que pierda estabilidad y termine fuera de control.\n",
    "\n",
    "Estos errores indican que el agente tiene dificultades para manejar situaciones críticas en los últimos momentos del aterrizaje, donde la precisión y el control fino son esenciales. Una pequeña desviación en velocidad o ángulo es suficiente para desestabilizar completamente el aterrizaje, sin oportunidad de recuperación.\n",
    "\n",
    "\n",
    "#### Conclusiones sobre el Éxito y el Fracaso\n",
    "\n",
    "Podemos concluir que el éxito de un episodio está altamente influenciado por la capacidad del agente para tomar decisiones acertadas durante todo el proceso, y no solo al principio. Aunque las decisiones iniciales son importantes, el mayor desafío se presenta al final del episodio, cuando el agente debe ser capaz de ajustar con precisión su velocidad y ángulo para lograr un aterrizaje exitoso.\n",
    "\n",
    "El agente es particularmente sensible a las correcciones después de tomar una mala decisión, y su incapacidad para recuperarse adecuadamente de estas puede llevar rápidamente al fracaso. La limitación de no poder activar más de un propulsor a la vez y el uso de un espacio discretizado para las decisiones agrava este problema, haciendo que pequeñas desviaciones se tornen críticas.\n",
    "\n",
    "La clave del éxito radica en que el agente logre recuperarse eficazmente ante situaciones desfavorables y ajuste de manera precisa sus movimientos en los momentos críticos, especialmente al acercarse al aterrizaje. Esto maximiza sus probabilidades de éxito, minimizando las penalizaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f7d407",
   "metadata": {},
   "source": [
    "## Limitantes del agente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93fe904-e691-42ff-8fd4-b360d08431cc",
   "metadata": {},
   "source": [
    "### Capacidad del agente a tomar una única acción a la vez\n",
    "\n",
    "En el entorno Lunar Lander, solo se permite realizar una acción a la vez, es decir, no tenemos la posibilidad de propulsarnos en más de una dirección a la vez, lo que podría estar produciendo la poca capacidad del agente a recuperarse luego de inclinarse por demás hacia un lado.\n",
    "\n",
    "En este problema, esto no tendría solución ya que es parte del entorno establecido."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e2d1ba",
   "metadata": {},
   "source": [
    "### Discretización de estados\n",
    "\n",
    "Al discretizar un espacio de estados continuo (como la posición, velocidad o ángulo en Lunar Lander), se simplifican los valores en \"bins\" o rangos discretos, lo que implica que se pierde información importante. Dos estados que podrían ser distintos en el espacio continuo pueden terminar representados como el mismo estado discreto, lo que reduce la precisión del aprendizaje.\n",
    "\n",
    "Además, a medida que aumenta el número de dimensiones (variables como posición, velocidad, ángulo, etc.), discretizar cada una puede llevar a un crecimiento exponencial del espacio de estados, donde el número de combinaciones posibles de estados se dispara, haciendo que el aprendizaje sea ineficiente y más difícil de escalar.\n",
    "\n",
    "#### Posible mejora\n",
    "\n",
    "En lugar de utilizar una tabla Q donde cada estado es discretizado, se pueden emplear otros métodos que nos permitan trabajar directamente con el espacio continuo, ya que pueden aproximar valores Q para cualquier estado en lugar de depender de estados discretizados fijos. Esto mejora la capacidad del agente para generalizar y aprender políticas más suaves en un espacio continuo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1454be23",
   "metadata": {},
   "source": [
    "### Escasez de retroalimentación inmediata\n",
    "\n",
    "En Lunar Lander, la recompensa principal llega solo al final del episodio (aterrizaje exitoso o colisión). Esto dificulta que el agente aprenda qué acciones tempranas contribuyen a un aterrizaje exitoso.\n",
    "\n",
    "#### Posible mejora\n",
    "\n",
    "Se podría adaptar la funcion de recompensa del problema y agregar mayores recomensas por buenos movimientos al comienzo del episodio. Lo que tiene esto de contraparte es que estamos imponiendole al agente movimientos que nosotros creemos correctos y pueden no serlo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875478a0",
   "metadata": {},
   "source": [
    "### Conclusión"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7095f89",
   "metadata": {},
   "source": [
    "El desarrollo de este laboratorio con Q-learning en el entorno Lunar Lander nos permitió profundizar en el tema de aprendizaje por refuerzos. A través de la implementación de estrategias y las decisiones que tomamos, aprendimos la importancia de equilibrar la exploración y la explotación para optimizar el aprendizaje del agente.\n",
    "\n",
    "El agente, aunque inicialmente carecía de información sobre el entorno, mejoró su capacidad de aterrizaje con el tiempo, alcanzando un 35% de aciertos en 1000 episodios. A lo largo del proceso, fue clave entender cómo el entorno no determinista afecta el rendimiento y cómo las malas decisiones del agente tienen un impacto duradero en los resultados.\n",
    "\n",
    "Este ejercicio destacó la importancia de ajustar las estrategias de aprendizaje para adaptarse a las particularidades del problema, como la naturaleza continua del espacio de estados.\n",
    "\n",
    "En conclusión creemos haber obtenido un buen resultado a pesar de la complejidad del problema y las decisiones tomadas para simplificarlo como por ejemplo, la discretización de los estados. Además, entendimos la influencia de los parámetros elegidos sobre los resultados y cómo debemos adaptar estos al problema específico que queremos resolver."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
