{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d13bbe5-80d1-4879-bc47-d8e366f38456",
   "metadata": {},
   "source": [
    "# **Lunar Lander con Q-Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82127016-b75d-4621-a53e-8b2bb63cd3f8",
   "metadata": {},
   "source": [
    "### **1. Bibliotecas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6e0a19d-696b-440e-84eb-fe2b8761d955",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# Tal vez tengan que ejecutar lo siguiente en sus máquinas (ubuntu 20.04)\n",
    "# sudo apt-get remove swig\n",
    "# sudo apt-get install swig3.0\n",
    "# sudo ln -s /usr/bin/swig3.0 /usr/bin/swig\n",
    "# En windows tambien puede ser necesario MSVC++\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9831e9d4-7840-485c-bc38-af987a76de4f",
   "metadata": {},
   "source": [
    "### **2. Jugando a mano**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e744255-5e3f-4c17-b9dc-c6374c2f06c2",
   "metadata": {},
   "source": [
    "A continuación se puede jugar un episodio del lunar lander. Se controlan los motores con el teclado. Notar que solo se puede realizar una acción a la vez (que es parte del problema), y que en esta implementación, izq toma precedencia sobre derecha, que toma precedencia sobre el motor principal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6211ed30-b1a3-4b8e-9858-17eee433ddca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nota: hay que transformar esta celda en código para ejecutar (Esc + y)\n",
    "\n",
    "import pygame\n",
    "from pygame.locals import *\n",
    "\n",
    "# Inicializar pygame (para el control con el teclado) y el ambiente\n",
    "pygame.init()\n",
    "env = gym.make('LunarLander-v2', render_mode='human')\n",
    "env.reset()\n",
    "pygame.display.set_caption('Lunar Lander')\n",
    "\n",
    "clock = pygame.time.Clock()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    for event in pygame.event.get():\n",
    "        if event.type == QUIT:\n",
    "            done = True\n",
    "            break\n",
    "\n",
    "    keys = pygame.key.get_pressed()\n",
    "\n",
    "    # Map keys to actions\n",
    "    if keys[K_LEFT]:\n",
    "        action = 3  # Fire left orientation engine\n",
    "    elif keys[K_RIGHT]:\n",
    "        action = 1 # Fire right orientation engine\n",
    "    elif keys[K_UP]:\n",
    "        action = 2  # Fire main engine\n",
    "    else:\n",
    "        action = 0  # Do nothing\n",
    "\n",
    "    _, _, terminated, truncated, _ = env.step(action)\n",
    "    env.render()\n",
    "    clock.tick(10)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        done = True\n",
    "\n",
    "env.close()\n",
    "pygame.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d721a629-7f3f-4a70-83c0-e12f43b0f285",
   "metadata": {},
   "source": [
    "## **3. Discretizando el estado**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d35189-3a15-4ceb-b978-38226518092a",
   "metadata": {},
   "source": [
    "El estado consiste de posiciones y velocidades en (x,y,theta) y en información de contacto de los pies con la superficie.\n",
    "\n",
    "Como varios de estos son continuos, tenemos que discretizarlos para aplicar nuestro algoritmo de aprendizaje por refuerzo tabular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7b6bed3-12ba-4d92-8b09-3d3f8429217f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High:  [1.5       1.5       5.        5.        3.1415927 5.        1.\n",
      " 1.       ]\n",
      "Low:  [-1.5        0.        -5.        -5.        -3.1415927 -5.\n",
      " -0.        -0.       ]\n"
     ]
    }
   ],
   "source": [
    "# Cuántos bins queremos por dimensión\n",
    "# Pueden considerar variar este parámetro\n",
    "bins_per_dim = 15\n",
    "\n",
    "# Estado:\n",
    "# (x, y, x_vel, y_vel, theta, theta_vel, pie_izq_en_contacto, pie_derecho_en_contacto)\n",
    "NUM_BINS = [bins_per_dim, bins_per_dim, bins_per_dim, bins_per_dim, bins_per_dim, bins_per_dim, 2, 2]\n",
    "\n",
    "env = gym.make('LunarLander-v2')\n",
    "env.reset()\n",
    "\n",
    "# Tomamos los rangos del env\n",
    "OBS_SPACE_HIGH = env.observation_space.high\n",
    "OBS_SPACE_LOW = env.observation_space.low\n",
    "OBS_SPACE_LOW[1] = 0 # Para la coordenada y (altura), no podemos ir más abajo que la zona dea aterrizae (que está en el 0, 0)\n",
    "\n",
    "# Los bins para cada dimensión\n",
    "bins = [\n",
    "    np.linspace(OBS_SPACE_LOW[i], OBS_SPACE_HIGH[i], NUM_BINS[i] - 1)\n",
    "    for i in range(len(NUM_BINS) - 2) # last two are binary\n",
    "]\n",
    "# Se recomienda observar los bins para entender su estructura\n",
    "#print (\"Bins: \", bins)\n",
    "print(\"High: \", OBS_SPACE_HIGH)\n",
    "print(\"Low: \", OBS_SPACE_LOW)\n",
    "def discretize_state(state, bins):\n",
    "    \"\"\"Discretize the continuous state into a tuple of discrete indices.\"\"\"\n",
    "    state_disc = list()\n",
    "    for i in range(len(state)):\n",
    "        if i >= len(bins):  # For binary features (leg contacts)\n",
    "            state_disc.append(int(state[i]))\n",
    "        else:\n",
    "            state_disc.append(\n",
    "                np.digitize(state[i], bins[i])\n",
    "            )\n",
    "    return tuple(state_disc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45bcc921-c5f6-4f06-9702-6e740b26fc6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(np.int64(7), np.int64(1), np.int64(7), np.int64(7), np.int64(7), np.int64(7), 1, 1)\n",
      "(np.int64(7), np.int64(14), np.int64(7), np.int64(7), np.int64(7), np.int64(7), 0, 0)\n"
     ]
    }
   ],
   "source": [
    "# Ejemplos\n",
    "print(discretize_state([0.0, 0.0, 0, 0, 0, 0, 1, 1], bins)) # En la zona de aterrizaje y quieto\n",
    "print(discretize_state([0, 1.5, 0, 0, 0, 0, 0, 0], bins)) # Comenzando la partida, arriba y en el centro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779c1576-eff2-4b3c-8069-f6d30243f1e6",
   "metadata": {},
   "source": [
    "## **4. Agentes y la interacción con el entorno**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d594f89c-70b1-4c7c-939f-d654a5263f1c",
   "metadata": {},
   "source": [
    "Vamos a definir una interfaz para nuestro agente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93e7059a-7b68-4b13-857b-16e96c5f9793",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agente:\n",
    "    def elegir_accion(self, estado, max_accion, explorar = True) -> int:\n",
    "        \"\"\"Elegir la accion a tomar en el estado actual y el espacio de acciones\n",
    "            - estado_anterior: el estado desde que se empezó\n",
    "            - estado_siguiente: el estado al que se llegó\n",
    "            - accion: la acción que llevo al agente desde estado_anterior a estado_siguiente\n",
    "            - recompensa: la recompensa recibida en la transicion\n",
    "            - terminado: si el episodio terminó\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def aprender(self, estado_anterior, estado_siguiente, accion, recompensa, terminado):\n",
    "        \"\"\"Aprender a partir de la tupla \n",
    "            - estado_anterior: el estado desde que se empezó\n",
    "            - estado_siguiente: el estado al que se llegó\n",
    "            - accion: la acción que llevo al agente desde estado_anterior a estado_siguiente\n",
    "            - recompensa: la recompensa recibida en la transicion\n",
    "            - terminado: si el episodio terminó en esta transición\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def fin_episodio(self):\n",
    "        \"\"\"Actualizar estructuras al final de un episodio\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c61e38-f7d3-40bf-af19-c7fefd143ffd",
   "metadata": {},
   "source": [
    "Para un agente aleatorio, la implementación sería:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3adcedd-b300-4e30-9cb1-19d5ec96fb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class AgenteAleatorio(Agente):\n",
    "    def elegir_accion(self, estado, max_accion, explorar = True) -> int:\n",
    "        # Elige una acción al azar\n",
    "        return random.randrange(max_accion)\n",
    "\n",
    "    def aprender(self, estado_anterior, estado_siguiente, accion, recompensa, terminado):\n",
    "        # No aprende\n",
    "        pass\n",
    "\n",
    "    def fin_episodio(self):\n",
    "        # Nada que actualizar\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19498b0d-eb74-431e-ac7f-612497ca07f3",
   "metadata": {},
   "source": [
    "Luego podemos definir una función para ejecutar un episodio con un agente dado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "038e9f56-f0cf-40b7-b3c3-a63b34572bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ejecutar_episodio(agente, aprender = True, render = None, max_iteraciones=500):\n",
    "    entorno = gym.make('LunarLander-v2', render_mode=render).env\n",
    "    \n",
    "    iteraciones = 0\n",
    "    recompensa_total = 0\n",
    "\n",
    "    termino = False\n",
    "    truncado = False\n",
    "    estado_anterior, info = entorno.reset()\n",
    "    while iteraciones < max_iteraciones and not termino and not truncado:\n",
    "        # Le pedimos al agente que elija entre las posibles acciones (0..entorno.action_space.n)\n",
    "        accion = agente.elegir_accion(estado_anterior, entorno.action_space.n, aprender)\n",
    "        # Realizamos la accion\n",
    "        estado_siguiente, recompensa, termino, truncado, info = entorno.step(accion)\n",
    "        # Le informamos al agente para que aprenda\n",
    "        if (aprender):\n",
    "            agente.aprender(estado_anterior, estado_siguiente, accion, recompensa, termino)\n",
    "\n",
    "        estado_anterior = estado_siguiente\n",
    "        iteraciones += 1\n",
    "        recompensa_total += recompensa\n",
    "    if (aprender):\n",
    "        agente.fin_episodio()\n",
    "    entorno.close()\n",
    "    return recompensa_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a104779-f5e3-4bfa-b88b-fb44de195d49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(-104.25733799682678)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nota: hay que transformar esta celda en código para ejecutar (Esc + y)\n",
    "\n",
    "# Ejecutamos un episodio con el agente aleatorio y modo render 'human', para poder verlo\n",
    "ejecutar_episodio(AgenteAleatorio(), render = 'human')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b9cb2a-37b0-4115-afe8-bb89ce49f605",
   "metadata": {},
   "source": [
    "Podemos ejecutar este ambiente muchas veces y tomar métricas al respecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "536e81ee-6038-44c7-b887-35db442815ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasa de éxito: 0.0. Se obtuvo -169.90152325832003 de recompensa, en promedio\n"
     ]
    }
   ],
   "source": [
    "agente = AgenteAleatorio()\n",
    "recompensa_episodios = []\n",
    "\n",
    "exitos = 0\n",
    "num_episodios = 100\n",
    "for i in range(num_episodios):\n",
    "    recompensa = ejecutar_episodio(agente)\n",
    "    # Los episodios se consideran exitosos si se obutvo 200 o más de recompensa total\n",
    "    if (recompensa >= 200):\n",
    "        exitos += 1\n",
    "    recompensa_episodios += [recompensa]\n",
    "\n",
    "import numpy\n",
    "print(f\"Tasa de éxito: {exitos / num_episodios}. Se obtuvo {numpy.mean(recompensa_episodios)} de recompensa, en promedio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086cb250-7bc9-4bd4-a4cd-43cfd561facb",
   "metadata": {},
   "source": [
    "### **5. Programando un agente que aprende**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0827e3e2-a84a-462f-8b86-e03425cfc645",
   "metadata": {},
   "source": [
    "La tarea a realizar consiste en programar un agente de aprendizaje por refuerzos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3f37011-ffaa-4d08-b832-c45d0b9060da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "class AgenteRL(Agente):\n",
    "    # Agregar código aqui\n",
    "\n",
    "    # Pueden agregar parámetros al constructor\n",
    "    def __init__(self, max_accion, bins, initial_epsilon: float, epsilon_decay: float, final_epsilon: float, discount_factor: float = 0.95):\n",
    "        super().__init__()\n",
    "        \"\"\"Initialize a Reinforcement Learning agent with an empty dictionary\n",
    "        of state-action values (q_values), a learning rate and an epsilon.\n",
    "\n",
    "        Args:\n",
    "            learning_rate: The learning rate\n",
    "            initial_epsilon: The initial epsilon value\n",
    "            epsilon_decay: The decay for epsilon\n",
    "            final_epsilon: The final epsilon value\n",
    "            discount_factor: The discount factor for computing the Q-value\n",
    "        \"\"\"\n",
    "        self.q_table = defaultdict(lambda: np.zeros(max_accion))\n",
    "        self.visitas = defaultdict(lambda: np.zeros(max_accion))\n",
    "\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "        self.epsilon = initial_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.final_epsilon = final_epsilon\n",
    "\n",
    "        self.bins = bins\n",
    "\n",
    "        self.training_error = []\n",
    "    \n",
    "    def elegir_accion(self, estado, max_accion, explorar = True) -> int:\n",
    "\n",
    "        estado_discreto = discretize_state(estado, self.bins)\n",
    "        if (explorar and np.random.random() < self.epsilon):\n",
    "            return random.randrange(max_accion)\n",
    "        else:\n",
    "            return int(np.argmax(self.q_table[estado_discreto]))\n",
    "    \n",
    "    def aprender(self, estado_anterior, estado_siguiente, accion, recompensa, terminado):\n",
    "        estado_anterior_discreto = discretize_state(estado_anterior, self.bins)\n",
    "        estado_siguiente_discreto = discretize_state(estado_siguiente, self.bins)\n",
    "\n",
    "        self.visitas[estado_anterior_discreto][accion] += 1\n",
    "\n",
    "        \"\"\"Updates the Q-value of an action.\"\"\"\n",
    "        future_q_value = (not terminado) * np.max(self.q_table[estado_siguiente_discreto])\n",
    "        temporal_difference = (\n",
    "            recompensa + self.discount_factor * future_q_value - self.q_table[estado_anterior_discreto][accion]\n",
    "        )\n",
    "\n",
    "        lr = 1 / self.visitas[estado_anterior_discreto][accion]\n",
    "\n",
    "        self.q_table[estado_anterior_discreto][accion] = (\n",
    "            self.q_table[estado_anterior_discreto][accion] + lr * temporal_difference\n",
    "        )\n",
    "        self.training_error.append(temporal_difference)\n",
    "\n",
    "    def fin_episodio(self):\n",
    "        self.epsilon = max(self.final_epsilon, self.epsilon - self.epsilon_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ee51f5-aff8-4d18-934e-92acdcb617c0",
   "metadata": {},
   "source": [
    "Y ejecutar con el muchos episodios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e0524d5-0d12-46a8-8437-984b981fbae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]/home/julio/.local/lib/python3.10/site-packages/numpy/_core/fromnumeric.py:3904: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/julio/.local/lib/python3.10/site-packages/numpy/_core/_methods.py:147: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "  0%|          | 9/10000 [00:00<02:36, 63.68it/s, promedio=-177, recompensa=-414] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo nan de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1013/10000 [00:16<02:21, 63.42it/s, promedio=-162, recompensa=-74.8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -163.12802467743523 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2011/10000 [00:35<02:32, 52.23it/s, promedio=-155, recompensa=-109]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -147.0510355111974 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3008/10000 [00:58<03:03, 38.15it/s, promedio=-146, recompensa=-283]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -128.09173633813037 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4006/10000 [01:28<03:09, 31.71it/s, promedio=-145, recompensa=-60.3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -141.23919507058133 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5003/10000 [02:06<04:05, 20.35it/s, promedio=-142, recompensa=-70.8]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -129.78717829849032 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6004/10000 [02:49<02:50, 23.39it/s, promedio=-133, recompensa=-47.8] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -90.65348238757313 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7005/10000 [03:31<02:08, 23.22it/s, promedio=-124, recompensa=-39.3] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -69.07272416387069 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8005/10000 [04:15<01:28, 22.54it/s, promedio=-116, recompensa=-69.5]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -62.106232901033714 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9003/10000 [05:00<00:51, 19.33it/s, promedio=-109, recompensa=-47.5]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Se obtuvo -48.36376092504068 de recompensa, en promedio en los últimos 1000 episodios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [05:45<00:00, 28.94it/s, promedio=-101, recompensa=-94.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasa de éxito: 0.0351. Se obtuvo -101.47377887979772 de recompensa, en promedio\n"
     ]
    }
   ],
   "source": [
    "# Nota: hay que transformar esta celda en código para ejecutar (Esc + y)\n",
    "# Advertencia: este bloque es un loop infinito si el agente se deja sin implementar\n",
    "from tqdm import tqdm \n",
    "entorno = gym.make('LunarLander-v2').env\n",
    "num_episodios = 200000\n",
    "max_accion = entorno.action_space.n\n",
    "initial_epsilon = 1.0\n",
    "epsilon_decay = initial_epsilon / (num_episodios / 2)\n",
    "final_epsilon = 0.05\n",
    "discount_factor = 0.95\n",
    "\n",
    "agente = AgenteRL(max_accion = max_accion, bins = bins, initial_epsilon = initial_epsilon, epsilon_decay = epsilon_decay, final_epsilon = final_epsilon, discount_factor = discount_factor)\n",
    "exitos = 0\n",
    "recompensa_episodios = []\n",
    "with tqdm(total=num_episodios) as pbar:\n",
    "    for i in range(num_episodios):\n",
    "        recompensa = ejecutar_episodio(agente)\n",
    "        if i % 1000 == 0:\n",
    "            ultimas_1000_recompensas = recompensa_episodios[-1000:]\n",
    "            promedio_ultimas_1000 = np.mean(ultimas_1000_recompensas)\n",
    "            print(f\" Se obtuvo {promedio_ultimas_1000} de recompensa, en promedio en los últimos 1000 episodios\")\n",
    "        if (recompensa >= 200):\n",
    "            exitos += 1\n",
    "        recompensa_episodios += [recompensa]\n",
    "        pbar.set_postfix(recompensa=recompensa, promedio=numpy.mean(recompensa_episodios))\n",
    "        pbar.update(1)\n",
    "print(f\"Tasa de éxito: {exitos / num_episodios}. Se obtuvo {numpy.mean(recompensa_episodios)} de recompensa, en promedio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b7079e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 11 exitoso, recompensa: 239.96107233471562\n",
      "Episodio 14 exitoso, recompensa: 246.3168570101475\n",
      "Episodio 16 exitoso, recompensa: 271.6828052055199\n",
      "Episodio 19 exitoso, recompensa: 219.70718377182348\n",
      "Episodio 20 exitoso, recompensa: 219.08523342720133\n",
      "Episodio 34 exitoso, recompensa: 229.8447348608864\n",
      "Episodio 35 exitoso, recompensa: 297.47629625788863\n",
      "Episodio 36 exitoso, recompensa: 248.77618998099882\n",
      "Episodio 56 exitoso, recompensa: 248.56141877709865\n",
      "Episodio 59 exitoso, recompensa: 282.3006643948541\n",
      "Episodio 64 exitoso, recompensa: 253.0081444946986\n",
      "Episodio 73 exitoso, recompensa: 250.77195381987494\n",
      "Episodio 88 exitoso, recompensa: 249.48539620030712\n",
      "Episodio 92 exitoso, recompensa: 235.12792565044023\n",
      "Episodio 93 exitoso, recompensa: 257.3708106813933\n",
      "Episodio 100 exitoso, recompensa: 276.18263390651265\n",
      "Episodio 101 exitoso, recompensa: 285.0830658690552\n",
      "Episodio 107 exitoso, recompensa: 268.96794795257745\n",
      "Episodio 108 exitoso, recompensa: 238.03193486668536\n",
      "Episodio 128 exitoso, recompensa: 249.2058469606125\n",
      "Episodio 172 exitoso, recompensa: 222.71529864466095\n",
      "Episodio 174 exitoso, recompensa: 295.4487757061433\n",
      "Episodio 183 exitoso, recompensa: 272.0423227382911\n",
      "Episodio 186 exitoso, recompensa: 223.1091900026125\n",
      "Episodio 193 exitoso, recompensa: 245.1308232118336\n",
      "Episodio 217 exitoso, recompensa: 278.1905693019237\n",
      "Episodio 224 exitoso, recompensa: 284.59283363788916\n",
      "Episodio 233 exitoso, recompensa: 210.6951622824065\n",
      "Episodio 234 exitoso, recompensa: 231.25408365745506\n",
      "Episodio 235 exitoso, recompensa: 239.8743680229702\n",
      "Episodio 237 exitoso, recompensa: 258.27971090793426\n",
      "Episodio 262 exitoso, recompensa: 266.34629583844503\n",
      "Episodio 264 exitoso, recompensa: 275.7426451712121\n",
      "Episodio 270 exitoso, recompensa: 256.9912979875237\n",
      "Episodio 281 exitoso, recompensa: 243.9503024177752\n",
      "Episodio 299 exitoso, recompensa: 266.6271008010932\n",
      "Episodio 306 exitoso, recompensa: 257.60543098449347\n",
      "Episodio 309 exitoso, recompensa: 214.13711961369762\n",
      "Episodio 315 exitoso, recompensa: 230.829672028078\n",
      "Episodio 316 exitoso, recompensa: 278.18391369429\n",
      "Episodio 336 exitoso, recompensa: 290.4361370833358\n",
      "Episodio 346 exitoso, recompensa: 221.62896431739344\n",
      "Episodio 362 exitoso, recompensa: 221.29454486504596\n",
      "Episodio 363 exitoso, recompensa: 255.69825700465623\n",
      "Episodio 386 exitoso, recompensa: 225.38905309780006\n",
      "Episodio 394 exitoso, recompensa: 262.4430701825095\n",
      "Episodio 398 exitoso, recompensa: 265.24155425256043\n",
      "Episodio 407 exitoso, recompensa: 275.4667571178114\n",
      "Episodio 416 exitoso, recompensa: 234.4984249339923\n",
      "Episodio 420 exitoso, recompensa: 266.0020927609816\n",
      "Episodio 422 exitoso, recompensa: 252.34452548985456\n",
      "Episodio 423 exitoso, recompensa: 238.96414943480158\n",
      "Episodio 431 exitoso, recompensa: 274.9118493813562\n",
      "Episodio 456 exitoso, recompensa: 216.07508221238362\n",
      "Episodio 459 exitoso, recompensa: 289.80575626071413\n",
      "Episodio 464 exitoso, recompensa: 313.37773317692654\n",
      "Episodio 465 exitoso, recompensa: 269.9470363618091\n",
      "Episodio 476 exitoso, recompensa: 272.7319593320469\n",
      "Episodio 478 exitoso, recompensa: 245.48931269263974\n",
      "Episodio 488 exitoso, recompensa: 250.84664495576405\n",
      "Episodio 494 exitoso, recompensa: 254.3786063222433\n",
      "Episodio 498 exitoso, recompensa: 257.39924143775954\n",
      "Episodio 499 exitoso, recompensa: 265.33448846890303\n",
      "Episodio 500 exitoso, recompensa: 260.63874897029905\n",
      "Episodio 512 exitoso, recompensa: 259.0120753520693\n",
      "Episodio 517 exitoso, recompensa: 214.3417861847654\n",
      "Episodio 522 exitoso, recompensa: 251.05869487429723\n",
      "Episodio 534 exitoso, recompensa: 215.92012883126512\n",
      "Episodio 541 exitoso, recompensa: 259.91818891499713\n",
      "Episodio 543 exitoso, recompensa: 282.5252368621791\n",
      "Episodio 544 exitoso, recompensa: 270.02475240036733\n",
      "Episodio 546 exitoso, recompensa: 276.571390091404\n",
      "Episodio 550 exitoso, recompensa: 223.40121866580478\n",
      "Episodio 553 exitoso, recompensa: 274.77656538317933\n",
      "Episodio 554 exitoso, recompensa: 292.65797522893115\n",
      "Episodio 570 exitoso, recompensa: 239.17975681805927\n",
      "Episodio 581 exitoso, recompensa: 235.18943880078928\n",
      "Episodio 582 exitoso, recompensa: 229.29599274337994\n",
      "Episodio 589 exitoso, recompensa: 256.7116194091715\n",
      "Episodio 594 exitoso, recompensa: 261.55313043143525\n",
      "Episodio 604 exitoso, recompensa: 263.2670431248198\n",
      "Episodio 605 exitoso, recompensa: 242.90466996038793\n",
      "Episodio 613 exitoso, recompensa: 238.14803464940167\n",
      "Episodio 615 exitoso, recompensa: 201.20105990101888\n",
      "Episodio 617 exitoso, recompensa: 291.3040630662904\n",
      "Episodio 638 exitoso, recompensa: 214.1794606977872\n",
      "Episodio 643 exitoso, recompensa: 267.6388102108651\n",
      "Episodio 644 exitoso, recompensa: 289.56305646817407\n",
      "Episodio 657 exitoso, recompensa: 267.7977508508246\n",
      "Episodio 665 exitoso, recompensa: 205.57782106696408\n",
      "Episodio 670 exitoso, recompensa: 283.70127913962654\n",
      "Episodio 682 exitoso, recompensa: 269.1730381355582\n",
      "Episodio 698 exitoso, recompensa: 301.1542423428683\n",
      "Episodio 704 exitoso, recompensa: 257.04686953548526\n",
      "Episodio 705 exitoso, recompensa: 233.58936010155446\n",
      "Episodio 706 exitoso, recompensa: 244.2608950073512\n",
      "Episodio 712 exitoso, recompensa: 257.3688357016777\n",
      "Episodio 713 exitoso, recompensa: 274.95806598149136\n",
      "Episodio 715 exitoso, recompensa: 222.2380674427361\n",
      "Episodio 716 exitoso, recompensa: 261.30803204030155\n",
      "Episodio 721 exitoso, recompensa: 213.16164133222725\n",
      "Episodio 722 exitoso, recompensa: 214.50123553691836\n",
      "Episodio 726 exitoso, recompensa: 261.93210681897926\n",
      "Episodio 740 exitoso, recompensa: 235.4659959451157\n",
      "Episodio 741 exitoso, recompensa: 261.4239653038138\n",
      "Episodio 744 exitoso, recompensa: 250.5131458208179\n",
      "Episodio 750 exitoso, recompensa: 255.47510159440378\n",
      "Episodio 752 exitoso, recompensa: 262.0943278357528\n",
      "Episodio 760 exitoso, recompensa: 251.92322400235338\n",
      "Episodio 767 exitoso, recompensa: 250.62769172249773\n",
      "Episodio 796 exitoso, recompensa: 230.76706302074376\n",
      "Episodio 807 exitoso, recompensa: 251.898528162308\n",
      "Episodio 809 exitoso, recompensa: 284.4853539982762\n",
      "Episodio 811 exitoso, recompensa: 277.232611635937\n",
      "Episodio 829 exitoso, recompensa: 250.4272805019923\n",
      "Episodio 839 exitoso, recompensa: 308.4801904141451\n",
      "Episodio 842 exitoso, recompensa: 235.85880102859915\n",
      "Episodio 847 exitoso, recompensa: 268.0902429167983\n",
      "Episodio 849 exitoso, recompensa: 224.63858317210827\n",
      "Episodio 850 exitoso, recompensa: 244.90449746675867\n",
      "Episodio 853 exitoso, recompensa: 262.6928918063388\n",
      "Episodio 886 exitoso, recompensa: 224.51195403279274\n",
      "Episodio 887 exitoso, recompensa: 272.6057648174133\n",
      "Episodio 896 exitoso, recompensa: 235.61088103160753\n",
      "Episodio 904 exitoso, recompensa: 272.1322957200442\n",
      "Episodio 926 exitoso, recompensa: 235.82213180888024\n",
      "Episodio 928 exitoso, recompensa: 202.5912365002306\n",
      "Episodio 929 exitoso, recompensa: 254.8802946998817\n",
      "Episodio 941 exitoso, recompensa: 254.81031687663292\n",
      "Episodio 947 exitoso, recompensa: 287.6370209811008\n",
      "Episodio 965 exitoso, recompensa: 250.5591402129535\n",
      "Episodio 969 exitoso, recompensa: 286.9518325122373\n",
      "Episodio 974 exitoso, recompensa: 222.81951987992306\n",
      "Episodio 984 exitoso, recompensa: 216.90442763435328\n",
      "Episodio 989 exitoso, recompensa: 257.30972731843156\n",
      "Episodio 994 exitoso, recompensa: 240.04331709193835\n",
      "Episodio 997 exitoso, recompensa: 263.284603758828\n",
      "Tasa de éxito: 0.137. Se obtuvo -23.75216834973866 de recompensa, en promedio\n"
     ]
    }
   ],
   "source": [
    "# Nota: hay que transformar esta celda en código para ejecutar (Esc + y)\n",
    "# Advertencia: este bloque es un loop infinito si el agente se deja sin implementar\n",
    "\n",
    "entorno = gym.make('LunarLander-v2').env\n",
    "exitos = 0\n",
    "recompensa_episodios = []\n",
    "num_episodios = 1000\n",
    "for i in range(num_episodios):\n",
    "    recompensa = ejecutar_episodio(agente,aprender=False)\n",
    "    # Los episodios se consideran exitosos si se obutvo 200 o más de recompensa total\n",
    "    if (recompensa >= 200):\n",
    "        print(f\"Episodio {i} exitoso, recompensa: {recompensa}\")\n",
    "        exitos += 1\n",
    "    recompensa_episodios += [recompensa]\n",
    "print(f\"Tasa de éxito: {exitos / num_episodios}. Se obtuvo {numpy.mean(recompensa_episodios)} de recompensa, en promedio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d6cf54e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[ 2.14378328  0.1456758  -2.5471625  -1.04198937]\n",
      "[10.88168007  8.41582346  9.66311813  9.30587503]\n",
      "[ 8.03607187 11.02052866  4.70930265 10.14997914]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[3.05764835 3.05794997 2.30918161 3.0555464 ]\n"
     ]
    }
   ],
   "source": [
    "# Estado:\n",
    "# (x, y, x_vel, y_vel, theta, theta_vel, pie_izq_en_contacto, pie_derecho_en_contacto)\n",
    "# Nave cayendo rapidamente hacia abajo sin rotacion en la matriz Q\n",
    "estado = discretize_state([0, 1.5, 0, -5, 0, 0, 0, 0], bins)\n",
    "print(agente.q_table[estado])\n",
    "# Nave cayendo rapidamente hacia abajo sin rotacion en la matriz Q\n",
    "estado = discretize_state([0, 1.5, 0, -4, 0, 0, 0, 0], bins)\n",
    "print(agente.q_table[estado])\n",
    "# Nave cayendo rapidamente hacia abajo sin rotacion en la matriz Q\n",
    "estado = discretize_state([0, 0.5, 0, -2, 0, 0, 0, 0], bins)\n",
    "print(agente.q_table[estado])\n",
    "estado_final = discretize_state([0, 0, 0, 0, 0, 0, 1, 1], bins)\n",
    "print(agente.q_table[estado_final])\n",
    "\n",
    "estado_2 = discretize_state([0, 0, 0, 0, 0, 0, 0, 1], bins)\n",
    "print(agente.q_table[estado_2])\n",
    "\n",
    "estado_3 = discretize_state([0, 0, 0, 0, 0, 0, 1, 0], bins)\n",
    "print(agente.q_table[estado_3])\n",
    "\n",
    "# Aterriza pero no en el centro der\n",
    "estado_4 = discretize_state([1.5, 0, 0, 0, 0, 0, 0, 0], bins)\n",
    "print(agente.q_table[estado_4])\n",
    "\n",
    "# Aterriza en el centro izq\n",
    "estado_5 = discretize_state([-1.5, 0, 0, 0, 0, 0, 0, 0], bins)\n",
    "print(agente.q_table[estado_5])\n",
    "\n",
    "# Estado inicial\n",
    "estado_6 = discretize_state([0, 1.5, 0, 0, 0, 0, 0, 0], bins)\n",
    "print(agente.q_table[estado_6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e7b346-6f23-44e2-a645-e4548ab470ef",
   "metadata": {},
   "source": [
    "Analizar los resultados de la ejecución anterior, incluyendo:\n",
    " * Un análisis de los parámetros utilizados en el algoritmo (aprendizaje, política de exploración)\n",
    " * Un análisis de algunos 'cortes' de la matriz Q y la política (p.e. qué hace la nave cuando está cayendo rápidamente hacia abajo, sin rotación)\n",
    " * Un análisis de la evolución de la recompensa promedio\n",
    " * Un análisis de los casos de éxito\n",
    " * Un análisis de los casos en el que el agente falla\n",
    " * Qué limitante del agente de RL les parece que afecta más negativamente su desempeño. Cómo lo mejorarían? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a680260",
   "metadata": {},
   "source": [
    "# Análisis de resultados\n",
    "\n",
    "En la siguiente sección estaremos realizando un análisis de los resultados obtenidos tras la implementación y el entrenamiento de un agente de RL basado en Q Learning. En esta sección explicaremos cómo construimos el agente, en qué nos basamos, qué parámetros tuvimos en cuenta y por qué. También analizaremos algunas situaciones particulares del agente para ver su comportamiento en detalle, algunos casos de éxito y de falla, cómo evolucionó la recompensa promedio a medida que avanzaba el entrenamiento y qué limitantes vemos en el agente y cómo se podrían mitigar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7858dc3d",
   "metadata": {},
   "source": [
    "## Análisis de implementación y decisiones de diseño\n",
    "\n",
    "A la hora de construir nuestro agente RL basado en Q Learning, nos basamos fuertemente en el teórico del curso, utilizando la metodología vista en clase, y también en la documentación de gymnasium, en la cual explican, sobre otro ejemplo, como entrenarlo con Q Learning utilizando la librería. El link a la documentación utilizada es el siguiente: https://gymnasium.farama.org/tutorials/training_agents/blackjack_tutorial/.\n",
    "\n",
    "### Estrategia de acción a tomar\n",
    "\n",
    "Lo primero que decidimos sobre el modelo es cómo debiamos elegir una acción, dado que si siempre elegimos la mejor acción nos perdemos de encontrar acciones que no fueron descubiertas previamente, por lo que hay que encontrar un balance entre tomar decisiones azarosas y las mejores decisiones. Ante esto teníamos 2 opciones: \n",
    "- Soft Max: En esta estrategia, se asignan probabilidades a las diferentes acciones usando una función. Las acciones con valores de Q más altos tendrán mayores probabilidades de ser seleccionadas, pero incluso las acciones con valores menores todavía tienen una probabilidad no nula de ser escogidas, lo que genera que se suela elegir las acciones con valores de Q más altos, pero que con cierta probabilidad no nula se tomen otras acciones que pueden resultar beneficiosas.\n",
    "- Epsilon-Greedy: En esta estrategia, se toma un valor epsilon, que viene dado como parámetro, y se toma la mejor acción (mayor valor de la tabla Q para ese estado) con probabilidad 1-epsilon, y una decisión azarosa con probabilidad epsilon. De esta forma se asegura que tomemos acciones azarosas e informadas, lo que contribuye a un mejor entrenamiento.\n",
    "- Epsilon-Greedy con decaimiento: Esta estrategia es una variante de la estrategia Epsilon-Greedy que aparte del valor epsilon toma un valor de epsilon minimo y un valor de decaimiento de epsilon. Por cada episodio que pasa se reduce el epsilon restandole el valor de decaimiento, hasta que en algún episodio se llega al valor mínimo de epsilon, el cual se utiliza durante el resto de entrenamiento. Esto lo que genera es que al principio del entrenamiento tomemos más decisiones azarosas, ya que no tenemos tanto conocimiento del entorno aún, y a medida que pasa el tiempo y se asume que tenemos más información y podemos tomar más decisiones informadas, la probabilidad de tomar decisiones azarosas se reduce y en su lugar se toman decisiones informadas con respecto a los valores de la tabla Q.\n",
    "\n",
    "Ante estas tres opciones decidimos inclinarnos por la tercera opción: Epsilon-Greedy con decaimiento. Nos pareció una estrategia coherente, un poco mejor que el Epsilon-Greedy sin esta variante, y creímos que en términos de implementación no era muy compleja. Decidimos probar esta estrategia y en caso de notar un bajo desempeño, cambiar a Soft Max. Cómo se verá más adelante en este informe, tras ver que los resultados fueron óptimos, se mantuvo la decisión.\n",
    "\n",
    "### Hiperparámetros a utilizar y valores utilizados\n",
    "\n",
    "#### Epsilon, Epsilon Mínimo y Decaimiento de Epsilon\n",
    "\n",
    "Tras la decisión de la estrategia a utilizar, debíamos decidir qué valores utilizar para dichos parámetros (epsilon, decaimiento de epsilon y epsilon mínimo). Para el epsilon se tomó la decisión del valor de 1, ya que entendimos que cuando recién se comienza el entrenamiento queremos explorar lo más que se pueda, ya que no tenemos ningún tipo de información del entorno aún, por lo que la probabilidad conviene que sea 1, y que luego esta vaya disminuyendo linealmente. Cómo epsilon mínimo tomamos el valor de 0.05. Esta decisión se baso mayormente en experiencia. Sabíamos que queríamos un valor bajo de epsilon mínimo ya que una vez que tengamos la tabla Q bastante llena, queremos tomar decisiones informadas la mayor parte de las veces. Al principio probamos con un valor de 0.1, de 0.2, de 0.05, y así fuimos probando con distintos valores, llegando a que los mayores resultados se daban con el valor de 0.05, aunque la diferencia no era tanta, sí había una diferencia.\n",
    "Por último, debimos decidir sobre el valor del decaimiento de epsilon.\n",
    "Sobre este valor nos encontramos con un tema a la hora de la implementación, y fue que al principio utilizamos el decaimiento de epsilon como factor que se le multiplicaba al valor de epsilon hasta llegar al epsilon mínimo. Por lo que poniamos un valor fijo alto de decaimiento de epsilon (ej: 0.995). Vimos que los resultados no eran buenos y decidimos probar con valores paramétricos, utilizando la cantidad de episodios para decidir sobre el valor. Para esto se nos ocurrió la fórmula: decaimiento de epsilon = (epsilon - epsilon min) ^ (1/episodios), con la idea de que esto haría que el epsilon fuera decayendo y en el último episodio se llegara al valor mínimo de epsilon.\n",
    "Esta opción tampoco nos trajo buenos resultados, por lo que tras revisar la documentación brindada por gymnasium, vimos que estaba bien poner un valor parámetrico en función de la cantidad de episodios, pero que la disminución del epsilon con el factor de decaimiento era a través de una resta. Epsilon = Epsilon - decaimiento_epsilon. En este caso vimos que en la documentación utilizaban un valor de decaimiento descripto por la fórmula: decaimiento_epsilon = epsilon / (num_episodios / 2). Tras utilizar esta fórmula nuestros resultados mejoraron bastante. Probamos luego variar el valor de 2 en la fórmula por distintos múltiplos de 2, ya que entendimos que por cómo es su funcionamiento, cuanto más grande dicho valor, durante menos tiempo se mantiene el epsilon en un número alto, pero vimos que la fórmula detallada por la documentación fue la que nos dió mejores resultados en la práctica.\n",
    "\n",
    "#### Tasa de aprendizaje en ambiente no determinista\n",
    "\n",
    "Dado que nos encontramos en el problema de Lunar Lander, este es un problema continuo, donde los estados son los ejes horizontal, vertical y de rotación, sus velocidades y 2 booleanos que indican si las patas de la nave están en contacto con el suelo o no. Por más de que en un principio discretizamos el estado en una cantidad de bins, esto no quita que el problema deje de ser continuo, por lo que al trabajar con un problema continuo, nos va a suceder muchas veces que al tomar una misma acción desde un mismo estado caigamos en estados distintos. Esto tiene un problema si no lo tomamos en cuenta, y es que a la hora de llenar la tabla Q con los valores de recompensa, las recompensas no van a ser las mismas desde un estado y una acción ya que los resultados posteriores dependen del estado en el que se caiga posteriormente, y estos varían cada vez que los tomamos. \n",
    "Para esto se utiliza el hiperparámetro tasa de aprendizaje.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab48702f",
   "metadata": {},
   "source": [
    "## Análisis de \"cortes\" de la matriz Q y la política"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3825b32b",
   "metadata": {},
   "source": [
    "## Análisis de evolución de recompensa promedio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac3d75d",
   "metadata": {},
   "source": [
    "## Análisis de casos (Éxitos y Fallos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f7d407",
   "metadata": {},
   "source": [
    "## Limitantes del agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f93fe904-e691-42ff-8fd4-b360d08431cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analizar los resultados aqui\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
